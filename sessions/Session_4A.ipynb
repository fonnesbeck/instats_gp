{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_4A.ipynb)\n",
    "\n",
    "# Session 4A: Multi-Output Gaussian Processes\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Model correlated outputs** using Intrinsic Coregionalization Models (ICM)\n",
    "2. **Extend to multiple timescales** with Linear Coregionalization Models (LCM)\n",
    "3. **Apply multi-output GPs** to real data (baseball pitcher spin rates)\n",
    "4. **Interpret coregionalization matrices** to understand output correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import polars as pl\n",
    "\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:= 8675309)\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "OUTPUT_DIR = \"../output/\"\n",
    "\n",
    "print(f\"PyMC: {pm.__version__})\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Polars: {pl.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Multi-Output Gaussian Processes\n",
    "\n",
    "So far in this workshop, we've modeled one output at a time‚Äîa single time series, a single spatial field. But many real-world scenarios involve **multiple related outputs** that we want to model jointly.\n",
    "\n",
    "### Why Model Multiple Outputs Together?\n",
    "\n",
    "Consider analyzing fastball spin rates for several elite baseball pitchers across a season. You could fit separate GPs for each pitcher, but this approach misses something crucial: **pitchers may share patterns**. Similar mechanics, weather conditions, or even changes to the ball itself could create correlations between their spin rates.\n",
    "\n",
    "Multi-output GPs offer several advantages:\n",
    "\n",
    "1. **Information sharing**: Data-rich outputs help inform data-scarce ones\n",
    "2. **Learning correlation structure**: Discover which outputs vary together  \n",
    "3. **Improved predictions**: Borrowing strength across related outputs\n",
    "4. **Computational efficiency**: One joint model vs. many separate models\n",
    "\n",
    "The key innovation is the **coregionalization matrix**, which learns how outputs correlate. We'll start with the Intrinsic Coregionalization Model (ICM), then extend to the more flexible Linear Coregionalization Model (LCM)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8ba97",
   "metadata": {},
   "source": [
    "## Intrinsic Coregionalization Model (ICM)\n",
    "\n",
    "The ICM provides an elegant way to model multiple related outputs jointly. The core idea: combine two sources of structure using the **Hadamard (elementwise) product**:\n",
    "\n",
    "1. **How inputs relate** (e.g., how does time affect the output?)\n",
    "2. **How outputs correlate with each other** (e.g., which pitchers have similar patterns?)\n",
    "\n",
    "Think of it like a recipe: if we're tracking the spin rates of 5 pitchers over a season, the ICM learns both the temporal dynamics (captured by a kernel over time) and the cross-pitcher correlations (captured by a coregionalization matrix)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2516e",
   "metadata": {},
   "source": [
    "### The Mathematics of Sharing Structure\n",
    "\n",
    "ICM uses the Kronecker product (‚äó) to combine two covariance structures:\n",
    "\n",
    "$$K_{ICM}([\\mathbf{x}_i, o_i], [\\mathbf{x}_j, o_j]) = K_{input}(\\mathbf{x}_i, \\mathbf{x}_j) \\times B(o_i, o_j)$$\n",
    "\n",
    "Where:\n",
    "- $K_{input}(\\mathbf{x}_i, \\mathbf{x}_j)$: Covariance over inputs (e.g., time)\n",
    "- $B(o_i, o_j)$: **Coregionalization matrix** ‚Äî covariance between outputs\n",
    "- $o_i, o_j$: Output indices (e.g., pitcher 0, pitcher 1, ...)\n",
    "\n",
    "Think of it as: *\"How similar are these inputs?\"* multiplied by *\"How correlated are these outputs?\"*\n",
    "\n",
    "> **üì¶ What's a Kronecker product?**\n",
    ">\n",
    "> The Kronecker product $A \\otimes B$ takes each element of matrix $A$ and multiplies it by the entire matrix $B$, creating a larger block matrix. If $A$ is $2 \\times 2$ and $B$ is $3 \\times 3$, the result is $6 \\times 6$. In our context: for every pair of time points (captured by $K_{input}$), we get a separate copy of the output correlation structure (captured by $B$), scaled by how similar those time points are.\n",
    "\n",
    "The coregionalization matrix has a special structure that ensures it's positive semi-definite:\n",
    "\n",
    "$$B = WW^T + \\text{diag}(\\kappa)$$\n",
    "\n",
    "This separates:\n",
    "- $WW^T$: Shared variations across outputs (low-rank structure)\n",
    "- $\\text{diag}(\\kappa)$: Output-specific independent noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea50ae",
   "metadata": {},
   "source": [
    "### Real Data: Baseball Pitcher Spin Rates\n",
    "\n",
    "Let's see ICM in action with real data. We'll model fastball spin rates of 5 elite pitchers across the 2021 MLB season.\n",
    "\n",
    "**Why spin rate matters:** Higher spin rates make fastballs harder to hit. Spin rate fluctuates game-to-game due to:\n",
    "- Fatigue accumulation  \n",
    "- Mechanics adjustments  \n",
    "- Measurement noise  \n",
    "- Potentially shared factors (weather, ball characteristics)\n",
    "\n",
    "Some pitchers' spin rates may be correlated if they have similar mechanics or respond similarly to external factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseball spin rate data\n",
    "df_spin = pl.read_csv(DATA_DIR + \"fastball_spin_rates.csv\")\n",
    "\n",
    "# Standardize spin rates to z-scores\n",
    "mean_spin = df_spin[\"avg_spin_rate\"].mean()\n",
    "std_spin = df_spin[\"avg_spin_rate\"].std()\n",
    "df_spin = df_spin.with_columns([\n",
    "    ((pl.col(\"avg_spin_rate\") - mean_spin) / std_spin).alias(\"avg_spin_rate_std\")\n",
    "])\n",
    "\n",
    "print(f\"Total: {df_spin.height} observations, {df_spin['pitcher_name'].n_unique()} pitchers\")\n",
    "print(f\"Date range: {df_spin['game_date'].min()} to {df_spin['game_date'].max()}\")\n",
    "df_spin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1692087",
   "metadata": {},
   "source": [
    "We standardized spin rates so all pitchers are on the same scale (z-scores). This makes the coregionalization matrix more interpretable: values close to 1 indicate strong correlation between pitchers.\n",
    "\n",
    "For computational efficiency and demonstration purposes, we'll randomly select 5 pitchers from those with at least 20 games. This minimum threshold ensures sufficient data for stable GP inference with multi-output models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select 5 random pitchers with sufficient data\n",
    "all_pitchers_df = (df_spin\n",
    "    .group_by(\"pitcher_name\")\n",
    "    .agg(pl.count(\"game_date\").alias(\"n_games\"))\n",
    "    .filter(pl.col(\"n_games\") >= 20)  # Require at least 20 games for stability\n",
    "    .sort(\"n_games\", descending=True)\n",
    ")\n",
    "\n",
    "# Use RNG to select 5 random indices\n",
    "n_pitchers_available = all_pitchers_df.height\n",
    "random_indices = RNG.choice(n_pitchers_available, size=5, replace=False)\n",
    "top_pitchers_df = all_pitchers_df[random_indices.tolist()]\n",
    "\n",
    "top_pitchers = top_pitchers_df[\"pitcher_name\"].to_list()\n",
    "print(\"5 randomly selected pitchers (with ‚â•20 games each):\")\n",
    "print(top_pitchers_df)\n",
    "\n",
    "df_train = df_spin.filter(pl.col(\"pitcher_name\").is_in(top_pitchers))\n",
    "print(f\"\\nTraining data: {df_train.height} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410799e",
   "metadata": {},
   "source": [
    "Now we create two key index variables:\n",
    "\n",
    "1. **`game_date_idx`**: Integer days since season start (April 1, 2021 = day 0)  \n",
    "2. **`output_idx`**: Pitcher number (0 to 4)\n",
    "\n",
    "Our input matrix $X$ will be $(n, 2)$ where each row is `[game_date_idx, output_idx]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df_train = df_train.with_columns([\n",
    "    pl.col(\"game_date\").str.strptime(pl.Date, format=\"%Y-%m-%d\").alias(\"game_date_dt\")\n",
    "])\n",
    "\n",
    "# Create game date index (days since season start)\n",
    "min_date = df_train[\"game_date_dt\"].min()\n",
    "df_train = df_train.with_columns([\n",
    "    (pl.col(\"game_date_dt\") - min_date).dt.total_days().alias(\"game_date_idx\")\n",
    "])\n",
    "\n",
    "# Create output index\n",
    "pitcher_to_idx = {name: idx for idx, name in enumerate(top_pitchers)}\n",
    "df_train = df_train.with_columns([\n",
    "    pl.col(\"pitcher_name\").replace(pitcher_to_idx).alias(\"output_idx\")\n",
    "])\n",
    "\n",
    "# Sort by output then time\n",
    "df_train = df_train.sort([\"output_idx\", \"game_date_idx\"])\n",
    "\n",
    "print(\"Data structure:\")\n",
    "print(df_train.select([\"pitcher_name\", \"game_date_idx\", \"output_idx\", \"avg_spin_rate_std\"]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59474214",
   "metadata": {},
   "source": [
    "### Visualizing the Raw Data\n",
    "\n",
    "Before modeling, let's examine the raw time series. This helps us understand trends, volatility, and potential correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc91a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive time series plot\n",
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, pitcher in enumerate(top_pitchers):\n",
    "    pitcher_data = df_train.filter(pl.col(\"pitcher_name\") == pitcher)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pitcher_data[\"game_date_idx\"].to_list(),\n",
    "        y=pitcher_data[\"avg_spin_rate_std\"].to_list(),\n",
    "        mode='markers',\n",
    "        name=pitcher,\n",
    "        marker=dict(size=5, color=colors[i]),\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Fastball Spin Rates: 2021 Season (Standardized)\",\n",
    "    xaxis_title=\"Days Since Season Start\",\n",
    "    yaxis_title=\"Standardized Spin Rate\",\n",
    "    height=450,\n",
    "    hovermode='closest'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55949a",
   "metadata": {},
   "source": [
    "Each pitcher shows noisy variation. Some appear to have trends (gradual changes over the season), while others look more stationary. The ICM model will:\n",
    "\n",
    "1. **Smooth** trajectories to separate signal from noise  \n",
    "2. **Learn correlations** between pitchers  \n",
    "3. **Quantify uncertainty** with credible intervals\n",
    "\n",
    "Now let's build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc60ad4",
   "metadata": {},
   "source": [
    "### Building the ICM: Helper Function\n",
    "\n",
    "We define `get_icm()` to construct an ICM kernel. This combines an input kernel with a `Coregion` kernel using the Hadamard product (`*`).\n",
    "\n",
    "The `active_dims` parameter tells each kernel which columns to operate on:\n",
    "- Input kernel uses `active_dims=[0]` (time)  \n",
    "- Coregion kernel uses `active_dims=[1]` (pitcher index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22503314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icm(input_dim, kernel, W=None, kappa=None, B=None, active_dims=None):\n",
    "    \"\"\"\n",
    "    Construct Intrinsic Coregionalization Model kernel.\n",
    "    \n",
    "    Combines input kernel with output coregionalization via Hadamard product.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Total input dimensions (including output index)\n",
    "    kernel : pm.gp.cov.Covariance\n",
    "        Base kernel for inputs (e.g., ExpQuad over time)\n",
    "    W, kappa, B : tensors, optional\n",
    "        Coregionalization parameters\n",
    "    active_dims : list, optional\n",
    "        Dimensions for coregion kernel (typically [1] for output index)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pm.gp.cov.Covariance\n",
    "        ICM kernel\n",
    "    \"\"\"\n",
    "    coreg = pm.gp.cov.Coregion(\n",
    "        input_dim=input_dim,\n",
    "        W=W,\n",
    "        kappa=kappa,\n",
    "        B=B,\n",
    "        active_dims=active_dims\n",
    "    )\n",
    "    # Hadamard product: kernel * coreg\n",
    "    icm_cov = kernel * coreg\n",
    "    return icm_cov\n",
    "\n",
    "print(\"‚úì Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d2fb0",
   "metadata": {},
   "source": [
    "**Critical distinction:**\n",
    "- Kernel **addition** (`+`): combines multiple processes  \n",
    "- Kernel **multiplication** (`*`): Hadamard product for ICM\n",
    "\n",
    "The `*` operator creates a covariance where the input kernel and coregion kernel operate **independently** on their designated dimensions, then multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba438de3",
   "metadata": {},
   "source": [
    "### Preparing Data for PyMC\n",
    "\n",
    "Convert polars DataFrame to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training arrays\n",
    "X_train = df_train.select([\"game_date_idx\", \"output_idx\"]).to_numpy().astype(np.float64)\n",
    "y_train = df_train.select(\"avg_spin_rate_std\").to_numpy().flatten()\n",
    "\n",
    "n_outputs = len(top_pitchers)\n",
    "\n",
    "print(f\"X shape: {X_train.shape} (rows x [time, output_idx])\")\n",
    "print(f\"y shape: {y_train.shape}\")\n",
    "print(f\"n_outputs: {n_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae12ad2",
   "metadata": {},
   "source": [
    "### Specifying Priors\n",
    "\n",
    "We need priors for:\n",
    "- **Lengthscale** (`ell`): How quickly spin rate changes. `Gamma(2, 0.5)` gives mean ‚âà 4 days.\n",
    "- **Amplitude** (`eta`): Overall temporal variation. `Gamma(3, 1)` gives mean = 3.\n",
    "- **W**: Weight matrix $(5 \\times 2)$. Rank 2 assumes pitchers share ‚â§2 latent patterns.\n",
    "- **kappa**: Output-specific variances.\n",
    "- **sigma**: Observation noise.\n",
    "\n",
    "**Numerical stability note**: Multi-output GPs with irregular temporal spacing can produce ill-conditioned covariance matrices. We add **jitter** (a small diagonal term, here 1e-3) to ensure numerical stability during matrix inversion. This is especially important when:\n",
    "- Pitchers have different numbers of observations\n",
    "- Game dates are irregularly spaced\n",
    "- Some posterior samples produce extreme parameter values\n",
    "\n",
    "The jitter acts as a small amount of independent noise, regularizing the problem without meaningfully affecting predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c35ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as icm_model:\n",
    "    # Temporal kernel parameters\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=0.5)\n",
    "    eta = pm.Gamma(\"eta\", alpha=3, beta=1)\n",
    "    \n",
    "    # Base kernel on time (active_dims=[0])\n",
    "    kernel_time = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ell, active_dims=[0])\n",
    "    \n",
    "    # Coregionalization parameters\n",
    "    W = pm.Normal(\"W\", mu=0, sigma=3, shape=(n_outputs, 2))\n",
    "    kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "    \n",
    "    # Track B matrix\n",
    "    B = pm.Deterministic(\"B\", pt.dot(W, W.T) + pt.diag(kappa))\n",
    "    \n",
    "    # ICM kernel\n",
    "    cov_icm = get_icm(input_dim=2, kernel=kernel_time, W=W, kappa=kappa, active_dims=[1])\n",
    "    \n",
    "    # GP with increased jitter for numerical stability\n",
    "    gp = pm.gp.Marginal(cov_func=cov_icm)\n",
    "    \n",
    "    # Noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    \n",
    "    # Likelihood with increased jitter (1e-3 for robustness with sparse data)\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=sigma, jitter=1e-3)\n",
    "\n",
    "pm.model_to_graphviz(icm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606c16",
   "metadata": {},
   "source": [
    "### Sampling the Posterior\n",
    "\n",
    "This may take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e68e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with icm_model:\n",
    "    trace_icm = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        nuts_sampler=\"nutpie\",\n",
    "        random_seed=RANDOM_SEED,\n",
    "        chains=2,\n",
    "        target_accept=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855fe69",
   "metadata": {},
   "source": [
    "Let's check convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(trace_icm, var_names=[\"ell\", \"eta\", \"sigma\", \"kappa\"])\n",
    "print(f\"R-hat: [{summary['r_hat'].min():.4f}, {summary['r_hat'].max():.4f}]\")\n",
    "print(f\"ESS: [{summary['ess_bulk'].min():.0f}, {summary['ess_bulk'].max():.0f}]\")\n",
    "print(\"\\nEstimates:\")\n",
    "print(summary[[\"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5debc5",
   "metadata": {},
   "source": [
    "Good convergence! The model has learned temporal dynamics and output correlations simultaneously.\n",
    "\n",
    "Now let's make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc09f8f",
   "metadata": {},
   "source": [
    "### Posterior Predictions\n",
    "\n",
    "Create test grid for all 5 pitchers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29357e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data: 200 time points\n",
    "n_test = 200\n",
    "time_test = np.linspace(0, 199, n_test)\n",
    "\n",
    "# Stack for all outputs\n",
    "X_test_list = []\n",
    "for out_idx in range(n_outputs):\n",
    "    X_out = np.column_stack([time_test, np.full(n_test, out_idx)])\n",
    "    X_test_list.append(X_out)\n",
    "\n",
    "X_test = np.vstack(X_test_list)\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_CACHED = True\n",
    "\n",
    "if USE_CACHED:\n",
    "    ppc_icm = az.from_netcdf(OUTPUT_DIR + \"icm_spin_rate_ppc.nc\")\n",
    "\n",
    "else:\n",
    "    with icm_model:\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test, jitter=1e-3)\n",
    "        ppc_icm = pm.sample_posterior_predictive(\n",
    "            trace_icm,\n",
    "            var_names=[\"f_pred\"],\n",
    "            random_seed=RANDOM_SEED\n",
    "        )\n",
    "    ppc_icm.to_netcdf(OUTPUT_DIR + \"icm_spin_rate_ppc.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064ea5d",
   "metadata": {},
   "source": [
    "### Visualizing Multi-Output Predictions\n",
    "\n",
    "Plot posterior for each pitcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=5, cols=1,\n",
    "    subplot_titles=top_pitchers,\n",
    "    vertical_spacing=0.05,\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "for i, pitcher in enumerate(top_pitchers):\n",
    "    # Extract predictions for this output\n",
    "    f_pred_i = ppc_icm.posterior_predictive[\"f_pred\"].isel(\n",
    "        f_pred_dim_0=slice(i*n_test, (i+1)*n_test)\n",
    "    )\n",
    "    \n",
    "    mean = f_pred_i.mean(dim=[\"chain\", \"draw\"]).values\n",
    "    lower = np.percentile(f_pred_i.values, 2.5, axis=(0,1))\n",
    "    upper = np.percentile(f_pred_i.values, 97.5, axis=(0,1))\n",
    "    \n",
    "    # HDI band\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=upper, line=dict(width=0),\n",
    "        showlegend=False, hoverinfo='skip'\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=lower, fill='tonexty',\n",
    "        line=dict(width=0), showlegend=False,\n",
    "        fillcolor='rgba(135,206,250,0.3)'\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Mean\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=mean, mode='lines',\n",
    "        line=dict(color='steelblue', width=2),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Training data\n",
    "    pitcher_data = df_train.filter(pl.col(\"pitcher_name\") == pitcher)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pitcher_data[\"game_date_idx\"].to_list(),\n",
    "        y=pitcher_data[\"avg_spin_rate_std\"].to_list(),\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=3),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days Since Season Start\", row=5, col=1)\n",
    "fig.update_layout(height=1000, title_text=\"ICM Posterior Predictions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329aff5",
   "metadata": {},
   "source": [
    "Notice how the model smooths noisy observations while respecting each pitcher's unique pattern. The uncertainty (shaded regions) is wider where data is sparse and narrower where we have more observations‚Äîexactly what we want from a principled probabilistic model.\n",
    "\n",
    "**Why ICM matters here**: Compared to fitting 5 independent GPs, the ICM learns a **shared lengthscale** (‚âà25 days) and **shared amplitude** across all pitchers. This means pitchers with fewer observations (like Ross with 20 games) benefit from information borrowed from data-rich pitchers (like Lyles with 32 games) when estimating temporal smoothness. The result: more stable predictions and better uncertainty quantification, especially for pitchers with sparse data.\n",
    "\n",
    "Now let's examine what the model learned about correlations between pitchers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92d8ae",
   "metadata": {},
   "source": [
    "### The Coregionalization Matrix: Who's Correlated?\n",
    "\n",
    "Extract learned $B$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean of B\n",
    "B_post = az.extract(trace_icm, var_names=[\"B\"]).mean(dim=\"sample\").values\n",
    "\n",
    "# Heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "z=B_post,\n",
    "x=top_pitchers,\n",
    "y=top_pitchers,\n",
    "colorscale='RdBu',\n",
    "zmid=0,\n",
    "text=np.round(B_post, 2),\n",
    "texttemplate='%{text}',\n",
    "textfont={\"size\": 10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "title=\"Learned Coregionalization Matrix B\",\n",
    "xaxis_title=\"Pitcher\",\n",
    "yaxis_title=\"Pitcher\",\n",
    "height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Convert to correlation\n",
    "std_devs = np.sqrt(np.diag(B_post))\n",
    "corr = B_post / np.outer(std_devs, std_devs)\n",
    "print(\"\\nCorrelations:\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fe85f",
   "metadata": {},
   "source": [
    "The diagonal elements show each pitcher's variance (their own variability), while off-diagonal elements reveal shared variation. High positive values mean those pitchers' spin rates tend to fluctuate together‚Äîperhaps they have similar mechanics or respond similarly to external factors like weather.\n",
    "\n",
    "This is the power of multi-output GPs: we're not just smoothing trajectories, we're **learning the latent structure** of how outputs relate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84590b41",
   "metadata": {},
   "source": [
    "### ü§ñ EXERCISE: Exploring Rank Structure in ICM\n",
    "\n",
    "The rank of the weight matrix $W$ determines how many latent patterns the ICM can capture. In our model above, we used `W` with shape `(5, 2)` (rank 2), assuming pitchers share at most 2 common patterns.\n",
    "\n",
    "Let's explore how rank affects model fit and interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514437e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your LLM to help complete this comparison\n",
    "\n",
    "def compare_icm_ranks(X_train, y_train, n_outputs, ranks=[1, 2, 3]):\n",
    "    \"\"\"\n",
    "    Fit ICM models with different rank structures and compare graphically.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array\n",
    "        Training inputs (n_obs, 2) with [time, output_idx]\n",
    "    y_train : array\n",
    "        Training outputs (n_obs,)\n",
    "    n_outputs : int\n",
    "        Number of output dimensions (pitchers)\n",
    "    ranks : list of int\n",
    "        Ranks to try for the W matrix\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary mapping rank -> (model, trace)\n",
    "        \n",
    "    Tasks:\n",
    "    1. For each rank r in ranks:\n",
    "       - Build an ICM model with W shape (n_outputs, r)\n",
    "       - Sample from the posterior (500 draws, 500 tune)\n",
    "    2. Create comparison visualizations:\n",
    "       - Side-by-side heatmaps of learned correlation matrices for each rank\n",
    "       - Posterior distributions of lengthscale and amplitude parameters\n",
    "       - Distribution of kappa (output-specific variance) across ranks\n",
    "    3. Interpret: \n",
    "       - How do correlation structures differ across ranks?\n",
    "       - Does rank=1 produce very different correlations than rank=2 or 3?\n",
    "       - What's the practical tradeoff: simplicity vs capturing complex patterns?\n",
    "    \n",
    "    Prompt suggestion: \"Help me write a function that fits ICM models with different\n",
    "    ranks for the W matrix (shape n_outputs x rank), extracts the learned B matrices,\n",
    "    converts them to correlation matrices, and creates side-by-side Plotly heatmap\n",
    "    visualizations comparing the correlation structures across different ranks.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# Uncomment to test:\n",
    "# results = compare_icm_ranks(X_train, y_train, n_outputs, ranks=[1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141332d6",
   "metadata": {},
   "source": [
    "## Linear Coregionalization Model (LCM)\n",
    "\n",
    "The ICM we just built makes a strong assumption: **all correlations between outputs operate at the same timescale**. The single input kernel (ExpQuad over time) and single coregionalization matrix $B$ mean that whether two pitchers' spin rates move together on a day-to-day basis or across the entire season, they're modeled with the same correlation structure.\n",
    "\n",
    "But real-world processes often exhibit **multiple timescales of variation**. Consider our pitcher spin rates:\n",
    "\n",
    "- **Long-term trends**: Gradual seasonal changes from mechanics adjustments, aging effects, or evolving training regimens (weeks to months)\n",
    "- **Medium-term patterns**: Multi-week fluctuations from minor injuries, hot/cold streaks, or facing stronger/weaker batting lineups\n",
    "- **Short-term noise**: Game-to-game variation from fatigue, measurement error, pitch count, or weather conditions (days)\n",
    "\n",
    "Moreover, the **correlation structure might differ across timescales**. Two pitchers might share similar long-term seasonal arcs (high correlation at coarse timescales) but exhibit independent day-to-day fluctuations (low correlation at fine timescales). ICM cannot capture this nuance‚Äîit learns one lengthscale and applies the same coregionalization uniformly.\n",
    "\n",
    "### The Mathematics: Summing Independent Coregionalized Processes\n",
    "\n",
    "The **Linear Model of Coregionalization (LCM)** addresses this limitation by decomposing the covariance into a sum of independent ICM components:\n",
    "\n",
    "$$K_{LCM}(\\mathbf{x}_i, o_i; \\mathbf{x}_j, o_j) = \\sum_{q=1}^{Q} B_q \\otimes K_q(\\mathbf{x}_i, \\mathbf{x}_j)$$\n",
    "\n",
    "Where:\n",
    "- $Q$: Number of independent latent processes (typically 2-4)\n",
    "- $K_q$: The $q$-th input kernel (e.g., ExpQuad with lengthscale $\\ell_q$, Mat√©rn32 with $\\ell_q'$)\n",
    "- $B_q$: The $q$-th coregionalization matrix, capturing output correlations for process $q$\n",
    "\n",
    "**Intuition**: Imagine $Q=2$ with two kernels:\n",
    "1. **Kernel 1** (ExpQuad, $\\ell_1 \\approx 60$ days): Captures smooth seasonal trends\n",
    "2. **Kernel 2** (Mat√©rn32, $\\ell_2 \\approx 5$ days): Captures wiggly short-term fluctuations\n",
    "\n",
    "Each kernel gets its own coregionalization matrix:\n",
    "- $B_1$ tells us which pitchers share similar seasonal arcs\n",
    "- $B_2$ tells us which pitchers' day-to-day noise is correlated\n",
    "\n",
    "The total covariance is the sum of these independent contributions. If two pitchers have high correlation in $B_1$ but low in $B_2$, their trajectories will track closely over months but diverge significantly game-to-game.\n",
    "\n",
    "### Simplification: Shared Coregionalization\n",
    "\n",
    "In practice, estimating separate $B_q$ matrices for each kernel can be **statistically demanding**‚Äîyou're learning $Q$ coregionalization matrices, each with $O(P^2)$ parameters (where $P$ is the number of outputs). For moderate datasets, this often leads to overfitting or unidentifiable parameters.\n",
    "\n",
    "A common simplification is to **share the coregionalization structure** across kernels:\n",
    "\n",
    "$$K_{LCM}(\\mathbf{x}_i, o_i; \\mathbf{x}_j, o_j) = B \\otimes \\left( \\sum_{q=1}^{Q} K_q(\\mathbf{x}_i, \\mathbf{x}_j) \\right)$$\n",
    "\n",
    "Now we have:\n",
    "- One coregionalization matrix $B$ (same output correlations at all timescales)\n",
    "- Multiple input kernels $K_1, K_2, \\ldots$ with different lengthscales\n",
    "\n",
    "This variant still captures **multi-scale temporal dynamics**‚Äîthe different kernels model smooth trends, medium-frequency oscillations, and high-frequency noise independently. But it assumes the correlation pattern between outputs is consistent across these timescales.\n",
    "\n",
    "Think of it this way: we're saying \"Pitcher A and Pitcher B's spin rates are 70% correlated, and this correlation applies equally to their long-term seasonal arcs and their day-to-day fluctuations.\" This is less flexible than learning separate $B_q$ matrices, but it's often sufficient and far more statistically stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two Ways to Parameterize Coregionalization\n",
    "\n",
    "Recall from the ICM section that the coregionalization matrix has a structured decomposition:\n",
    "\n",
    "$$B = WW^T + \\text{diag}(\\kappa)$$\n",
    "\n",
    "Where:\n",
    "- $W$: A $(P \\times R)$ weight matrix, with $P$ outputs and $R$ latent factors (the rank)\n",
    "- $\\kappa$: A length-$P$ vector of output-specific variances\n",
    "\n",
    "In PyMC, you can specify coregionalization in **two equivalent ways**:\n",
    "\n",
    "**Option 1: Specify $W$ and $\\kappa$** (low-rank parameterization)\n",
    "```python\n",
    "W = pm.Normal(\"W\", mu=0, sigma=3, shape=(n_outputs, rank))\n",
    "kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "cov = pm.gp.cov.Coregion(input_dim=2, W=W, kappa=kappa, active_dims=[1])\n",
    "```\n",
    "\n",
    "This parameterization is **statistically interpretable**: \n",
    "- The rank $R$ controls model complexity (how many shared patterns drive output correlations)\n",
    "- $W$ captures shared latent factors\n",
    "- $\\kappa$ captures output-specific independent variation\n",
    "- PyMC internally constructs $B = WW^T + \\text{diag}(\\kappa)$\n",
    "\n",
    "**Option 2: Specify $B$ directly**\n",
    "```python\n",
    "B = pm.LKJCholeskyCov(\"B\", n=n_outputs, eta=2.0, sd_dist=pm.Exponential.dist(1.0))\n",
    "cov = pm.gp.cov.Coregion(input_dim=2, B=B, active_dims=[1])\n",
    "```\n",
    "\n",
    "This gives you direct control over the covariance matrix, typically using priors like LKJ that encourage specific correlation structures. However, you lose the interpretable low-rank structure.\n",
    "\n",
    "**Which to use?** \n",
    "- Use $W$ and $\\kappa$ when you want **interpretable low-rank structure** and can reasonably assume outputs share a small number of latent factors\n",
    "- Use $B$ directly when you need **maximum flexibility** or want to impose specific correlation structures via the prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lcm(input_dim, active_dims, num_outputs, kernels, W=None, kappa=None, B=None, name=\"LCM\"):\n",
    "    \"\"\"\n",
    "    Construct Linear Coregionalization Model kernel.\n",
    "    \n",
    "    Sums multiple ICM kernels.\n",
    "    \"\"\"\n",
    "    if B is None:\n",
    "        if kappa is None:\n",
    "            kappa = pm.Gamma(f\"{name}_kappa\", alpha=5, beta=1, shape=num_outputs)\n",
    "        if W is None:\n",
    "            W = pm.Normal(f\"{name}_W\", mu=0, sigma=5, shape=(num_outputs, 1))\n",
    "    else:\n",
    "        kappa = None\n",
    "    \n",
    "    # Sum ICMs\n",
    "    cov_lcm = 0\n",
    "    for kernel in kernels:\n",
    "        icm = get_icm(input_dim, kernel, W, kappa, B, active_dims)\n",
    "        cov_lcm += icm\n",
    "    \n",
    "    return cov_lcm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9ad74",
   "metadata": {},
   "source": [
    "This implementation reuses the same $W$ and $\\kappa$ across both kernels (shared coregionalization structure), but each kernel contributes its own input covariance shape. The ExpQuad kernel will capture smooth long-term trends, while the Mat√©rn32 kernel will capture rougher short-term wiggles.\n",
    "\n",
    "Let's see this in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as lcm_model:\n",
    "    # Two lengthscales\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=0.5, shape=2)\n",
    "    eta = pm.Gamma(\"eta\", alpha=3, beta=1, shape=2)\n",
    "    \n",
    "    # Two kernels\n",
    "    kernel_list = [\n",
    "        eta[0]**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ell[0], active_dims=[0]),\n",
    "        eta[1]**2 * pm.gp.cov.Matern32(input_dim=2, ls=ell[1], active_dims=[0])\n",
    "    ]\n",
    "    \n",
    "    # LCM kernel\n",
    "    cov_lcm = get_lcm(input_dim=2, active_dims=[1], num_outputs=n_outputs,\n",
    "                      kernels=kernel_list, name=\"LCM\")\n",
    "    \n",
    "    # GP\n",
    "    gp = pm.gp.Marginal(cov_func=cov_lcm)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=sigma)\n",
    "\n",
    "pm.model_to_graphviz(lcm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lcm_model:\n",
    "    trace_lcm = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        nuts_sampler=\"nutpie\",\n",
    "        random_seed=RANDOM_SEED,\n",
    "        chains=2,\n",
    "        target_accept=0.9\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7501ee",
   "metadata": {},
   "source": [
    "The LCM has more parameters than ICM (two lengthscales, two amplitudes) but maintains shared coregionalization. This added flexibility allows the model to capture both smooth seasonal trends and short-term game-to-game fluctuations‚Äîthough it comes at the cost of increased computational complexity and potential overfitting if data is limited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lcm_model:\n",
    "    f_pred_lcm = gp.conditional(\"f_pred\", X_test)\n",
    "    ppc_lcm = pm.sample_posterior_predictive(\n",
    "        trace_lcm,\n",
    "        var_names=[\"f_pred\"],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing LCM Multi-Output Predictions\n",
    "\n",
    "Just as we did for the ICM, let's visualize the LCM posterior predictions for each pitcher to compare how the multi-timescale model captures the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=5, cols=1,\n",
    "    subplot_titles=top_pitchers,\n",
    "    vertical_spacing=0.05,\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "for i, pitcher in enumerate(top_pitchers):\n",
    "    # Extract predictions for this output\n",
    "    f_pred_i = ppc_lcm.posterior_predictive[\"f_pred\"].isel(\n",
    "        f_pred_dim_0=slice(i*n_test, (i+1)*n_test)\n",
    "    )\n",
    "    \n",
    "    mean = f_pred_i.mean(dim=[\"chain\", \"draw\"]).values\n",
    "    lower = np.percentile(f_pred_i.values, 2.5, axis=(0,1))\n",
    "    upper = np.percentile(f_pred_i.values, 97.5, axis=(0,1))\n",
    "    \n",
    "    # HDI band\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=upper, line=dict(width=0),\n",
    "        showlegend=False, hoverinfo='skip'\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=lower, fill='tonexty',\n",
    "        line=dict(width=0), showlegend=False,\n",
    "        fillcolor='rgba(255,165,0,0.3)'  # Orange shade to distinguish from ICM\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Mean\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=mean, mode='lines',\n",
    "        line=dict(color='darkorange', width=2),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Training data\n",
    "    pitcher_data = df_train.filter(pl.col(\"pitcher_name\") == pitcher)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pitcher_data[\"game_date_idx\"].to_list(),\n",
    "        y=pitcher_data[\"avg_spin_rate_std\"].to_list(),\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=3),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days Since Season Start\", row=5, col=1)\n",
    "fig.update_layout(height=1000, title_text=\"LCM Posterior Predictions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LCM predictions show similar smoothing behavior to ICM but with potentially different dynamics due to the two-kernel structure. The ExpQuad kernel captures smooth long-term trends while the Mat√©rn32 kernel handles shorter-term variations.\n",
    "\n",
    "**Comparing ICM vs LCM visually**: Look for differences in how the models handle:\n",
    "- **Smoothness**: LCM may show slightly different smoothness characteristics due to the Mat√©rn32 component\n",
    "- **Uncertainty**: The credible intervals (shown in orange) might differ in width, especially in regions with sparse data\n",
    "- **Trend capture**: LCM's dual-kernel structure may pick up multi-scale patterns that ICM averages over\n",
    "\n",
    "The key question is whether this added modeling flexibility translates to meaningfully better predictions or if it introduces unnecessary complexity for this particular dataset. Notice that both models effectively smooth the noisy observations while quantifying uncertainty‚Äîthe choice between them often depends on whether multiple distinct timescales are evident in your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Use LCM?\n",
    "\n",
    "**Choose LCM over ICM when**:\n",
    "- Multiple distinct timescales are evident in your data\n",
    "- You suspect different processes drive variation at different frequencies\n",
    "- You have enough data to support additional kernel parameters\n",
    "\n",
    "**Stick with ICM if**:\n",
    "- One timescale dominates the signal\n",
    "- Data is limited and you need a simpler model\n",
    "- Interpretability of a single correlation matrix is paramount\n",
    "\n",
    "For our baseball application, we'll implement the **shared-B variant** of LCM using:\n",
    "- **ExpQuad kernel**: Smooth long-term seasonal trends\n",
    "- **Mat√©rn32 kernel**: Rougher short-term fluctuations (Mat√©rn32 is less smooth than ExpQuad)\n",
    "\n",
    "Each kernel will have its own lengthscale and amplitude, but they'll share the coregionalization matrix $B$. This gives us the flexibility to model multi-scale temporal variation while keeping the correlation structure interpretable and estimable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use LCM Over ICM\n",
    "\n",
    "**Use ICM when:**\n",
    "- Single timescale dominates\n",
    "- Simplicity is priority\n",
    "- Limited data\n",
    "\n",
    "**Use LCM when:**\n",
    "- Multiple timescales evident\n",
    "- Different correlations at different scales\n",
    "- Sufficient data for complexity\n",
    "\n",
    "For baseball, we might combine:\n",
    "- ExpQuad: smooth long-term trends\n",
    "- Matern32: short-term wiggles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
