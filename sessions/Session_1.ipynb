{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_1.ipynb)\n",
    "\n",
    "# Session 1: Introduction to Gaussian Processes and PyMC\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Understand what Bayesian non-parametric models are and how Gaussian processes fit into this framework\n",
    "- Connect the familiar concept of multivariate normal distributions to the more general idea of Gaussian processes\n",
    "- Learn the basics of PyMC and how to implement simple models\n",
    "- Understand the roles of mean and covariance functions in defining a GP's behavior\n",
    "- Build and fit your first GP model using real data\n",
    "\n",
    "## LLM-Assisted Exercises\n",
    "\n",
    "Throughout these notebooks, you'll notice sections marked with ðŸ¤–. These are **LLM-assisted exercises** where you'll practice using large language models (like ChatGPT, Claude, or your favorite coding assistant) to help you implement GP concepts. This approach reflects modern data science practice: knowing *what* you want to accomplish and *how* to verify it is often more important than memorizing every implementation detail.\n",
    "\n",
    "The exercises are designed to help you develop the skill of effectively communicating with AI assistantsâ€”a critical ability in today's data science workflow. You'll learn to write clear prompts, test implementations, and validate results, building both your GP expertise and your collaborative coding skills."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.1: Introduction and Setup\n",
    "\n",
    "### What Does \"Non-Parametric\" Mean in a GP Context?\n",
    "\n",
    "When we talk about Gaussian processes being \"non-parametric,\" we're not saying they have no parametersâ€”that would be confusing! Instead, we mean that GPs don't assume a fixed functional form with a finite number of parameters. \n",
    "\n",
    "Think about it this way: in linear regression, you commit to a straight line (or hyperplane) defined by a slope and intercept. You're making a strong assumption about the shape of your function before seeing the data. With a GP, you're defining a *distribution over functions*. The data then tells you which functions from this distribution are most plausible.\n",
    "\n",
    "This flexibility makes GPs incredibly powerful for modeling complex, unknown relationships. The \"parameters\" in a GP are actually hyperparameters that control properties like smoothness and lengthscaleâ€”they shape the space of possible functions rather than defining a single function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import arviz as az\n",
    "import plotly.io as pio\n",
    "from scipy import stats\n",
    "import platform\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Configure plotly for nice interactive plots\n",
    "pio.templates.default = \"plotly_white\"\n",
    "\n",
    "# Print versions to verify environment\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"\\nEnvironment ready! Let's explore Gaussian Processes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything should import cleanly and you should see version numbers printed above. If you encounter any import errors, make sure your environment is set up correctly with PyMC 5.16+, NumPy 2.x, and Polars 1.x.\n",
    "\n",
    "Notice that we're setting `RANDOM_SEED` right at the start. Reproducibility is crucial in Bayesian workflowsâ€”you want to be able to recreate your results exactly, especially when debugging or sharing your work with colleagues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.2: Bayesian Inference Primer\n",
    "\n",
    "Before we learn the PyMC API or dive into Gaussian processes, let's build intuition for the Bayesian framework itself. Understanding how we update beliefs with data is fundamental to everything that follows.\n",
    "\n",
    "### The Three-Step Bayesian Workflow\n",
    "\n",
    "Every Bayesian analysis follows the same three-step pattern:\n",
    "\n",
    "1. **Specify a probability model**: Assign probability distributions to all unknownsâ€”parameters, predictions, even missing data\n",
    "2. **Calculate the posterior distribution**: Update our beliefs by combining prior knowledge with observed data  \n",
    "3. **Check and interpret**: Validate the model and draw conclusions\n",
    "\n",
    "This workflow applies whether we're estimating a simple proportion or building complex Gaussian process models.\n",
    "\n",
    "### Bayes' Theorem: The Foundation\n",
    "\n",
    "At its heart, Bayesian inference is about updating beliefs with data. We start with prior knowledge (the prior), observe data (the likelihood), and combine them through multiplication to get updated knowledge (the posterior):\n",
    "\n",
    "$$P(\\theta | y) \\propto P(y | \\theta) P(\\theta)$$\n",
    "\n",
    "**Reading this equation aloud:**\n",
    "- **Posterior** âˆ **Likelihood** Ã— **Prior**\n",
    "- What we believe about $\\theta$ after seeing data is proportional to how likely the data would be under different $\\theta$ values, weighted by what we believed before\n",
    "\n",
    "This seemingly simple equation is incredibly powerful. Think of it visually: if your prior was a curve and the likelihood was another curve, the posterior is their productâ€”peaks where both agree, valleys where they disagree.\n",
    "\n",
    "The proportionality constant is just a normalization factor ensuring probabilities sum to oneâ€”important mathematically, but not essential for building intuition.\n",
    "\n",
    "### A Concrete Example: Estimating a Proportion\n",
    "\n",
    "Let's work through a simple analytical example to see Bayesian updating in action. Imagine we want to estimate the probability $\\theta$ that a coin lands heads.\n",
    "\n",
    "**Setup**: We flip a coin 5 times and observe 3 heads. What can we infer about $\\theta$?\n",
    "\n",
    "**Step 1: Specify the model**\n",
    "- **Likelihood** (Binomial): Number of heads follows $\\text{Binomial}(n=5, p=\\theta)$\n",
    "- **Prior** (Beta): We start with a uniform prior, $\\text{Beta}(1, 1)$â€”maximum uncertainty about $\\theta$\n",
    "\n",
    "**Step 2: Compute the posterior**\n",
    "\n",
    "The beautiful thing about the Beta-Binomial combination is that we can calculate the posterior analyticallyâ€”no fancy algorithms needed! This is called *conjugacy*.\n",
    "\n",
    "The posterior is $\\text{Beta}(1+3, 1+2) = \\text{Beta}(4, 3)$. Notice how the parameters simply add:\n",
    "- Prior \"successes\": 1 -> Posterior: 1 + 3 = 4  \n",
    "- Prior \"failures\": 1 -> Posterior: 1 + 2 = 3\n",
    "\n",
    "Let's visualize this updating process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup: 3 heads from 5 flips\n",
    "n, y = 5, 3\n",
    "theta = np.linspace(0, 1, 200)\n",
    "\n",
    "# Prior: Beta(1, 1) = Uniform\n",
    "prior = stats.beta.pdf(theta, 1, 1)\n",
    "\n",
    "# Likelihood: Binomial (normalized for visibility)\n",
    "likelihood = theta**y * (1-theta)**(n-y)\n",
    "likelihood = likelihood / likelihood.max()\n",
    "\n",
    "# Posterior: Beta(1+y, 1+n-y) = Beta(4, 3)\n",
    "posterior = stats.beta.pdf(theta, 1+y, 1+n-y)\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=theta, y=prior, name='Prior',\n",
    "                         mode='lines', line=dict(color='gray', width=2, dash='dash')))\n",
    "fig.add_trace(go.Scatter(x=theta, y=likelihood, name='Likelihood (normalized)',\n",
    "                         mode='lines', line=dict(color='blue', width=2)))\n",
    "fig.add_trace(go.Scatter(x=theta, y=posterior, name='Posterior',\n",
    "                         mode='lines', line=dict(color='red', width=3)))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Bayesian Updating: Prior Ã— Likelihood = Posterior',\n",
    "    xaxis_title='Î¸ (probability of heads)',\n",
    "    yaxis_title='Probability Density',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Posterior mean\n",
    "posterior_mean = (1 + y) / (1 + 1 + n)\n",
    "print(f\"\\nPosterior mean: {posterior_mean:.3f}\")\n",
    "print(f\"This is between our prior mean (0.5) and the observed proportion ({y/n:.3f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the beautiful interplay:\n",
    "\n",
    "- The **prior** (gray dashed) is flatâ€”we started with no preference for any value of $\\theta$\n",
    "- The **likelihood** (blue) peaks at the observed proportion (3/5 = 0.6)â€”this is what the data alone suggests\n",
    "- The **posterior** (red) is their product, peaked near the likelihood but slightly regularized by the prior\n",
    "\n",
    "With only 5 flips, there's still substantial uncertainty. Let's see what happens as we collect more data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how posterior narrows with more data (keeping proportion at 60%)\n",
    "sample_sizes = [5, 20, 100]\n",
    "observed_proportion = 0.6\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "for n in sample_sizes:\n",
    "    y = int(n * observed_proportion)\n",
    "    posterior = stats.beta.pdf(theta, 1+y, 1+n-y)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=theta, y=posterior,\n",
    "        name=f'n={n}, y={y}',\n",
    "        mode='lines', line=dict(width=2)\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Posterior Distribution Narrows with More Data',\n",
    "    xaxis_title='Î¸ (probability of heads)',\n",
    "    yaxis_title='Posterior Density',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This progression beautifully illustrates Bayesian learning:\n",
    "\n",
    "- With **5 observations**, the posterior is wideâ€”we're still quite uncertain\n",
    "- With **20 observations**, uncertainty decreases substantially  \n",
    "- With **100 observations**, the posterior becomes very sharpâ€”we're confident $\\theta$ is near 0.6\n",
    "\n",
    "Notice that the posterior mean is always a weighted average of the prior and the data, with the data getting more weight as sample size grows. This is Bayesian learning in actionâ€”prior beliefs get updated, and eventually overwhelmed, by evidence.\n",
    "\n",
    "### Why This Matters for Gaussian Processes\n",
    "\n",
    "This same Bayesian framework powers GP modeling. Instead of updating beliefs about a single parameter $\\theta$, we'll update beliefs about entire *functions*. The prior becomes a distribution over functions (specified by mean and covariance), the likelihood describes how data relates to function values, and the posterior gives us updated beliefs about which functions are plausible.\n",
    "\n",
    "But first, we need to learn how to implement Bayesian models in PyMC. Let's dive into the API with a real dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.3: Introduction to PyMC with Real Data\n",
    "\n",
    "Now that we understand the Bayesian framework conceptually, let's learn how to implement it using PyMC. We'll start with the simplest possible model: estimating the distribution of a single continuous variable.\n",
    "\n",
    "### The Data: Baseball Launch Angles\n",
    "\n",
    "We'll use real baseball data from the `fastball_bat_angles.csv` dataset. When a batter makes contact with a fastball, the ball leaves the bat at a particular **launch angle**â€”the vertical angle relative to horizontal. Launch angle is crucial for hitting outcomes: too low and you hit a ground ball, too high and you pop out, just right and you might hit a home run.\n",
    "\n",
    "Let's focus on a single player and estimate the distribution of their launch angles. We'll use this to learn PyMC syntax before moving to more complex models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fastball bat angle data\n",
    "df = pl.read_csv(DATA_DIR + 'fastball_bat_angles.csv')\n",
    "\n",
    "# See what batters we have\n",
    "print(\"Sample of available batters:\")\n",
    "print(df.select('batter_name').unique().head(10))\n",
    "\n",
    "# Let's focus on Bryce Harper\n",
    "harper = df.filter(pl.col('batter_name') == 'Harper, Bryce')\n",
    "print(f\"\\nBryce Harper: {harper.shape[0]} fastball contacts\")\n",
    "\n",
    "# Extract launch angles as numpy array\n",
    "launch_angles = harper['launch_angle'].to_numpy()\n",
    "\n",
    "# Visualize the distribution\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=launch_angles,\n",
    "    nbinsx=30,\n",
    "    name='Observed Launch Angles',\n",
    "    marker_color='steelblue'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Bryce Harper: Launch Angle Distribution',\n",
    "    xaxis_title='Launch Angle (degrees)',\n",
    "    yaxis_title='Count',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nSummary statistics:\")\n",
    "print(f\"Mean: {launch_angles.mean():.1f}Â°\")\n",
    "print(f\"Std Dev: {launch_angles.std():.1f}Â°\")\n",
    "print(f\"Range: [{launch_angles.min():.1f}Â°, {launch_angles.max():.1f}Â°]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution looks roughly bell-shaped, though perhaps not perfectly symmetric. A normal distribution seems like a reasonable starting point.\n",
    "\n",
    "### Building Your First PyMC Model\n",
    "\n",
    "Let's estimate the mean and standard deviation of Harper's launch angle distribution. The model is simple:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\mu &\\sim \\text{Normal}(30, 20) \\\\\n",
    "\\sigma &\\sim \\text{HalfNormal}(20) \\\\\n",
    "y_i &\\sim \\text{Normal}(\\mu, \\sigma)\n",
    "\\end{aligned}$$\n",
    "\n",
    "We're saying:\n",
    "- The mean launch angle $\\mu$ is probably around 30Â° (that's a good launch angle), but we're quite uncertain (SD of 20Â°)\n",
    "- The standard deviation $\\sigma$ is positive, probably not hugeâ€”maybe around 20Â° or less\n",
    "- Each observed launch angle is drawn from a normal distribution with these parameters\n",
    "\n",
    "Here's how to write this in PyMC:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as launch_model:\n",
    "    # Priors\n",
    "    mu = pm.Normal('mu', mu=30, sigma=20)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=20)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y', mu=mu, sigma=sigma, observed=launch_angles)\n",
    "    \n",
    "    # Sample from the posterior\n",
    "    trace = pm.sample(1000, tune=1000, nuts_sampler='nutpie',\n",
    "                      random_seed=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's unpack what just happened:\n",
    "\n",
    "1. `with pm.Model() as launch_model:` creates a model context. All variables defined inside this block are part of the model.\n",
    "\n",
    "2. `pm.Normal('mu', mu=30, sigma=20)` defines our prior for the mean. The first argument is the variable name, then come the parameters.\n",
    "\n",
    "3. `pm.HalfNormal('sigma', sigma=20)` defines our prior for the standard deviation. HalfNormal is like Normal but constrained to be positiveâ€”perfect for scale parameters.\n",
    "\n",
    "4. `pm.Normal('y', mu=mu, sigma=sigma, observed=launch_angles)` defines the likelihood. The `observed=` argument connects the model to our actual data.\n",
    "\n",
    "5. `pm.sample()` runs MCMC sampling to approximate the posterior distribution. We'll learn more about this laterâ€”for now, just know it's finding the parameter values most consistent with our data and priors.\n",
    "\n",
    "Now let's examine what we learned:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"Posterior Summary:\")\n",
    "print(az.summary(trace, var_names=['mu', 'sigma']))\n",
    "\n",
    "# Visualize posteriors\n",
    "fig = az.plot_posterior(trace, var_names=['mu', 'sigma'])\n",
    "print(\"\\n(See posterior plots above)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior tells us what we've learned about Harper's launch angle distribution:\n",
    "\n",
    "- The **posterior mean** for $\\mu$ is our best estimate of Harper's average launch angle\n",
    "- The **94% HDI** (Highest Density Interval) tells us the range of plausible valuesâ€”there's a 94% chance the true mean is in this interval\n",
    "- Notice how much narrower the posterior is compared to our priorâ€”the data has substantially reduced our uncertainty\n",
    "\n",
    "### Posterior Predictive Checks\n",
    "\n",
    "A crucial part of the Bayesian workflow is asking: \"If this model is correct, would it generate data that looks like what we actually observed?\" This is called a **posterior predictive check**.\n",
    "\n",
    "We'll use the posterior distributions of $\\mu$ and $\\sigma$ to simulate new datasets and compare them to our observed data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with launch_model:\n",
    "    # Sample from the posterior predictive distribution\n",
    "    trace.extend(pm.sample_posterior_predictive(trace, random_seed=rng))\n",
    "\n",
    "# Use ArviZ's built-in posterior predictive check plot\n",
    "az.plot_ppc(trace, num_pp_samples=100);\n",
    "print(\"\\n(Blue: observed data, Orange: posterior predictive samples)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The posterior predictive samples (orange) overlay nicely with the observed data (blue), suggesting our simple normal model is a reasonable approximation. If there were systematic discrepanciesâ€”say, the observed data had heavy tails that the model couldn't captureâ€”we'd see clear differences and might consider a different likelihood (like Student's t).\n",
    "\n",
    "This workflowâ€”specify model, fit, checkâ€”is fundamental to Bayesian modeling and will be our pattern throughout this workshop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE 1: Compare Launch Angles Across Batters\n",
    "\n",
    "Now that you've seen the basic workflow, practice it with different players."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help complete this comparison\n",
    "\n",
    "def compare_batters(batter_names, df):\n",
    "    \"\"\"\n",
    "    Fit launch angle models for multiple batters and compare posteriors.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    batter_names : list of str\n",
    "        Names of batters to compare (e.g., ['Harper, Bryce', 'Alonso, Pete'])\n",
    "    df : polars DataFrame\n",
    "        The fastball bat angles dataset\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    results : dict\n",
    "        Dictionary mapping batter names to their trace objects\n",
    "        \n",
    "    Prompt suggestion: \"Help me write a function that fits the same PyMC launch angle\n",
    "    model (Normal likelihood with Normal and HalfNormal priors) for multiple batters,\n",
    "    stores the traces, and creates a forest plot comparing the posterior mean launch\n",
    "    angles across batters using ArviZ.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test with a few batters\n",
    "# batters = ['Harper, Bryce', 'Alonso, Pete', 'Tucker, Kyle']\n",
    "# results = compare_batters(batters, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.4: Multivariate Normal Models with Coordinates\n",
    "\n",
    "So far we've modeled single variables in isolation. But often we have multiple related measurementsâ€”like attack angle and launch angle in baseballâ€”that vary together. A **multivariate normal distribution** lets us model the joint distribution of multiple variables, capturing how they covary.\n",
    "\n",
    "This is also where PyMC's **coordinate system** shines. Coordinates (`coords`) let us:\n",
    "- Give meaningful names to dimensions (like \"variable\" or \"observation\")\n",
    "- Make models more readable and easier to extend\n",
    "- Enable advanced features like `pm.Data()` for out-of-sample prediction\n",
    "\n",
    "### Attack Angle and Launch Angle\n",
    "\n",
    "When a batter swings, two angles matter:\n",
    "- **Attack angle**: The vertical angle of the bat's path as it moves through the hitting zone\n",
    "- **Launch angle**: The vertical angle of the ball as it leaves the bat\n",
    "\n",
    "Intuitively, these should be relatedâ€”if you swing with a more upward bat path (higher attack angle), you're more likely to hit the ball upward (higher launch angle). Let's model their joint distribution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to observations with BOTH attack_angle and launch_angle\n",
    "df_both = df.filter(pl.col('attack_angle').is_not_null())\n",
    "\n",
    "print(f\"Observations with both angles: {df_both.shape[0]}\")\n",
    "print(f\"Original dataset: {df.shape[0]}\")\n",
    "print(f\"Fraction with attack angle data: {df_both.shape[0]/df.shape[0]:.1%}\")\n",
    "\n",
    "# Stack into a (n_obs, 2) array\n",
    "angles = np.column_stack([\n",
    "    df_both['attack_angle'].to_numpy(),\n",
    "    df_both['launch_angle'].to_numpy()\n",
    "])\n",
    "\n",
    "print(f\"\\nData shape: {angles.shape}\")\n",
    "print(f\"Attack angle range: [{angles[:, 0].min():.1f}, {angles[:, 0].max():.1f}]\")\n",
    "print(f\"Launch angle range: [{angles[:, 1].min():.1f}, {angles[:, 1].max():.1f}]\")\n",
    "\n",
    "# Visualize the joint distribution\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=angles[:, 0],\n",
    "    y=angles[:, 1],\n",
    "    mode='markers',\n",
    "    marker=dict(size=3, opacity=0.3, color='steelblue'),\n",
    "    name='Observed'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Joint Distribution of Attack Angle and Launch Angle',\n",
    "    xaxis_title='Attack Angle (degrees)',\n",
    "    yaxis_title='Launch Angle (degrees)',\n",
    "    width=600, height=500\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a clear positive relationship! Higher attack angles tend to produce higher launch angles, though there's substantial scatter.\n",
    "\n",
    "### Building a Multivariate Normal Model\n",
    "\n",
    "A bivariate normal distribution is characterized by:\n",
    "- A **mean vector** $\\boldsymbol{\\mu} = [\\mu_{\\text{attack}}, \\mu_{\\text{launch}}]$\n",
    "- A **covariance matrix** $\\boldsymbol{\\Sigma}$ that captures variances and covariance:\n",
    "\n",
    "$$\\boldsymbol{\\Sigma} = \\begin{bmatrix}\n",
    "\\sigma_{\\text{attack}}^2 & \\rho \\sigma_{\\text{attack}} \\sigma_{\\text{launch}} \\\\\n",
    "\\rho \\sigma_{\\text{attack}} \\sigma_{\\text{launch}} & \\sigma_{\\text{launch}}^2\n",
    "\\end{bmatrix}$$\n",
    "\n",
    "where $\\rho$ is the correlation coefficient.\n",
    "\n",
    "In PyMC, we'll use the **LKJ prior** for the correlation matrixâ€”it's a flexible prior that doesn't favor any particular correlation structure. We'll also use coordinates to make the model readable:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coordinates\n",
    "COORDS = {\n",
    "    'variable': ['attack_angle', 'launch_angle'],\n",
    "    'obs': np.arange(angles.shape[0])\n",
    "}\n",
    "\n",
    "with pm.Model(coords=COORDS) as mvn_model:\n",
    "    # Mean vector (one mean per variable)\n",
    "    mu = pm.Normal('mu', mu=0, sigma=30, dims='variable')\n",
    "    \n",
    "    # Correlation matrix with LKJ prior (eta=2 is weakly informative)\n",
    "    # Standard deviations for each variable\n",
    "    chol, corr, stds = pm.LKJCholeskyCov(\n",
    "        'chol', n=2, eta=2.0, \n",
    "        sd_dist=pm.HalfNormal.dist(sigma=20, shape=2),\n",
    "        compute_corr=True\n",
    "    )\n",
    "    \n",
    "    # Multivariate normal likelihood\n",
    "    pm.MvNormal('angles', mu=mu, chol=chol,\n",
    "                      observed=angles, dims=('obs', 'variable'))\n",
    "    \n",
    "    # Sample\n",
    "    trace_mvn = pm.sample(1000, tune=1000, nuts_sampler='nutpie',\n",
    "                          random_seed=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine what we learned about the means, standard deviations, and correlation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table\n",
    "print(\"Posterior Summary:\")\n",
    "print(az.summary(trace_mvn, var_names=['mu', 'chol_stds', 'chol_corr']))\n",
    "\n",
    "# Visualize mean and standard deviation estimates\n",
    "az.plot_posterior(trace_mvn, var_names=['mu', 'chol_stds']);\n",
    "print(\"\\n(See posterior distributions for means and standard deviations above)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize the correlation\n",
    "corr_samples = trace_mvn.posterior['chol_corr'].values\n",
    "# Correlation is the off-diagonal element [0,1]\n",
    "rho = corr_samples[:, :, 0, 1].flatten()\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=rho,\n",
    "    nbinsx=40,\n",
    "    name='Correlation',\n",
    "    marker_color='coral'\n",
    "))\n",
    "fig.update_layout(\n",
    "    title='Posterior Distribution of Correlation between Attack Angle and Launch Angle',\n",
    "    xaxis_title='Correlation (Ï)',\n",
    "    yaxis_title='Count',\n",
    "    showlegend=False\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(f\"\\nPosterior mean correlation: {rho.mean():.3f}\")\n",
    "print(f\"95% credible interval: [{np.percentile(rho, 2.5):.3f}, {np.percentile(rho, 97.5):.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The positive correlation confirms our intuition: attack angle and launch angle are positively related. When batters swing upward, they tend to hit the ball upward.\n",
    "\n",
    "### Why Use Coordinates?\n",
    "\n",
    "Notice how `dims='variable'` makes the model self-documenting. When we look at posterior summaries, we see `mu[attack_angle]` and `mu[launch_angle]` instead of `mu[0]` and `mu[1]`. This readability becomes crucial in larger models.\n",
    "\n",
    "The `pm.Data()` container (which we could add) would let us:\n",
    "- Update the data without rebuilding the model\n",
    "- Make predictions on new observations\n",
    "- Perform leave-one-out cross-validation\n",
    "\n",
    "We'll use these features extensively when building GP models in Session 2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE 2: Posterior Predictive Check for Multivariate Model\n",
    "\n",
    "Verify that the bivariate normal model captures the joint distribution well by generating posterior predictive samples and comparing to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help create a posterior predictive check\n",
    "\n",
    "def mvn_posterior_predictive_check(trace_mvn, angles):\n",
    "    \"\"\"\n",
    "    Generate posterior predictive samples and visualize against observed data.\n",
    "    \n",
    "    Create a scatter plot with:\n",
    "    - Observed (attack_angle, launch_angle) points in blue\n",
    "    - A few posterior predictive samples in red/orange with transparency\n",
    "    \n",
    "    Prompt suggestion: \"Help me sample from the posterior predictive distribution\n",
    "    of the bivariate normal model using pm.sample_posterior_predictive(), then\n",
    "    create a Plotly scatter plot overlaying observed data and 3-5 posterior\n",
    "    predictive samples to visually check model fit.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# Uncomment to test:\n",
    "# mvn_posterior_predictive_check(trace_mvn, angles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.5: From Multivariate Normals to Gaussian Processes\n",
    "\n",
    "### Building Intuition: Functions as Infinite-Dimensional Vectors\n",
    "\n",
    "Here's the key insight that unlocks Gaussian processes: if you squint just right, a function is just a really long vectorâ€”infinitely long, in fact. At every point $x$ in the input space, the function has a value $f(x)$. You can think of $f$ as an infinite-dimensional vector, with one entry for each possible $x$.\n",
    "\n",
    "Now, you already know what a multivariate normal (MVN) distribution isâ€”it's a probability distribution over vectors. A Gaussian process is simply the natural extension of an MVN distribution to this infinite-dimensional case. It's a distribution over *functions*.\n",
    "\n",
    "The magic is that we can work with GPs using only finite-dimensional operations. We never actually deal with infinityâ€”we only care about the function values at the specific points where we have (or want) observations. Let's make this concrete.\n",
    "\n",
    "### Drawing Functions from a GP Prior\n",
    "\n",
    "A Gaussian process is completely specified by two functions:\n",
    "- A **mean function** $m(x)$ that gives the expected value at each point\n",
    "- A **covariance function** (or kernel) $k(x, x')$ that describes how function values at different points relate to each other\n",
    "\n",
    "We write: $f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$\n",
    "\n",
    "Let's implement a simple GP prior using the squared exponential (also called RBF or ExpQuad) kernel and draw some functions from it. First, we'll implement the kernel function ourselves to build intuition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE 3: Implement the ExpQuad Kernel\n",
    "\n",
    "Let's start by implementing the exponential quadratic (RBF) kernel from scratch. This exercise will help you understand what a covariance function actually does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE 3: Use your LLM to help complete this implementation\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def exp_quad_kernel(X1, X2, lengthscale=1.0, variance=1.0):\n",
    "    \"\"\"\n",
    "    Compute the ExpQuad (RBF/Squared Exponential) covariance matrix.\n",
    "    \n",
    "    The ExpQuad kernel is: k(x, x') = variance * exp(-||x - x'||^2 / (2 * lengthscale^2))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X1 : array-like, shape (n_samples_1, n_features)\n",
    "        First input matrix\n",
    "    X2 : array-like, shape (n_samples_2, n_features)  \n",
    "        Second input matrix\n",
    "    lengthscale : float\n",
    "        Length scale parameter - controls how quickly correlation decays with distance\n",
    "    variance : float\n",
    "        Variance parameter - controls overall scale of function values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    K : array, shape (n_samples_1, n_samples_2)\n",
    "        Covariance matrix\n",
    "    \n",
    "    Prompt suggestion: \"Help me implement an RBF/ExpQuad kernel that returns the pairwise\n",
    "    covariance matrix k(x, x') = sigma^2 * exp(-||x - x'||^2 / (2 * ell^2)) using NumPy 2.\n",
    "    The inputs X1 and X2 should be 2D arrays where each row is a data point.\"\n",
    "    \"\"\"\n",
    "    # REFERENCE SOLUTION (for instructors):\n",
    "    # Compute squared Euclidean distances\n",
    "    # For each pair (i,j), we need ||X1[i] - X2[j]||^2\n",
    "    X1_sq = np.sum(X1**2, axis=1, keepdims=True)  # shape (n1, 1)\n",
    "    X2_sq = np.sum(X2**2, axis=1, keepdims=True)  # shape (n2, 1)\n",
    "    distances_sq = X1_sq + X2_sq.T - 2 * X1 @ X2.T  # shape (n1, n2)\n",
    "    \n",
    "    # Apply RBF kernel formula\n",
    "    K = variance * np.exp(-distances_sq / (2 * lengthscale**2))\n",
    "    return K\n",
    "\n",
    "# STEP 2: Test your implementation\n",
    "# Create a simple test case\n",
    "X_test = np.array([[0.], [1.], [2.]])\n",
    "K_test = exp_quad_kernel(X_test, X_test, lengthscale=1.0, variance=1.0)\n",
    "\n",
    "print(\"Covariance matrix:\")\n",
    "print(K_test)\n",
    "print(\"\\nProperties to check:\")\n",
    "print(f\"1. Symmetric? {np.allclose(K_test, K_test.T)}\")\n",
    "print(f\"2. Diagonal values (should be ~{1.0}): {np.diag(K_test)}\")\n",
    "print(f\"3. Positive semi-definite? {np.all(np.linalg.eigvals(K_test) >= -1e-10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize this kernel as a heatmap to understand what it's doing. The kernel matrix tells us how correlated function values are at different input locations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a grid of points\n",
    "X_grid = np.linspace(0, 10, 50)[:, None]\n",
    "K_grid = exp_quad_kernel(X_grid, X_grid, lengthscale=1.5, variance=1.0)\n",
    "\n",
    "# Visualize the covariance matrix\n",
    "fig = px.imshow(K_grid, \n",
    "                x=X_grid.flatten(),\n",
    "                y=X_grid.flatten(),\n",
    "                color_continuous_scale='Viridis',\n",
    "                labels=dict(x=\"x\", y=\"x'\", color=\"Covariance\"),\n",
    "                title=\"ExpQuad Kernel: How function values covary\")\n",
    "fig.update_layout(width=600, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the structure: the diagonal is bright (variance = 1.0), meaning each point is perfectly correlated with itself. As you move away from the diagonal, the correlation decays smoothly. This decay rate is controlled by the lengthscale parameterâ€”it determines how far apart two points can be before they become essentially uncorrelated.\n",
    "\n",
    "Now let's use this kernel to draw actual functions from a GP prior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input points where we want to evaluate the function\n",
    "X = np.linspace(0, 10, 100)[:, None]\n",
    "\n",
    "# Build the covariance matrix\n",
    "lengthscale = 1.5\n",
    "variance = 1.0\n",
    "K = exp_quad_kernel(X, X, lengthscale=lengthscale, variance=variance)\n",
    "\n",
    "# Add small jitter for numerical stability\n",
    "K += 1e-8 * np.eye(len(X))\n",
    "\n",
    "# Zero mean function (we'll explore non-zero means later)\n",
    "mean = np.zeros(len(X))\n",
    "\n",
    "# Draw 5 sample functions from the GP prior\n",
    "n_samples = 5\n",
    "f_samples = rng.multivariate_normal(mean, K, size=n_samples)\n",
    "\n",
    "# Plot the sample functions\n",
    "fig = go.Figure()\n",
    "for i in range(n_samples):\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X.flatten(),\n",
    "        y=f_samples[i],\n",
    "        mode='lines',\n",
    "        name=f'Sample {i+1}',\n",
    "        line=dict(width=2),\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'Functions Drawn from GP Prior (lengthscale={lengthscale}, variance={variance})',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    showlegend=True\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are functions drawn from our GP prior! Each colored line represents one possible function that the GP considers plausible before seeing any data. Notice how smooth they areâ€”that smoothness comes from the ExpQuad kernel. \n",
    "\n",
    "Here's the beautiful thing: we just drew these functions by sampling from a multivariate normal distribution. The connection is direct:\n",
    "- The mean vector of the MVN is our mean function evaluated at our input points\n",
    "- The covariance matrix of the MVN is our kernel evaluated at all pairs of input points\n",
    "\n",
    "### Understanding Lengthscale and Variance\n",
    "\n",
    "Let's explore how the kernel parameters affect the functions we draw:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lengthscales\n",
    "lengthscales = [0.5, 1.5, 3.0]\n",
    "\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    subplot_titles=[f'lengthscale = {l}' for l in lengthscales])\n",
    "\n",
    "for idx, ls in enumerate(lengthscales, 1):\n",
    "    K = exp_quad_kernel(X, X, lengthscale=ls, variance=1.0)\n",
    "    K += 1e-8 * np.eye(len(X))\n",
    "    \n",
    "    samples = rng.multivariate_normal(np.zeros(len(X)), K, size=3)\n",
    "    \n",
    "    for sample in samples:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=X.flatten(), y=sample, mode='lines',\n",
    "                      showlegend=False, line=dict(width=1.5)),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_yaxes(title_text=\"f(x)\", col=1)\n",
    "fig.update_layout(height=300, width=900,\n",
    "                  title_text=\"Effect of Lengthscale on Function Smoothness\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lengthscale parameter controls the \"wiggliness\" of functions:\n",
    "- **Small lengthscale** (left): Functions vary rapidly, with high-frequency wiggles\n",
    "- **Medium lengthscale** (middle): Smooth functions with moderate variation\n",
    "- **Large lengthscale** (right): Very smooth, slowly varying functions\n",
    "\n",
    "Think of lengthscale as answering the question: \"How far do I need to move in input space before function values become uncorrelated?\" This is one of the most important hyperparameters you'll tune when fitting GPs to real data.\n",
    "\n",
    "The variance parameter, on the other hand, controls the typical distance of functions from the mean function (in this case, zero). Larger variance means larger excursions from the mean.\n",
    "\n",
    "We've just connected multivariate normals to Gaussian processes! A GP is simply a generalization of the MVN to infinite dimensions, and we can work with it by considering only the finite set of points we care about. This is the key insight that makes GPs practical."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.6: Mean Functions\n",
    "\n",
    "### The Role of the Mean Function\n",
    "\n",
    "So far, we've been using a zero mean function, but this is just one choice among many. The mean function $m(x)$ specifies the expected value of the function at each input point, before we see any data. Think of it as your baseline assumption about what the function looks like.\n",
    "\n",
    "The mean function shifts functions up or down (or even gives them a trend), but it doesn't affect their smoothness or correlation structureâ€”that's the job of the covariance function. In many applications, we stick with a zero mean because:\n",
    "1. It's simple and introduces no additional parameters\n",
    "2. The covariance function is usually flexible enough to capture the important structure\n",
    "3. If we're working with standardized data (mean-centered), a zero mean function makes sense\n",
    "\n",
    "However, when you have strong prior knowledge about the general shape of your functionâ€”maybe you know it should have a linear trend or oscillate around some valueâ€”encoding this in the mean function can improve your model.\n",
    "\n",
    "Let's explore the three most common mean functions: zero, constant, and linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our input points\n",
    "X_mean = np.linspace(0, 10, 100)[:, None]\n",
    "n_samples = 3\n",
    "\n",
    "# Shared covariance (we'll use the same kernel for all)\n",
    "K = exp_quad_kernel(X_mean, X_mean, lengthscale=1.5, variance=1.0)\n",
    "K += 1e-8 * np.eye(len(X_mean))\n",
    "\n",
    "# Three different mean functions\n",
    "mean_zero = np.zeros(len(X_mean))\n",
    "mean_constant = np.full(len(X_mean), 5.0)  # Constant offset of 5\n",
    "mean_linear = 0.5 * X_mean.flatten() + 2.0  # Linear trend\n",
    "\n",
    "# Create subplots\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    subplot_titles=['Zero Mean', 'Constant Mean (5.0)', 'Linear Mean (0.5x + 2)'])\n",
    "\n",
    "for idx, (mean_func, title) in enumerate([\n",
    "    (mean_zero, 'Zero'),\n",
    "    (mean_constant, 'Constant'),\n",
    "    (mean_linear, 'Linear')\n",
    "], 1):\n",
    "    # Draw samples\n",
    "    samples = rng.multivariate_normal(mean_func, K, size=n_samples)\n",
    "    \n",
    "    # Plot mean function\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_mean.flatten(), y=mean_func, mode='lines',\n",
    "                  line=dict(color='black', width=3, dash='dash'),\n",
    "                  showlegend=False, name='Mean'),\n",
    "        row=1, col=idx\n",
    "    )\n",
    "    \n",
    "    # Plot samples\n",
    "    for sample in samples:\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=X_mean.flatten(), y=sample, mode='lines',\n",
    "                      showlegend=False, line=dict(width=1.5), opacity=0.7),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_yaxes(title_text=\"f(x)\", col=1)\n",
    "fig.update_layout(height=300, width=900,\n",
    "                  title_text=\"GP Prior Samples with Different Mean Functions (black dashed = mean)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the mean function (black dashed line) acts as an \"anchor\" around which the random functions vary:\n",
    "\n",
    "- **Zero mean** (left): Functions wander around zero, with equal probability of being positive or negative\n",
    "- **Constant mean** (middle): Functions are shifted up to fluctuate around 5.0, but remain roughly constant on average\n",
    "- **Linear mean** (right): Functions inherit the upward trend, but with wiggles around that trend\n",
    "\n",
    "The key insight: the covariance function determines the smoothness and correlation structure (how wiggly the functions are), while the mean function determines where they're centered or whether they have a systematic trend. They work together, but control different aspects of the prior.\n",
    "\n",
    "### When to Use Each Mean Function\n",
    "\n",
    "Here's a practical guide:\n",
    "\n",
    "- **Zero mean**: Default choice, especially with standardized data or when you have no strong prior beliefs\n",
    "- **Constant mean**: When you expect the function to fluctuate around some non-zero value (e.g., temperature anomalies around a baseline)\n",
    "- **Linear mean**: When you have a clear trend in your data and want the GP to model deviations from that trend\n",
    "\n",
    "In PyMC, you can easily specify these using built-in mean functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyMC makes it easy to specify mean functions\n",
    "mean_zero_pymc = pm.gp.mean.Zero()  # Default\n",
    "mean_const_pymc = pm.gp.mean.Constant(c=5.0)\n",
    "mean_linear_pymc = pm.gp.mean.Linear(coeffs=0.5, intercept=2.0)\n",
    "\n",
    "# Test them\n",
    "X_test = np.array([[0.], [5.], [10.]])\n",
    "print(\"Zero mean:\", mean_zero_pymc(X_test).eval())\n",
    "print(\"Constant mean:\", mean_const_pymc(X_test).eval())\n",
    "print(\"Linear mean:\", mean_linear_pymc(X_test).eval())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE 4: Compare Different Mean Functions\n",
    "\n",
    "Now let's explore how different mean functions affect GP prior draws more systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE 3: Use your LLM to help complete this implementation\n",
    "\n",
    "def compare_mean_functions(X, n_samples=5, lengthscale=1.5, variance=1.0):\n",
    "    \"\"\"\n",
    "    Create comparison plots of GP priors with different mean functions.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array, shape (n_points, 1)\n",
    "        Input locations\n",
    "    n_samples : int\n",
    "        Number of function samples to draw from each GP\n",
    "    lengthscale : float\n",
    "        Kernel lengthscale\n",
    "    variance : float\n",
    "        Kernel variance\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    fig : plotly Figure\n",
    "        Comparison plot\n",
    "    \n",
    "    Prompt suggestion: \"Help me write a function that compares GP prior samples using \n",
    "    three different mean functions: zero, constant (value=5), and linear (slope=0.5, intercept=2).\n",
    "    Use the exp_quad_kernel defined earlier and create side-by-side Plotly subplots showing\n",
    "    samples from each GP. Draw the mean function as a dashed black line and samples as colored lines.\"\n",
    "    \"\"\"\n",
    "    # REFERENCE SOLUTION:\n",
    "    K = exp_quad_kernel(X, X, lengthscale=lengthscale, variance=variance)\n",
    "    K += 1e-8 * np.eye(len(X))\n",
    "    \n",
    "    means = {\n",
    "        'Zero': np.zeros(len(X)),\n",
    "        'Constant (5.0)': np.full(len(X), 5.0),\n",
    "        'Linear (0.5x + 2)': 0.5 * X.flatten() + 2.0\n",
    "    }\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=list(means.keys()))\n",
    "    \n",
    "    for idx, (name, mean_func) in enumerate(means.items(), 1):\n",
    "        samples = rng.multivariate_normal(mean_func, K, size=n_samples)\n",
    "        \n",
    "        # Plot mean\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=X.flatten(), y=mean_func, mode='lines',\n",
    "                      line=dict(color='black', width=2, dash='dash'),\n",
    "                      name='Mean', showlegend=(idx==1)),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "        \n",
    "        # Plot samples\n",
    "        for i, sample in enumerate(samples):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=X.flatten(), y=sample, mode='lines',\n",
    "                          name=f'Sample {i+1}' if idx==1 else None,\n",
    "                          showlegend=(idx==1), line=dict(width=1.5),\n",
    "                          opacity=0.7),\n",
    "                row=1, col=idx\n",
    "            )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"x\")\n",
    "    fig.update_yaxes(title_text=\"f(x)\", col=1)\n",
    "    fig.update_layout(height=350, width=1000,\n",
    "                      title_text=\"Comparing Different Mean Functions\")\n",
    "    return fig\n",
    "\n",
    "# Test the function\n",
    "X_compare = np.linspace(0, 10, 100)[:, None]\n",
    "fig = compare_mean_functions(X_compare, n_samples=5)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise above reinforces an important point: **the mean function doesn't change the \"wiggliness\" or correlation structure**â€”that's entirely determined by the covariance function. The mean function simply shifts or tilts the family of functions we're considering.\n",
    "\n",
    "In practice, most GP applications use a zero or constant mean function, letting the flexible covariance function do the heavy lifting. Now let's dive deep into those covariance functions!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.7: Covariance Functions\n",
    "\n",
    "### The Heart of Gaussian Processes\n",
    "\n",
    "If the mean function is where your prior belief lives, the covariance function (or kernel) is where the magic happens. The kernel defines the structure, smoothness, and patterns that your GP can express. Choosing the right kernel is often the most important modeling decision you'll make.\n",
    "\n",
    "A covariance function $k(x, x')$ must satisfy one critical property: it must produce **positive semi-definite covariance matrices** for any set of input points. This ensures that the resulting multivariate normal distributions are valid. Fortunately, PyMC provides a rich library of kernels that satisfy this property, and you can combine them in powerful ways.\n",
    "\n",
    "Let's explore the most commonly used kernels and understand when to use each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to visualize kernels\n",
    "def visualize_kernel(cov_func, X, title, n_samples=3):\n",
    "    \"\"\"Visualize kernel covariance matrix and sample functions.\"\"\"\n",
    "    K = cov_func(X).eval()\n",
    "    K += 1e-6 * np.eye(len(X))\n",
    "    \n",
    "    fig = make_subplots(rows=1, cols=2, subplot_titles=['Covariance Matrix', 'Sample Functions'],\n",
    "                        specs=[[{'type': 'heatmap'}, {'type': 'scatter'}]])\n",
    "    \n",
    "    # Covariance matrix heatmap\n",
    "    fig.add_trace(go.Heatmap(z=K, colorscale='Viridis', showscale=True), row=1, col=1)\n",
    "    \n",
    "    # Sample functions\n",
    "    samples = rng.multivariate_normal(np.zeros(len(X)), K, size=n_samples)\n",
    "    for sample in samples:\n",
    "        fig.add_trace(go.Scatter(x=X.flatten(), y=sample, mode='lines', showlegend=False, line=dict(width=1.5)), row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Index\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Index\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"x\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"f(x)\", row=1, col=2)\n",
    "    fig.update_layout(height=350, width=900, title_text=title)\n",
    "    return fig\n",
    "\n",
    "# Grid for visualization\n",
    "X_kern = np.linspace(0, 10, 80)[:, None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ExpQuad (Squared Exponential / RBF)\n",
    "\n",
    "We've already seen this kernel! It's infinitely differentiable, producing extremely smooth functions. Use it when you expect your function to be smooth everywhere.\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{\\|x - x'\\|^2}{2\\ell^2}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_expquad = pm.gp.cov.ExpQuad(input_dim=1, ls=1.5)\n",
    "fig = visualize_kernel(cov_expquad, X_kern, \"ExpQuad Kernel\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatÃ©rn Family\n",
    "\n",
    "The MatÃ©rn kernels generalize the ExpQuad by controlling smoothness through a parameter $\\nu$. Smaller $\\nu$ means less smooth. PyMC provides three variants:\n",
    "\n",
    "- **MatÃ©rn 1/2**: Continuous but not differentiable (roughest)\n",
    "- **MatÃ©rn 3/2**: Once differentiable (medium smoothness)\n",
    "- **MatÃ©rn 5/2**: Twice differentiable (smooth, but not as extreme as ExpQuad)\n",
    "\n",
    "Use MatÃ©rn when you want control over smoothness or when your function might not be infinitely smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MatÃ©rn kernels\n",
    "matern_kernels = [\n",
    "    (pm.gp.cov.Matern12(input_dim=1, ls=1.5), \"MatÃ©rn 1/2 (Exponential)\"),\n",
    "    (pm.gp.cov.Matern32(input_dim=1, ls=1.5), \"MatÃ©rn 3/2\"),\n",
    "    (pm.gp.cov.Matern52(input_dim=1, ls=1.5), \"MatÃ©rn 5/2\")\n",
    "]\n",
    "\n",
    "for cov, title in matern_kernels:\n",
    "    fig = visualize_kernel(cov, X_kern, title)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the functions become progressively smoother as we go from MatÃ©rn 1/2 to 5/2. The MatÃ©rn 1/2 (also called Exponential kernel) produces jagged, continuous but non-differentiable functionsâ€”perfect for modeling rough phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic Kernel\n",
    "\n",
    "When your data has periodic structure (seasons, daily cycles, etc.), the Periodic kernel is your friend:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi|x - x'|/T)}{\\ell^2}\\right)$$\n",
    "\n",
    "where $T$ is the period. This kernel enforces exact periodicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cov_periodic = pm.gp.cov.Periodic(input_dim=1, period=3.0, ls=1.0)\n",
    "fig = visualize_kernel(cov_periodic, X_kern, \"Periodic Kernel (period=3.0)\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the repeating pattern? Functions drawn from this GP will have exact periodicity. In real applications, you often combine Periodic with other kernels to model periodic trends that slowly evolve over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE 5: Explore Lengthscale Effects\n",
    "\n",
    "Let's explore how lengthscale affects different kernels systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help complete this visualization\n",
    "\n",
    "def visualize_lengthscale_effects(X, lengthscales=(0.3, 1.0, 3.0), kernel_type='ExpQuad'):\n",
    "    \"\"\"\n",
    "    Visualize how lengthscale affects GP prior samples.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array\n",
    "        Input points\n",
    "    lengthscales : tuple\n",
    "        Different lengthscales to compare\n",
    "    kernel_type : str\n",
    "        'ExpQuad', 'Matern32', or 'Matern52'\n",
    "        \n",
    "    Prompt suggestion: \"Help me create a function that draws GP prior samples\n",
    "    for multiple lengthscales using PyMC kernels. Create Plotly subplots showing\n",
    "    3 functions for each lengthscale. Support ExpQuad, Matern32, and Matern52 kernels.\"\n",
    "    \"\"\"\n",
    "    # REFERENCE SOLUTION:\n",
    "    fig = make_subplots(rows=1, cols=len(lengthscales),\n",
    "                        subplot_titles=[f'lengthscale = {ls}' for ls in lengthscales])\n",
    "    \n",
    "    for idx, ls in enumerate(lengthscales, 1):\n",
    "        # Select kernel\n",
    "        if kernel_type == 'ExpQuad':\n",
    "            cov = pm.gp.cov.ExpQuad(input_dim=1, ls=ls)\n",
    "        elif kernel_type == 'Matern32':\n",
    "            cov = pm.gp.cov.Matern32(input_dim=1, ls=ls)\n",
    "        elif kernel_type == 'Matern52':\n",
    "            cov = pm.gp.cov.Matern52(input_dim=1, ls=ls)\n",
    "        \n",
    "        K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "        samples = rng.multivariate_normal(np.zeros(len(X)), K, size=3)\n",
    "        \n",
    "        for sample in samples:\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=X.flatten(), y=sample, mode='lines',\n",
    "                          showlegend=False, line=dict(width=1.5)),\n",
    "                row=1, col=idx\n",
    "            )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"x\")\n",
    "    fig.update_yaxes(title_text=\"f(x)\", col=1)\n",
    "    fig.update_layout(height=300, width=1000,\n",
    "                      title_text=f\"Lengthscale Effects on {kernel_type} Kernel\")\n",
    "    return fig\n",
    "\n",
    "# Test with different kernels\n",
    "X_test = np.linspace(0, 10, 100)[:, None]\n",
    "for kernel in ['ExpQuad', 'Matern32', 'Matern52']:\n",
    "    fig = visualize_lengthscale_effects(X_test, kernel_type=kernel)\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exercise above demonstrates that lengthscale has consistent effects across kernel families: smaller lengthscales create more rapidly varying functions, while larger lengthscales produce smoother, slowly varying functions. The kernel family (ExpQuad vs. MatÃ©rn) controls the *type* of smoothness, while lengthscale controls the *scale* at which variation occurs.\n",
    "\n",
    "Now you have a solid understanding of the main kernel building blocks. In Session 2, we'll explore how to combine these kernels to model complex phenomena like trends plus seasonality!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1.8: Summary and Next Steps\n",
    "\n",
    "Congratulations! You've completed Session 1 and built a strong foundation for working with Gaussian processes in PyMC.\n",
    "\n",
    "### What We Covered\n",
    "\n",
    "We took a three-part journey through the fundamentals:\n",
    "\n",
    "**1. Bayesian Inference Principles**\n",
    "- The three-step workflow: specify, calculate, check\n",
    "- Bayes' theorem: posterior âˆ likelihood Ã— prior\n",
    "- Conjugacy and analytical posteriors (Beta-Binomial example)\n",
    "- How data updates beliefs and reduces uncertainty\n",
    "\n",
    "**2. The PyMC API**\n",
    "- Building your first PyMC model with real baseball data\n",
    "- Priors, likelihoods, and the `observed=` keyword\n",
    "- MCMC sampling with `pm.sample()` and NUTPIE\n",
    "- Posterior predictive checks for model validation\n",
    "- Coordinate systems (`coords=` and `dims=`) for structured models\n",
    "- Multi-level models with shared parameters\n",
    "\n",
    "**3. Gaussian Process Components**\n",
    "- The connection between multivariate normals and Gaussian processes\n",
    "- Functions as infinite-dimensional vectors\n",
    "- Mean functions: zero, constant, and linear\n",
    "- Covariance functions (kernels): ExpQuad, MatÃ©rn family, Periodic\n",
    "- How lengthscale and variance parameters control smoothness and scale\n",
    "- Drawing sample functions from GP priors\n",
    "\n",
    "### The Path Forward\n",
    "\n",
    "We've deliberately kept things modular. You now understand:\n",
    "- **What** Gaussian processes are (distributions over functions)\n",
    "- **How** they're built (mean functions + covariance functions)\n",
    "- **How** to implement Bayesian models in PyMC\n",
    "\n",
    "In **Session 2**, we'll combine everything:\n",
    "- Building GP models in PyMC with `pm.gp.Marginal` and `pm.gp.Latent`\n",
    "- Fitting GPs to real data and making predictions\n",
    "- Kernel composition: combining kernels to model complex patterns\n",
    "- Non-Gaussian likelihoods for classification and count data\n",
    "- Understanding marginal vs latent formulations\n",
    "\n",
    "You're well-prepared. The concepts we've learnedâ€”Bayesian updating, PyMC workflow, and GP theoryâ€”will come together into a powerful modeling framework.\n",
    "\n",
    "See you in Session 2!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
