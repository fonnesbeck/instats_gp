{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 4: Multi-Output GPs and Case Studies\n",
    "\n",
    "**Duration:** 2-3 hours  \n",
    "**Prerequisites:** Sessions 1-3\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand multi-output GP models for correlated outputs\n",
    "2. Handle multidimensional inputs with ARD lengthscales\n",
    "3. Build coregionalized models using Hadamard product kernels\n",
    "4. Execute a comprehensive case study: Soccer player skill modeling\n",
    "5. Integrate hierarchical structure and non-Gaussian likelihoods\n",
    "6. Interpret factor models that decompose skill from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import polars as pl\n",
    "\n",
    "# PyMC ecosystem\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "print(f\"PyMC: {pm.__version__}, NumPy: {np.__version__}\")\n",
    "print(f\"Polars: {pl.__version__}, ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Multi-Output Gaussian Processes\n",
    "\n",
    "### Why Model Multiple Outputs Together?\n",
    "\n",
    "Imagine analyzing 27 elite soccer players. You could fit 27 separate GPs, but this misses that **players operate in a shared context**.\n",
    "\n",
    "Multi-output GPs offer:\n",
    "1. **Information sharing** between related outputs\n",
    "2. **Partial pooling** for data-scarce outputs\n",
    "3. **Learned correlation structure**  \n",
    "4. **Computational efficiency**\n",
    "\n",
    "Let's start with multidimensional inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Relevance Determination (ARD)\n",
    "\n",
    "ARD assigns independent lengthscales to each input dimension:\n",
    "\n",
    "$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^d \\frac{(x_i - x'_i)^2}{\\ell_i^2}\\right)$$\n",
    "\n",
    "Large $\\ell_i$ â†’ dimension $i$ is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data\n",
    "\n",
    "Create 2D data where only x1 matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 150\n",
    "x1 = RNG.uniform(-3, 3, n_obs)  # Relevant\n",
    "x2 = RNG.uniform(-3, 3, n_obs)  # Irrelevant\n",
    "y_obs = np.sin(2 * x1) + 0.5 * x1 + RNG.normal(0, 0.2, n_obs)\n",
    "X_train = np.column_stack([x1, x2])\n",
    "\n",
    "print(f\"{X_train.shape[0]} observations, {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting ARD Model\n",
    "\n",
    "Watch what lengthscales the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ard_model:\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1, shape=2)\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ls)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_obs, sigma=sigma)\n",
    "    trace_ard = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learned Lengthscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_post = az.extract(trace_ard, var_names=[\"ls\"])\n",
    "ls_means = ls_post.mean(dim=\"sample\").values\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(2):\n",
    "    fig.add_trace(go.Violin(y=ls_post.sel(ls_dim_0=i).values,\n",
    "                            name=f\"Feature {i+1}\", box_visible=True))\n",
    "fig.update_layout(title=\"Learned Lengthscales\", yaxis_title=\"Lengthscale\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"F1: {ls_means[0]:.2f}, F2: {ls_means[1]:.2f}\")\n",
    "print(f\"Ratio: {ls_means[1]/ls_means[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Feature 1: small ls â†’ relevant  \n",
    "Feature 2: large ls â†’ irrelevant\n",
    "\n",
    "ARD discovered which dimension matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Coregionalization\n",
    "\n",
    "ICM uses Hadamard product:\n",
    "\n",
    "$$k([\\mathbf{x}, i], [\\mathbf{x}', j]) = k_{\\text{input}}(\\mathbf{x}, \\mathbf{x}') \\times k_{\\text{coreg}}(i, j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 3 Related Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times, n_outputs = 40, 3\n",
    "t = np.linspace(0, 10, n_times)\n",
    "f1 = np.sin(t) + RNG.normal(0, 0.2, n_times)\n",
    "f2 = np.sin(t) + 0.5*np.cos(2*t) + RNG.normal(0, 0.2, n_times)\n",
    "f3 = -np.cos(t) + RNG.normal(0, 0.2, n_times)\n",
    "\n",
    "X_mogp = np.column_stack([np.tile(t, n_outputs), np.repeat([0,1,2], n_times)])\n",
    "y_mogp = np.concatenate([f1, f2, f3])\n",
    "\n",
    "print(f\"Multi-output: {X_mogp.shape[0]} obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Coregionalized GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mogp_model:\n",
    "    ls_time = pm.Gamma(\"ls_time\", alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    cov_time = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ls_time, active_dims=[0])\n",
    "    \n",
    "    W = pm.Normal(\"W\", mu=0, sigma=1, shape=(n_outputs, 2))\n",
    "    kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "    B = pm.Deterministic(\"B\", pt.dot(W, W.T) + pt.diag(kappa))\n",
    "    cov_out = pm.gp.cov.Coregion(input_dim=2, W=W, kappa=kappa, active_dims=[1])\n",
    "    \n",
    "    cov_total = cov_time * cov_out\n",
    "    gp = pm.gp.Marginal(cov_func=cov_total)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_mogp, y=y_mogp, sigma=sigma)\n",
    "    trace_mogp = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_post = az.extract(trace_mogp, var_names=[\"B\"]).mean(dim=\"sample\").values\n",
    "fig = go.Figure(data=go.Heatmap(z=B_post, colorscale='RdBu', zmid=0))\n",
    "fig.update_layout(title=\"Output Covariance B\", height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs 1-2: high covariance â†’ correlated  \n",
    "Output 3: lower covariance â†’ independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Soccer Player Skill Modeling\n",
    "\n",
    "### Challenge: Identify True Skill\n",
    "\n",
    "Account for team strength, opponent quality, context, varying sample sizes.\n",
    "\n",
    "### Hierarchical Logistic Regression\n",
    "\n",
    "$$P(\\text{goal}_{ij} = 1) = \\text{logit}^{-1}(\\alpha_i + \\mathbf{X}_{ij}^T\\boldsymbol{\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/SFM_data_byPlayer_clean.csv\")\n",
    "n_players = df.select(pl.col(\"name_player\")).unique().height\n",
    "goal_rate = df.select(pl.col(\"goal\").mean()).item()\n",
    "\n",
    "print(f\"{df.shape[0]} observations, {n_players} players\")\n",
    "print(f\"Goal rate: {goal_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors: Context Variables\n",
    "\n",
    "1. home_pitch: Home advantage\n",
    "2. points_diff: Recent form\n",
    "3. goal_balance_diff: Team vs opponent strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Factor Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [\"home_pitch\", \"points_diff\", \"goal_balance_diff\"]\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=factors)\n",
    "\n",
    "for i, factor in enumerate(factors, 1):\n",
    "    binned = (df.with_columns([\n",
    "        pl.col(factor).cut(breaks=[-np.inf,-1,0,1,np.inf], \n",
    "                          labels=[\"Low\",\"Mid-Low\",\"Mid-High\",\"High\"]).alias(\"bin\")\n",
    "    ]).group_by(\"bin\").agg([\n",
    "        pl.col(\"goal\").mean().alias(\"rate\")\n",
    "    ]).sort(\"bin\"))\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=binned[\"bin\"].to_list(), \n",
    "                        y=binned[\"rate\"].to_list()), row=1, col=i)\n",
    "\n",
    "fig.update_layout(title=\"Goal Rate by Factor\", showlegend=False, height=350)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_cols = [\"home_pitch\", \"points_diff\", \"goal_balance_diff\"]\n",
    "X_factors = df.select(factor_cols).to_numpy().astype(np.float64)\n",
    "y_goals = df.select(\"goal\").to_numpy().flatten().astype(int)\n",
    "\n",
    "player_names = df.select(\"name_player\").unique().sort(\"name_player\")[\"name_player\"].to_list()\n",
    "player_idx_map = {name: i for i, name in enumerate(player_names)}\n",
    "player_idx = df.select(pl.col(\"name_player\").replace(player_idx_map)).to_numpy().flatten()\n",
    "\n",
    "n_players, n_factors = len(player_names), len(factor_cols)\n",
    "print(f\"{len(y_goals)} obs, {n_players} players, {n_factors} factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Hierarchical Model\n",
    "\n",
    "Partial pooling: data-scarce players regularized toward population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p): \n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "with pm.Model() as sfm_model:\n",
    "    mu_alpha = pm.Normal(\"mu_alpha\", mu=logit(goal_rate), sigma=1)\n",
    "    sigma_alpha = pm.HalfNormal(\"sigma_alpha\", sigma=0.5)\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_alpha, sigma=sigma_alpha, shape=n_players)\n",
    "    \n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=n_factors)\n",
    "    \n",
    "    eta = alpha[player_idx] + pm.math.dot(X_factors, beta)\n",
    "    \n",
    "    y_obs = pm.Bernoulli(\"y_obs\", logit_p=eta, observed=y_goals)\n",
    "    \n",
    "    trace_sfm = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, \n",
    "                         target_accept=0.9, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(trace_sfm, var_names=[\"alpha\",\"beta\",\"mu_alpha\",\"sigma_alpha\"])\n",
    "print(f\"R-hat: [{summary['r_hat'].min():.4f}, {summary['r_hat'].max():.4f}]\")\n",
    "print(f\"ESS: [{summary['ess_bulk'].min():.0f}, {summary['ess_bulk'].max():.0f}]\")\n",
    "print(\"\\nFactor coefficients:\")\n",
    "print(summary.filter(like=\"beta\", axis=0)[[\"mean\",\"sd\",\"hdi_3%\",\"hdi_97%\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Player Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_post = az.extract(trace_sfm, var_names=[\"alpha\"])\n",
    "alpha_means = alpha_post.mean(dim=\"sample\").values\n",
    "\n",
    "results = pl.DataFrame({\n",
    "    \"player\": player_names,\n",
    "    \"skill_mean\": alpha_means,\n",
    "    \"skill_lower\": np.percentile(alpha_post.values, 2.5, axis=1),\n",
    "    \"skill_upper\": np.percentile(alpha_post.values, 97.5, axis=1)\n",
    "}).sort(\"skill_mean\", descending=True)\n",
    "\n",
    "print(\"Top 10:\")\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = results.sort(\"skill_mean\", descending=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=results_sorted[\"player\"].to_list(),\n",
    "    x=results_sorted[\"skill_mean\"].to_list(),\n",
    "    error_x=dict(type='data', symmetric=False,\n",
    "                array=(results_sorted[\"skill_upper\"]-results_sorted[\"skill_mean\"]).to_list(),\n",
    "                arrayminus=(results_sorted[\"skill_mean\"]-results_sorted[\"skill_lower\"]).to_list()),\n",
    "    mode='markers', marker=dict(size=8, color='steelblue')\n",
    "))\n",
    "fig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"Avg\")\n",
    "fig.update_layout(title=\"Player Skills (Î±)\", xaxis_title=\"Skill\", height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Skill hierarchy: Top players consistently better\n",
    "- Uncertainty varies: More data â†’ narrower intervals\n",
    "- Overlap matters: Can't confidently rank when intervals overlap\n",
    "- Context-adjusted: Fair comparison across situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_post = az.extract(trace_sfm, var_names=[\"beta\"])\n",
    "factor_res = pl.DataFrame({\n",
    "    \"factor\": factor_cols,\n",
    "    \"coef\": beta_post.mean(dim=\"sample\").values,\n",
    "    \"lower\": np.percentile(beta_post.values, 2.5, axis=1),\n",
    "    \"upper\": np.percentile(beta_post.values, 97.5, axis=1)\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=factor_res[\"coef\"].to_list(), \n",
    "                        y=factor_res[\"factor\"].to_list(),\n",
    "                        error_x=dict(type='data', symmetric=False,\n",
    "                                    array=(factor_res[\"upper\"]-factor_res[\"coef\"]).to_list(),\n",
    "                                    arrayminus=(factor_res[\"coef\"]-factor_res[\"lower\"]).to_list()),\n",
    "                        mode='markers', marker=dict(size=12, color='coral')))\n",
    "fig.add_vline(x=0, line_dash=\"dash\")\n",
    "fig.update_layout(title=\"Factor Effects (Î²)\", xaxis_title=\"Coefficient\", height=300)\n",
    "fig.show()\n",
    "\n",
    "print(factor_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal_balance_diff: strongest (team quality matters!)  \n",
    "home_pitch: positive home advantage  \n",
    "points_diff: form correlates with scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– LLM Exercise\n",
    "\n",
    "Extend the model with temporal dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Add time-varying skills\n",
    "\n",
    "def extend_with_hsgp():\n",
    "    \"\"\"\n",
    "    Add HSGP for player skills over seasons.\n",
    "    \n",
    "    Prompt: \"I have hierarchical logistic regression (Bernoulli, \n",
    "    alpha player effects, beta factors). Make alpha_i vary over \n",
    "    seasons with HSGP. Help me: 1) Define HSGP over seasons,  \n",
    "    2) Integrate with model, 3) Update predictor. PyMC code.\"\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"ðŸŽ¯ Extend SFM with time-varying HSGP skills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "\n",
    "1. Temporal dynamics: Aging curves, form  \n",
    "2. More factors: Defensive rating, rest, injuries\n",
    "3. Multi-level: Group by position\n",
    "4. Predictive checks: Simulate vs holdout\n",
    "5. Decision-making: Transfer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Summary\n",
    "\n",
    "### Sessions 1-4 Journey\n",
    "\n",
    "**Session 1**: Foundations (Bayesian inference, MVNâ†’GP, kernels)  \n",
    "**Session 2**: Model Building (kernel composition, likelihoods)  \n",
    "**Session 3**: Scaling (O(nÂ³), sparse, HSGP)  \n",
    "**Session 4**: Applications (multi-output, hierarchical, real case study)\n",
    "\n",
    "### GP Mindset\n",
    "\n",
    "1. Flexibility: Adapt to data\n",
    "2. Uncertainty: Full posteriors\n",
    "3. Interpretability: Clear meanings\n",
    "4. Composability: Complex from simple\n",
    "5. Scalability: Modern approximations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- PyMC docs: https://www.pymc.io/\n",
    "- Rasmussen & Williams book (free online)\n",
    "- PyMC examples and Discourse\n",
    "- Apply to your data!\n",
    "\n",
    "**Final thought**: GPs encode smoothness assumptions, let data speak. You're equipped for real-world problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "PyMC team, Alex Andorra & Max Goebel (soccer case), Danh Phan, Bill Engels, Chris Fonnesbeck (multi-output GPs).\n",
    "\n",
    "Materials for educational use under open-source licenses."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
