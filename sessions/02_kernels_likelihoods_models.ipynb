{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Kernels, Likelihoods, and Model Building\n",
    "\n",
    "**Duration:** 2-3 hours\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "\n",
    "- Understand different kernel families and their properties\n",
    "- Learn kernel composition via addition and multiplication\n",
    "- Understand marginal vs latent GP formulations\n",
    "- Work with non-Gaussian likelihoods\n",
    "- Build additive and multiplicative models for real-world data\n",
    "\n",
    "This session builds directly on Session 1's foundations. We'll move beyond basic GP models to explore the rich toolkit of covariance functions and likelihood choices that make Gaussian processes so flexible for real-world problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's begin by loading our standard toolkit. Notice we're using polars for data manipulation and plotly for visualization, following modern Python best practices for data science."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import scipy.stats as stats\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RANDOM_SEED = 8675309\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our environment ready, let's verify everything loaded correctly before proceeding to explore the world of kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: Introduction and Recap\n",
    "\n",
    "In Session 1, we built intuition about Gaussian processes by connecting multivariate normal distributions to functions. We learned that the **covariance function** (or kernel) is the heart of a GPâ€”it encodes our assumptions about how similar function values should be at different input locations. We also saw how to build simple GP models in PyMC using `pm.gp.Marginal` with Gaussian noise.\n",
    "\n",
    "This session takes us deeper. Real-world problems rarely involve perfectly smooth functions with Gaussian noise. You might encounter:\n",
    "\n",
    "- **Periodic patterns** that repeat over time (think seasonal sales, daily temperatures)\n",
    "- **Long-term trends** combined with weekly patterns (like the births data we'll explore)\n",
    "- **Binary outcomes** where we need classification, not regression\n",
    "- **Heavy-tailed noise** that makes outliers more likely than a normal distribution suggests\n",
    "\n",
    "Fortunately, the GP framework is remarkably flexible. By choosing appropriate kernels and likelihoods, we can handle all these scenarios. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: The Kernel Zoo\n",
    "\n",
    "Kernels are the building blocks that define the structure of our GP. Different kernels encode different assumptions about function smoothness, periodicity, and behavior. Let's explore the most important kernel families and develop intuition for when to use each one.\n",
    "\n",
    "### Understanding Kernel Parameters\n",
    "\n",
    "Before diving into specific kernels, let's understand the two parameters that appear in most covariance functions:\n",
    "\n",
    "**Lengthscale ($\\ell$)**: Controls how quickly the covariance between points decays with distance. Think of it as the \"wiggliness\" dial:\n",
    "- Small lengthscale â†’ function changes rapidly, lots of wiggles\n",
    "- Large lengthscale â†’ function changes slowly, smooth and gradually varying\n",
    "\n",
    "**Amplitude or scale ($\\eta$)**: Controls the vertical scale of variations. It determines how far from the mean function values typically deviate:\n",
    "- Small amplitude â†’ function stays close to the mean\n",
    "- Large amplitude â†’ function can wander far from the mean\n",
    "\n",
    "These parameters have profound effects on the GP prior. Let's see this in action by comparing different kernel families."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kernels for Different Smoothness Assumptions\n",
    "\n",
    "The degree of smoothness (differentiability) is a key property that distinguishes kernels. We'll compare three important kernels that span the spectrum from rough to infinitely smooth.\n",
    "\n",
    "First, let's create a helper function to visualize GP priors. This function will help us understand how each kernel shapes the functions we can represent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_prior_samples(X, cov_func, n_samples=5, title=\"GP Prior Samples\"):\n",
    "    \"\"\"\n",
    "    Plot samples from a GP prior to visualize kernel behavior.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        Input locations (1D array, will be reshaped to column vector)\n",
    "    cov_func : PyMC covariance function\n",
    "        The kernel to visualize\n",
    "    n_samples : int\n",
    "        Number of function samples to draw\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1, 1)\n",
    "    \n",
    "    # Compute covariance matrix and add jitter for numerical stability\n",
    "    K = cov_func(X).eval() + 1e-6 * np.eye(len(X))\n",
    "    \n",
    "    # Draw samples from the GP prior\n",
    "    samples = rng.multivariate_normal(np.zeros(len(X)), K, size=n_samples)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X.flatten(),\n",
    "            y=samples[i],\n",
    "            mode='lines',\n",
    "            name=f'Sample {i+1}',\n",
    "            opacity=0.7,\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='f(x)',\n",
    "        hovermode='x unified',\n",
    "        width=900,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's explore the three most commonly used stationary kernels. We'll use the same lengthscale and amplitude for each so we can directly compare their smoothness properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input points\n",
    "X_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "# Fixed parameters for fair comparison\n",
    "lengthscale = 1.0\n",
    "amplitude = 1.0\n",
    "\n",
    "# Define three kernels with different smoothness properties\n",
    "kernels = {\n",
    "    'ExpQuad (RBF)': amplitude**2 * pm.gp.cov.ExpQuad(1, lengthscale),\n",
    "    'Matern52': amplitude**2 * pm.gp.cov.Matern52(1, lengthscale),\n",
    "    'Matern32': amplitude**2 * pm.gp.cov.Matern32(1, lengthscale),\n",
    "}\n",
    "\n",
    "# Plot prior samples for each kernel\n",
    "for name, cov in kernels.items():\n",
    "    fig = plot_gp_prior_samples(X_grid, cov, n_samples=5, title=f'{name} Kernel - Prior Samples')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Kernel Comparisons\n",
    "\n",
    "Looking at these prior samples, you should notice clear differences in smoothness:\n",
    "\n",
    "**ExpQuad (Squared Exponential / RBF)**: These functions are infinitely differentiableâ€”silky smooth with no sharp corners. The ExpQuad kernel assumes the underlying function is extremely well-behaved. This is great for phenomena like temperature changes or other physical processes that vary continuously. However, it can be *too* smooth for many real-world applications, potentially over-smoothing genuine features in the data.\n",
    "\n",
    "**MatÃ©rn 5/2**: These functions are twice differentiable but not infinitely smooth. Notice they're still quite smooth but allow slightly more flexibility than ExpQuad. The MatÃ©rn 5/2 is often the go-to choice in practiceâ€”it's smooth enough for most applications but doesn't impose unrealistic smoothness assumptions. Think of it as the \"Goldilocks\" kernel.\n",
    "\n",
    "**MatÃ©rn 3/2**: These functions are once differentiable, allowing them to have sharper features and more dramatic changes. This kernel works well when you expect the function to change more abruptly or have kinks. It's particularly useful for phenomena that might have sudden transitions.\n",
    "\n",
    "**Practical guidance**: When in doubt, start with MatÃ©rn 5/2. It's flexible enough for most applications without the over-smoothing tendency of ExpQuad. Reserve ExpQuad for truly smooth phenomena, and use MatÃ©rn 3/2 when you anticipate sharper features or want to allow the data to speak more strongly about local variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring Lengthscale Effects\n",
    "\n",
    "Let's deepen our intuition by seeing how the lengthscale parameter affects function behavior. This is one of the most important parameters you'll tune in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare different lengthscales with the same kernel\n",
    "lengthscales = [0.3, 1.0, 3.0]\n",
    "amplitude = 1.0\n",
    "\n",
    "for ls in lengthscales:\n",
    "    cov = amplitude**2 * pm.gp.cov.Matern52(1, ls)\n",
    "    fig = plot_gp_prior_samples(\n",
    "        X_grid, \n",
    "        cov, \n",
    "        n_samples=5, \n",
    "        title=f'MatÃ©rn 5/2 with lengthscale = {ls}'\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Lengthscale Through the Visualizations\n",
    "\n",
    "Notice how dramatically the lengthscale changes function behavior:\n",
    "\n",
    "With **lengthscale = 0.3**, the functions wiggle rapidly, changing direction frequently. This says \"function values at points separated by more than 0.3 units are nearly independent.\" You'd use a small lengthscale when modeling high-frequency phenomena or when you have dense data and want to capture fine details.\n",
    "\n",
    "With **lengthscale = 1.0**, we see moderate smoothnessâ€”functions change gradually but can still capture important variations. This is often a reasonable starting point for exploration.\n",
    "\n",
    "With **lengthscale = 3.0**, the functions are very smooth and slowly varying. This says \"function values stay correlated across long distances.\" Large lengthscales work well for modeling long-term trends or when you have sparse data that requires more smoothing.\n",
    "\n",
    "The lengthscale essentially controls your model's **memory**: how far does the function need to travel before it \"forgets\" where it came from? This intuition will serve you well when building models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic Kernels for Seasonal Patterns\n",
    "\n",
    "Many real-world phenomena repeat: daily temperature cycles, weekly sales patterns, annual seasonal effects. For these, we need a kernel that encodes periodicity. The Periodic kernel is perfect for this.\n",
    "\n",
    "The Periodic kernel has two key parameters beyond amplitude:\n",
    "- **period**: The length of one complete cycle\n",
    "- **lengthscale**: Controls smoothness *within* each period (how similar nearby points within a cycle should be)\n",
    "\n",
    "Let's visualize periodic patterns and see how they differ from the stationary kernels we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate periodic kernel with annual period\n",
    "X_grid = np.linspace(0, 20, 400)  # Extended range to see multiple periods\n",
    "\n",
    "period = 5.0  # Complete cycle every 5 units\n",
    "lengthscale = 1.0\n",
    "amplitude = 1.0\n",
    "\n",
    "cov_periodic = amplitude**2 * pm.gp.cov.Periodic(1, period=period, ls=lengthscale)\n",
    "\n",
    "fig = plot_gp_prior_samples(\n",
    "    X_grid, \n",
    "    cov_periodic, \n",
    "    n_samples=5,\n",
    "    title=f'Periodic Kernel (period={period}, lengthscale={lengthscale})'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Periodic Patterns\n",
    "\n",
    "Notice how the functions repeat with the specified period. The pattern from $x=0$ to $x=5$ is similar to the pattern from $x=5$ to $x=10$, and so on. This is exactly what we want for seasonal data.\n",
    "\n",
    "The lengthscale parameter controls smoothness *within* each period. A smaller lengthscale would allow more wiggly behavior within each cycle, while a larger lengthscale would make each cycle smoother.\n",
    "\n",
    "**Real-world example**: If you're modeling daily births data with weekly seasonality, you'd set `period=7`. The GP would then learn that Mondays tend to be similar to other Mondays, Tuesdays to Tuesdays, and so forth, while still being flexible about the exact pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Kernel for Trends\n",
    "\n",
    "Sometimes your data contains linear or polynomial trends. The Linear kernel captures this by computing covariance proportional to the inner product of input locations. Unlike the stationary kernels we've seen, the Linear kernel is *non-stationary*â€”the covariance depends on the absolute position, not just the distance between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate linear kernel\n",
    "X_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "c = 1.0  # Center point\n",
    "variance = 1.0\n",
    "\n",
    "cov_linear = variance * pm.gp.cov.Linear(1, c=c)\n",
    "\n",
    "fig = plot_gp_prior_samples(\n",
    "    X_grid,\n",
    "    cov_linear,\n",
    "    n_samples=5,\n",
    "    title='Linear Kernel - Prior Samples'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Linear Kernel\n",
    "\n",
    "These functions are predominantly linear, though you'll notice they're not perfectly straight linesâ€”there's some flexibility around the linear trend. The Linear kernel alone is rarely sufficient for real data, but it becomes powerful when *combined* with other kernels (which we'll explore in Section 2.5).\n",
    "\n",
    "The parameter `c` acts as a center point or offset. The covariance between two points $x$ and $x'$ is proportional to $(x - c)(x' - c)$. This makes the variance grow as you move away from $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand individual kernels and their properties, we're ready to see how to use them in real models. But first, let's address an important modeling decision: should we use `pm.gp.Marginal` or `pm.gp.Latent`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Marginal Likelihood with pm.gp.Marginal\n",
    "\n",
    "When your data are continuous observations with Gaussian noise, `pm.gp.Marginal` provides the most efficient implementation. It analytically integrates out the latent GP function, directly computing the marginal likelihood $p(y | x)$ without sampling the intermediate function values.\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "Recall that a GP models a function $f(x)$, and our observations are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &\\sim \\mathcal{GP}(m(x), k(x, x')) \\\\\n",
    "y &= f(x) + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because both the GP prior and the Gaussian noise are normal distributions, we can analytically integrate out $f$ to get:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(m(X), K(X, X) + \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "This marginalization is exact and efficientâ€”we don't need to sample the potentially high-dimensional latent function. The `marginal_likelihood` method implements this approach.\n",
    "\n",
    "### When to Use pm.gp.Marginal\n",
    "\n",
    "Choose `pm.gp.Marginal` when:\n",
    "- Your likelihood is Gaussian (normal)\n",
    "- You have continuous-valued observations\n",
    "- You want the fastest inference possible\n",
    "- You don't need samples from the latent function itself\n",
    "\n",
    "Let's work through a complete example using real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Modeling COâ‚‚ Concentrations\n",
    "\n",
    "We'll use a classic dataset: atmospheric COâ‚‚ concentrations measured at Mauna Loa Observatory. This dataset exhibits both a long-term increasing trend and annual seasonal variationâ€”a perfect demonstration of why we need flexible covariance functions.\n",
    "\n",
    "Let's load and visualize the data first. We'll use polars for data manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic data similar to Mauna Loa CO2\n",
    "# (In practice, you'd load real data here)\n",
    "n_points = 100\n",
    "X_train = np.linspace(0, 10, n_points)[:, None]\n",
    "\n",
    "# True function: long-term trend + seasonal component + noise\n",
    "def true_function(x):\n",
    "    trend = 0.3 * x\n",
    "    seasonal = 0.5 * np.sin(2 * np.pi * x)\n",
    "    return trend + seasonal\n",
    "\n",
    "f_true = true_function(X_train.flatten())\n",
    "noise_std = 0.2\n",
    "y_train = f_true + noise_std * rng.standard_normal(n_points)\n",
    "\n",
    "# Create a polars dataframe for easier manipulation\n",
    "df_train = pl.DataFrame({\n",
    "    'x': X_train.flatten(),\n",
    "    'y': y_train,\n",
    "    'f_true': f_true\n",
    "})\n",
    "\n",
    "# Visualize the data\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_train['x'],\n",
    "    y=df_train['y'],\n",
    "    mode='markers',\n",
    "    name='Observed data',\n",
    "    marker=dict(size=4, color='black')\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_train['x'],\n",
    "    y=df_train['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function',\n",
    "    line=dict(color='dodgerblue', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Training Data with Trend and Seasonality',\n",
    "    xaxis_title='Time',\n",
    "    yaxis_title='Value',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Data Patterns\n",
    "\n",
    "Looking at this plot, we see exactly the patterns we discussed earlier: an increasing long-term trend combined with regular oscillations. The black dots show our noisy observations, while the blue line reveals the true underlying function.\n",
    "\n",
    "A simple linear model would miss the oscillations entirely. A purely periodic model would miss the upward trend. We need a GP with a covariance function that can capture both features simultaneously. This is a perfect job for a MatÃ©rn 5/2 kernel, which can flexibly model smooth, gradually varying patterns.\n",
    "\n",
    "Let's build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "We'll specify priors for the hyperparameters (lengthscale, amplitude, and noise level), then use `pm.gp.Marginal` to efficiently compute the marginal likelihood. We'll use weakly informative priors that allow the data to guide us to reasonable values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as marginal_model:\n",
    "    # Priors for hyperparameters\n",
    "    # Lengthscale: how quickly covariance decays\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    \n",
    "    # Amplitude: scale of variations\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    # Create the GP with mean function (default is zero)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    \n",
    "    # Noise level\n",
    "    sigma = pm.HalfNormal('sigma', sigma=0.5)\n",
    "    \n",
    "    # Marginal likelihood\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_train, y=y_train, sigma=sigma)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    marginal_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='numpyro',\n",
    "        chains=2,\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model Components\n",
    "\n",
    "Let's break down what we just specified:\n",
    "\n",
    "**Lengthscale prior (`ell`)**: We used a Gamma(2, 1) prior, which has most of its mass between 0.5 and 5. This weakly suggests that correlations decay over a moderate distance, but remains flexible enough for the data to pull us toward smaller or larger values if needed.\n",
    "\n",
    "**Amplitude prior (`eta`)**: The HalfNormal(2) prior allows for a wide range of vertical scales, centered around moderate values. This is often a safe choice that lets the data speak.\n",
    "\n",
    "**Noise prior (`sigma`)**: Another HalfNormal, this time with smaller scale, reflecting our expectation that observation noise is relatively modest compared to the signal.\n",
    "\n",
    "The key method call is `gp.marginal_likelihood`, which efficiently computes $p(y | \\theta)$ by marginalizing out the latent function. This is much faster than sampling the function explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Inference Diagnostics\n",
    "\n",
    "Before trusting our results, let's verify that sampling worked well. We'll check R-hat values and examine trace plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check R-hat values - should all be close to 1.0\n",
    "summary = az.summary(marginal_trace, var_names=['ell', 'eta', 'sigma'])\n",
    "print(summary)\n",
    "\n",
    "# Check for divergences\n",
    "divergences = marginal_trace.sample_stats['diverging'].sum().item()\n",
    "print(f\"\\nNumber of divergences: {divergences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Diagnostics\n",
    "\n",
    "R-hat values close to 1.0 (typically < 1.01) indicate that our chains have converged and are exploring the same posterior distribution. Effective sample sizes (ESS) tell us how many independent samples we effectively haveâ€”higher is better, and we generally want at least a few hundred.\n",
    "\n",
    "Zero divergences is ideal. Divergences suggest the sampler had trouble exploring certain regions of parameter space, often indicating problems with the model specification or prior choices. If you see divergences, you might need to reparameterize or use more informative priors.\n",
    "\n",
    "Assuming our diagnostics look good, let's visualize the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior distributions\n",
    "az.plot_trace(marginal_trace, var_names=['ell', 'eta', 'sigma'], compact=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Now comes the exciting part: using our trained GP to make predictions at new input locations. We'll use the `conditional` method to compute the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X locations for prediction (including extrapolation)\n",
    "X_new = np.linspace(-1, 12, 300)[:, None]\n",
    "\n",
    "with marginal_model:\n",
    "    # Conditional distribution for the latent function\n",
    "    f_pred = gp.conditional('f_pred', X_new)\n",
    "    \n",
    "    # Sample from the posterior predictive\n",
    "    marginal_ppc = pm.sample_posterior_predictive(\n",
    "        marginal_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Posterior Predictions\n",
    "\n",
    "Let's plot the posterior mean along with credible intervals to show our uncertainty. Notice how uncertainty grows as we extrapolate beyond the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples and compute statistics\n",
    "f_pred_samples = az.extract(\n",
    "    marginal_ppc.posterior_predictive, \n",
    "    var_names=['f_pred']\n",
    ")['f_pred'].values\n",
    "\n",
    "f_pred_mean = f_pred_samples.mean(axis=1)\n",
    "f_pred_lower = np.percentile(f_pred_samples, 2.5, axis=1)\n",
    "f_pred_upper = np.percentile(f_pred_samples, 97.5, axis=1)\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# 95% credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_new.flatten(),\n",
    "    y=f_pred_upper,\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_new.flatten(),\n",
    "    y=f_pred_lower,\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "    line=dict(width=0),\n",
    "    name='95% Credible Interval'\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_new.flatten(),\n",
    "    y=f_pred_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior mean',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_train['x'],\n",
    "    y=df_train['y'],\n",
    "    mode='markers',\n",
    "    name='Observed data',\n",
    "    marker=dict(size=4, color='black')\n",
    "))\n",
    "\n",
    "# True function\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_train['x'],\n",
    "    y=df_train['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function',\n",
    "    line=dict(color='dodgerblue', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Add vertical lines to show training range\n",
    "fig.add_vline(x=X_train.min(), line_dash='dot', line_color='gray', opacity=0.5)\n",
    "fig.add_vline(x=X_train.max(), line_dash='dot', line_color='gray', opacity=0.5)\n",
    "\n",
    "fig.update_layout(\n",
    "    title='GP Marginal: Posterior Predictions with Uncertainty',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='f(X)',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Predictions\n",
    "\n",
    "This plot reveals several important features of GP regression:\n",
    "\n",
    "**Within the training range** (between the vertical dotted lines), the posterior mean (red line) closely tracks the true function (blue dashed line), and the 95% credible interval (shaded region) is narrow. The model is confident because it has observed data in this region.\n",
    "\n",
    "**Outside the training range**, the uncertainty grows rapidly. Notice how the credible interval widens as we extrapolate to the left and right. The GP is honestly expressing its uncertainty: \"I haven't seen data out here, so I'm less sure about what's happening.\"\n",
    "\n",
    "The posterior mean continues to capture the general trend even in extrapolation regions, but the growing uncertainty reminds us to be cautious about predictions far from observed data. This is one of the great strengths of GPsâ€”they provide calibrated uncertainty quantification automatically.\n",
    "\n",
    "Now that we understand `pm.gp.Marginal`, let's explore the more flexible but computationally intensive alternative: `pm.gp.Latent`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.4: Latent GPs with pm.gp.Latent\n",
    "\n",
    "While `pm.gp.Marginal` is efficient for Gaussian likelihoods, many real-world problems don't produce continuous measurements with Gaussian noise. Consider:\n",
    "\n",
    "- **Classification**: Predicting whether a patient has a disease (binary outcome)\n",
    "- **Count data**: Number of events in a time period (discrete, non-negative)\n",
    "- **Survival analysis**: Time until an event occurs (censored data)\n",
    "\n",
    "For these cases, we need `pm.gp.Latent`, which keeps the GP function as a latent variable in the model and allows arbitrary likelihood functions.\n",
    "\n",
    "### The Latent GP Approach\n",
    "\n",
    "Instead of marginalizing out the GP, we explicitly sample it:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f &\\sim \\mathcal{GP}(m, k) \\\\\n",
    "y_i &\\sim \\text{Likelihood}(g(f(x_i)), \\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $g$ is a link function (like logistic for binary outcomes) and Likelihood can be any distribution (Bernoulli, Poisson, Student-T, etc.).\n",
    "\n",
    "### Trade-offs: Latent vs Marginal\n",
    "\n",
    "**pm.gp.Marginal**:\n",
    "- âœ“ Fast: analytically integrates out the GP\n",
    "- âœ“ Efficient: fewer parameters to sample\n",
    "- âœ— Limited: only works with Gaussian likelihoods\n",
    "\n",
    "**pm.gp.Latent**:\n",
    "- âœ“ Flexible: works with any likelihood\n",
    "- âœ“ Includes GP samples in posterior\n",
    "- âœ— Slower: must sample high-dimensional latent function\n",
    "- âœ— Can have sampling challenges\n",
    "\n",
    "Let's work through a classification example to see `pm.gp.Latent` in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Binary Classification with GPs\n",
    "\n",
    "Imagine we're predicting whether a medical treatment is effective based on patient characteristics. The outcome is binary (success/failure), but we believe the probability of success varies smoothly with the input features. A GP with a Bernoulli likelihood is perfect for this.\n",
    "\n",
    "Let's generate synthetic classification data where the probability varies smoothly with the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate binary classification data\n",
    "n_class = 150\n",
    "X_class = np.linspace(0, 10, n_class)[:, None]\n",
    "\n",
    "# True latent function (log-odds)\n",
    "ell_true = 1.0\n",
    "eta_true = 2.0\n",
    "cov_true = eta_true**2 * pm.gp.cov.ExpQuad(1, ell_true)\n",
    "K_true = cov_true(X_class).eval() + 1e-6 * np.eye(n_class)\n",
    "\n",
    "f_true_class = rng.multivariate_normal(np.zeros(n_class), K_true)\n",
    "\n",
    "# Convert to probabilities via logistic function\n",
    "p_true = 1 / (1 + np.exp(-f_true_class))\n",
    "\n",
    "# Generate binary observations\n",
    "y_class = rng.binomial(1, p_true)\n",
    "\n",
    "# Create polars dataframe\n",
    "df_class = pl.DataFrame({\n",
    "    'x': X_class.flatten(),\n",
    "    'y': y_class,\n",
    "    'p_true': p_true,\n",
    "    'f_true': f_true_class\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "# True probability\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_class['x'],\n",
    "    y=df_class['p_true'],\n",
    "    mode='lines',\n",
    "    name='True p(y=1)',\n",
    "    line=dict(color='dodgerblue', width=3)\n",
    "))\n",
    "\n",
    "# Binary observations (jittered for visibility)\n",
    "y_jittered = df_class['y'] + 0.02 * rng.standard_normal(n_class)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_class['x'],\n",
    "    y=y_jittered,\n",
    "    mode='markers',\n",
    "    name='Observed outcomes',\n",
    "    marker=dict(size=5, color='black', symbol='x')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Binary Classification Data',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Probability / Outcome',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Classification Data\n",
    "\n",
    "The blue line shows the true probability that $y=1$ as a function of $x$. Notice how it varies smoothlyâ€”this is exactly the kind of structure a GP can capture. The black X markers show our binary observations, which are scattered around the probability curve (we've added small jitter for visibility).\n",
    "\n",
    "In regions where the probability is close to 0 or 1, we see mostly failures or mostly successes. In the middle range where probability is around 0.5, we see a mix of both outcomes. Our goal is to recover this smooth probability function from the binary data alone.\n",
    "\n",
    "This is a challenging problemâ€”the observations are discrete, but we want to learn a smooth underlying probability. Let's see how `pm.gp.Latent` handles this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Classification Model\n",
    "\n",
    "We'll use a latent GP with a Bernoulli likelihood. The GP models the log-odds (logit) of the probability, and we transform to probabilities using the inverse logit function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as latent_model:\n",
    "    # Hyperparameter priors\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=3)\n",
    "    \n",
    "    # Covariance function\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(1, ell)\n",
    "    \n",
    "    # Latent GP\n",
    "    gp = pm.gp.Latent(cov_func=cov_func)\n",
    "    \n",
    "    # GP prior (this is the latent function)\n",
    "    f = gp.prior('f', X=X_class)\n",
    "    \n",
    "    # Transform to probability via logistic function\n",
    "    p = pm.Deterministic('p', pm.math.invlogit(f))\n",
    "    \n",
    "    # Bernoulli likelihood\n",
    "    y_obs = pm.Bernoulli('y', p=p, observed=y_class)\n",
    "    \n",
    "    # Sample (this will be slower than Marginal)\n",
    "    latent_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='numpyro',\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Latent Model Structure\n",
    "\n",
    "The key differences from `pm.gp.Marginal` are:\n",
    "\n",
    "**`gp.prior()`**: This explicitly adds the GP as a latent variable in the model. We're sampling the function values $f$ at our input locations.\n",
    "\n",
    "**Deterministic transformation**: We use `pm.Deterministic` to track the probability $p = \\text{logit}^{-1}(f)$. This is purely for convenienceâ€”we want to see probabilities in our posterior, not log-odds.\n",
    "\n",
    "**Non-Gaussian likelihood**: The Bernoulli likelihood connects our latent function to the binary observations. This is what makes `pm.gp.Marginal` unsuitableâ€”there's no way to marginalize a Bernoulli likelihood analytically.\n",
    "\n",
    "**Slower sampling**: Because we're sampling a potentially high-dimensional latent function, this model will be slower than an equivalent Gaussian regression. The `target_accept=0.95` helps with sampling stability.\n",
    "\n",
    "Let's check the diagnostics before looking at results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence\n",
    "summary_latent = az.summary(latent_trace, var_names=['ell', 'eta'])\n",
    "print(summary_latent)\n",
    "\n",
    "# Check divergences\n",
    "divergences_latent = latent_trace.sample_stats['diverging'].sum().item()\n",
    "print(f\"\\nNumber of divergences: {divergences_latent}\")\n",
    "\n",
    "if divergences_latent == 0:\n",
    "    print(\"âœ“ No divergences - sampling worked well!\")\n",
    "else:\n",
    "    print(\"âš  Warning: Some divergences detected. Consider reparameterization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Posterior Probability Function\n",
    "\n",
    "Now let's see how well we recovered the true probability function from the binary data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior samples for probability\n",
    "p_samples = az.extract(latent_trace, var_names=['p'])['p'].values\n",
    "\n",
    "p_mean = p_samples.mean(axis=1)\n",
    "p_lower = np.percentile(p_samples, 2.5, axis=1)\n",
    "p_upper = np.percentile(p_samples, 97.5, axis=1)\n",
    "\n",
    "# Create visualization\n",
    "fig = go.Figure()\n",
    "\n",
    "# 95% credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_class.flatten(),\n",
    "    y=p_upper,\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_class.flatten(),\n",
    "    y=p_lower,\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "    line=dict(width=0),\n",
    "    name='95% Credible Interval'\n",
    "))\n",
    "\n",
    "# Posterior mean probability\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_class.flatten(),\n",
    "    y=p_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior mean p(y=1)',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# True probability\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_class['x'],\n",
    "    y=df_class['p_true'],\n",
    "    mode='lines',\n",
    "    name='True p(y=1)',\n",
    "    line=dict(color='dodgerblue', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Binary observations\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_class['x'],\n",
    "    y=y_jittered,\n",
    "    mode='markers',\n",
    "    name='Observed outcomes',\n",
    "    marker=dict(size=5, color='black', symbol='x')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='GP Classification: Recovered Probability Function',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='P(y=1)',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Classification Results\n",
    "\n",
    "This is quite remarkable: from binary 0/1 observations alone, we've recovered a smooth probability function that closely tracks the true underlying probability (blue dashed line).\n",
    "\n",
    "The posterior mean (red line) captures the general shape of the true probability curve. The credible interval (shaded region) reflects our uncertaintyâ€”notice it's wider in regions with fewer data points or where observations are more variable.\n",
    "\n",
    "The GP classification model has learned that probability varies smoothly with $x$, and it's appropriately uncertain in regions where the data are ambiguous (around $p \\approx 0.5$). This is exactly the kind of calibrated uncertainty we want from a probabilistic model.\n",
    "\n",
    "Now let's move on to one of the most powerful features of GPs: kernel composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE 1: Binary GP Classifier\n",
    "\n",
    "Now it's your turn to build a binary classification model using `pm.gp.Latent`.\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the function below that builds a GP classification model. You'll need to:\n",
    "1. Define appropriate priors for lengthscale and amplitude\n",
    "2. Create a covariance function (your choice of kernel)\n",
    "3. Set up a `pm.gp.Latent` GP with Bernoulli likelihood\n",
    "4. Sample from the posterior\n",
    "\n",
    "**Prompt suggestion**: \"Help me implement a Latent GP classification model in PyMC with an RBF kernel and Bernoulli likelihood using a logit link. The model should include priors for hyperparameters and return an InferenceData object.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_binary_latent_gp(X, y, kernel_type='ExpQuad'):\n",
    "    \"\"\"\n",
    "    Build a pm.gp.Latent classification model with Bernoulli likelihood.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, 1)\n",
    "        Input features\n",
    "    y : array, shape (n,)\n",
    "        Binary outcomes (0 or 1)\n",
    "    kernel_type : str\n",
    "        Type of kernel to use: 'ExpQuad', 'Matern52', or 'Matern32'\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : pm.Model\n",
    "        The PyMC model\n",
    "    trace : az.InferenceData\n",
    "        Posterior samples\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# Uncomment the lines below once you've implemented the function\n",
    "# model, trace = build_binary_latent_gp(X_class, y_class)\n",
    "# az.summary(trace, var_names=['ell', 'eta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5: Additive and Multiplicative Kernels\n",
    "\n",
    "One of the most powerful features of GPs is that we can **compose kernels** through addition and multiplication. This allows us to build sophisticated models that capture multiple patterns simultaneously.\n",
    "\n",
    "### The Algebra of Kernels\n",
    "\n",
    "If $k_1$ and $k_2$ are valid covariance functions, then:\n",
    "\n",
    "**Addition ($k_1 + k_2$)**: The resulting function is a sum of independent components\n",
    "- Use when: You have multiple independent sources of variation (e.g., trend + seasonality)\n",
    "- Interpretation: Functions can vary in ways explained by *either* kernel\n",
    "\n",
    "**Multiplication ($k_1 \\times k_2$)**: The resulting function has features of both kernels interacting\n",
    "- Use when: One pattern modulates another (e.g., growing seasonal amplitude)\n",
    "- Interpretation: Functions must satisfy *both* kernels' constraints simultaneously\n",
    "\n",
    "Let's explore these through the classic example: birth rates over time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real Data: Daily Births in the USA\n",
    "\n",
    "Let's load real data on the number of births per day in the United States from 1969-1988. This is a fascinating dataset that exhibits multiple patterns:\n",
    "- Long-term trends (changing birth rates over decades)\n",
    "- Annual seasonality (more births in certain months)\n",
    "- Weekly patterns (fewer births on weekends due to scheduling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load births data\n",
    "# For this example, we'll create synthetic data with known structure\n",
    "# In practice, you would load real birth data from the data/ folder\n",
    "\n",
    "n_days = 365 * 3  # Three years of daily data\n",
    "time = np.arange(n_days)\n",
    "X_births = (time / 365)[:, None]  # Time in years\n",
    "\n",
    "# Simulate births with multiple components:\n",
    "# 1. Long-term trend (slight decrease)\n",
    "trend = 100 - 2 * X_births.flatten()\n",
    "\n",
    "# 2. Annual seasonality (more births in summer)\n",
    "seasonal = 5 * np.sin(2 * np.pi * X_births.flatten())\n",
    "\n",
    "# 3. Weekly pattern (fewer on weekends)\n",
    "weekly = -3 * ((time % 7) >= 5)  # Dip on days 5 and 6 (weekend)\n",
    "\n",
    "# Combine and add noise\n",
    "y_births = trend + seasonal + weekly + 2 * rng.standard_normal(n_days)\n",
    "\n",
    "# Create polars dataframe\n",
    "df_births = pl.DataFrame({\n",
    "    'time': X_births.flatten(),\n",
    "    'day': time,\n",
    "    'births': y_births,\n",
    "    'trend': trend,\n",
    "    'seasonal': seasonal,\n",
    "    'weekly': weekly\n",
    "})\n",
    "\n",
    "# Visualize the data\n",
    "fig = px.scatter(\n",
    "    df_births.to_pandas(),\n",
    "    x='time',\n",
    "    y='births',\n",
    "    title='Daily Birth Counts Over Time',\n",
    "    labels={'time': 'Time (years)', 'births': 'Relative births'}\n",
    ")\n",
    "\n",
    "fig.update_traces(marker=dict(size=2, opacity=0.5))\n",
    "fig.update_layout(width=900, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing Multiple Patterns\n",
    "\n",
    "Looking at this data, we can see:\n",
    "- A gradual downward trend over the years\n",
    "- Clear periodic oscillations (annual seasonality)\n",
    "- Regular weekly dips (though harder to see at this scale)\n",
    "\n",
    "No single kernel we've seen so far can capture all these patterns. We need to combine kernels. Let's build up an additive model piece by piece."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Additive Model: Trend + Seasonality\n",
    "\n",
    "We'll use an **additive kernel** to model the trend and seasonality as independent components:\n",
    "- A MatÃ©rn 5/2 kernel with long lengthscale for the gradual trend\n",
    "- A Periodic kernel with period=1 year for seasonal variation\n",
    "\n",
    "The key insight: $k_{\\text{total}} = k_{\\text{trend}} + k_{\\text{seasonal}}$ lets each component explain different features of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsample for computational efficiency\n",
    "n_train = 300\n",
    "indices = rng.choice(n_days, size=n_train, replace=False)\n",
    "indices.sort()\n",
    "\n",
    "X_train_births = X_births[indices]\n",
    "y_train_births = y_births[indices]\n",
    "\n",
    "with pm.Model() as additive_model:\n",
    "    # Trend component: smooth, long lengthscale\n",
    "    ell_trend = pm.Gamma('ell_trend', alpha=2, beta=0.5)  # Prior favors larger lengthscales\n",
    "    eta_trend = pm.HalfNormal('eta_trend', sigma=5)\n",
    "    cov_trend = eta_trend**2 * pm.gp.cov.Matern52(1, ell_trend)\n",
    "    \n",
    "    # Seasonal component: periodic with yearly period\n",
    "    ell_seasonal = pm.Gamma('ell_seasonal', alpha=2, beta=2)\n",
    "    eta_seasonal = pm.HalfNormal('eta_seasonal', sigma=5)\n",
    "    period = 1.0  # One year\n",
    "    cov_seasonal = eta_seasonal**2 * pm.gp.cov.Periodic(1, period=period, ls=ell_seasonal)\n",
    "    \n",
    "    # ADDITIVE KERNEL: sum of trend and seasonal\n",
    "    cov_total = cov_trend + cov_seasonal\n",
    "    \n",
    "    # GP with additive kernel\n",
    "    gp = pm.gp.Marginal(cov_func=cov_total)\n",
    "    \n",
    "    # Likelihood\n",
    "    sigma = pm.HalfNormal('sigma', sigma=3)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_train_births, y=y_train_births, sigma=sigma)\n",
    "    \n",
    "    # Sample\n",
    "    additive_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='numpyro',\n",
    "        chains=2,\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Additive Model\n",
    "\n",
    "The critical line is `cov_total = cov_trend + cov_seasonal`. This creates a new kernel where:\n",
    "- The trend component can vary slowly and independently\n",
    "- The seasonal component can oscillate annually\n",
    "- The total variation is the sum of both\n",
    "\n",
    "We gave each component its own hyperparameters (`ell_trend`, `eta_trend`, `ell_seasonal`, `eta_seasonal`), allowing the model to learn how much variation each component explains.\n",
    "\n",
    "Let's visualize the predictions to see how well this captures the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "X_pred_births = np.linspace(0, 3.5, 400)[:, None]\n",
    "\n",
    "with additive_model:\n",
    "    f_pred_births = gp.conditional('f_pred', X_pred_births)\n",
    "    ppc_births = pm.sample_posterior_predictive(\n",
    "        additive_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract predictions\n",
    "f_pred_births_samples = az.extract(\n",
    "    ppc_births.posterior_predictive,\n",
    "    var_names=['f_pred']\n",
    ")['f_pred'].values\n",
    "\n",
    "f_pred_births_mean = f_pred_births_samples.mean(axis=1)\n",
    "f_pred_births_lower = np.percentile(f_pred_births_samples, 2.5, axis=1)\n",
    "f_pred_births_upper = np.percentile(f_pred_births_samples, 97.5, axis=1)\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_births.flatten(),\n",
    "    y=f_pred_births_upper,\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False,\n",
    "    hoverinfo='skip'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_births.flatten(),\n",
    "    y=f_pred_births_lower,\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "    line=dict(width=0),\n",
    "    name='95% Credible Interval'\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_births.flatten(),\n",
    "    y=f_pred_births_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior mean',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Training data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_train_births.flatten(),\n",
    "    y=y_train_births,\n",
    "    mode='markers',\n",
    "    name='Training data',\n",
    "    marker=dict(size=3, color='black', opacity=0.5)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Additive GP: Trend + Seasonality',\n",
    "    xaxis_title='Time (years)',\n",
    "    yaxis_title='Births',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Additive Model Results\n",
    "\n",
    "The model has successfully decomposed the data into its trend and seasonal components:\n",
    "- The overall downward trajectory captures the long-term trend\n",
    "- The regular oscillations capture the annual seasonality\n",
    "- The model extrapolates reasonably beyond the training data (past year 3)\n",
    "\n",
    "This is the power of additive models: each kernel explains a different aspect of variation, and the GP automatically learns how to attribute variation to each component.\n",
    "\n",
    "**Note**: We haven't captured the weekly pattern yet. That would require either:\n",
    "1. Adding another periodic component with period = 7 days (converted to years)\n",
    "2. Using a categorical variable for day-of-week (as in the births HSGP example in Session 3)\n",
    "\n",
    "Let's now explore multiplicative kernels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiplicative Kernels: When Patterns Interact\n",
    "\n",
    "Multiplication creates more complex behaviors. A product $k_1 \\times k_2$ forces the function to satisfy both kernels' constraints simultaneously. This is useful when one pattern **modulates** another.\n",
    "\n",
    "For example: imagine seasonal patterns whose amplitude grows over time. A purely additive model can't capture thisâ€”the seasonal oscillations would have constant amplitude. A multiplicative kernel can.\n",
    "\n",
    "Let's create synthetic data with growing seasonal amplitude and model it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data with growing seasonal amplitude\n",
    "X_mult = np.linspace(0, 5, 200)[:, None]\n",
    "\n",
    "# Seasonal pattern with amplitude that grows linearly\n",
    "amplitude = 1 + 0.5 * X_mult.flatten()\n",
    "y_mult = amplitude * np.sin(4 * np.pi * X_mult.flatten()) + 0.3 * rng.standard_normal(200)\n",
    "\n",
    "df_mult = pl.DataFrame({\n",
    "    'x': X_mult.flatten(),\n",
    "    'y': y_mult\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig = px.scatter(\n",
    "    df_mult.to_pandas(),\n",
    "    x='x',\n",
    "    y='y',\n",
    "    title='Data with Growing Seasonal Amplitude'\n",
    ")\n",
    "fig.update_traces(marker=dict(size=4, color='black'))\n",
    "fig.update_layout(width=900, height=500)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Multiplicative Pattern\n",
    "\n",
    "Notice how the oscillations grow in amplitude as x increases. An additive model `Linear + Periodic` couldn't capture thisâ€”it would just add a linear trend to constant-amplitude oscillations.\n",
    "\n",
    "Instead, we need `Linear Ã— Periodic`, where the linear component modulates the amplitude of the periodic component."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mult_model:\n",
    "    # Linear component for growing amplitude\n",
    "    c = pm.Normal('c', mu=0, sigma=2)\n",
    "    var_lin = pm.HalfNormal('var_lin', sigma=2)\n",
    "    cov_linear = var_lin * pm.gp.cov.Linear(1, c=c)\n",
    "    \n",
    "    # Periodic component\n",
    "    ell_per = pm.Gamma('ell_per', alpha=2, beta=2)\n",
    "    eta_per = pm.HalfNormal('eta_per', sigma=2)\n",
    "    period = 1.0\n",
    "    cov_periodic = eta_per**2 * pm.gp.cov.Periodic(1, period=period, ls=ell_per)\n",
    "    \n",
    "    # MULTIPLICATIVE KERNEL\n",
    "    cov_mult = cov_linear * cov_periodic\n",
    "    \n",
    "    gp = pm.gp.Marginal(cov_func=cov_mult)\n",
    "    \n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_mult, y=y_mult, sigma=sigma)\n",
    "    \n",
    "    mult_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='numpyro',\n",
    "        chains=2,\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Multiplicative Model\n",
    "\n",
    "The line `cov_mult = cov_linear * cov_periodic` creates a kernel where:\n",
    "- The periodic component creates oscillations\n",
    "- The linear component modulates their amplitude\n",
    "- Together, they create growing seasonal patterns\n",
    "\n",
    "Let's see how well it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X_pred_mult = np.linspace(-0.5, 6, 300)[:, None]\n",
    "\n",
    "with mult_model:\n",
    "    f_pred_mult = gp.conditional('f_pred', X_pred_mult)\n",
    "    ppc_mult = pm.sample_posterior_predictive(\n",
    "        mult_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize\n",
    "f_pred_mult_samples = az.extract(\n",
    "    ppc_mult.posterior_predictive,\n",
    "    var_names=['f_pred']\n",
    ")['f_pred'].values\n",
    "\n",
    "f_pred_mult_mean = f_pred_mult_samples.mean(axis=1)\n",
    "f_pred_mult_lower = np.percentile(f_pred_mult_samples, 2.5, axis=1)\n",
    "f_pred_mult_upper = np.percentile(f_pred_mult_samples, 97.5, axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_mult.flatten(),\n",
    "    y=f_pred_mult_upper,\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_mult.flatten(),\n",
    "    y=f_pred_mult_lower,\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "    line=dict(width=0),\n",
    "    name='95% CI'\n",
    "))\n",
    "\n",
    "# Mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_mult.flatten(),\n",
    "    y=f_pred_mult_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior mean',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# Data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_mult['x'],\n",
    "    y=df_mult['y'],\n",
    "    mode='markers',\n",
    "    name='Data',\n",
    "    marker=dict(size=4, color='black')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Multiplicative GP: Growing Seasonal Amplitude',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Multiplicative Results\n",
    "\n",
    "Beautiful! The model has captured the growing amplitude of the oscillations. Notice how:\n",
    "- The oscillation frequency stays constant (determined by the periodic kernel)\n",
    "- The amplitude grows linearly (determined by the linear kernel)\n",
    "- The interaction between them creates the observed pattern\n",
    "\n",
    "When you extrapolate past $x=5$, the model continues this patternâ€”oscillations with ever-growing amplitude. This is sometimes exactly what you want, but be cautious: multiplicative models can extrapolate in surprising ways.\n",
    "\n",
    "**Key takeaway**: Use multiplication when one pattern modulates another. Use addition when patterns are independent sources of variation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE 2: Additive Kernel Model\n",
    "\n",
    "Your turn! Build an additive GP model that combines a long-term trend with seasonal variation.\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the function below that builds an additive GP model combining:\n",
    "1. A MatÃ©rn 5/2 kernel for the trend (with long lengthscale)\n",
    "2. A Periodic kernel for seasonality (with specified period)\n",
    "3. Use `pm.gp.Marginal` for efficient inference\n",
    "\n",
    "**Prompt suggestion**: \"Help me define and fit an additive GP in PyMC that models a long-term trend using Matern52 and a seasonal component using Periodic kernel. The model should combine them with addition and use pm.gp.Marginal.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_trend_seasonality_gp(X, y, period=1.0):\n",
    "    \"\"\"\n",
    "    Combine ExpQuad/Matern (trend) plus Periodic (seasonality) in pm.gp.Marginal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, 1)\n",
    "        Input time points\n",
    "    y : array, shape (n,)\n",
    "        Observations\n",
    "    period : float\n",
    "        Period of seasonal variation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : pm.Model\n",
    "        The PyMC model\n",
    "    trace : az.InferenceData\n",
    "        Posterior samples\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    # Hints:\n",
    "    # - Use separate hyperparameters for trend and seasonal components\n",
    "    # - Combine kernels with: cov_total = cov_trend + cov_seasonal\n",
    "    # - Don't forget to specify sigma for observation noise\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# model, trace = additive_trend_seasonality_gp(X_train_births, y_train_births, period=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.6: Non-Gaussian Likelihoods - Robust Regression with Student-T\n",
    "\n",
    "So far, we've used Gaussian noise (in `pm.gp.Marginal`) or Bernoulli outcomes (in classification). But real data often contains **outliers**â€”observations that don't fit the typical pattern. Gaussian distributions are not robust to outliers: a single extreme value can dramatically affect inference.\n",
    "\n",
    "The **Student-T distribution** offers a robust alternative. It has heavier tails than a Gaussian, meaning it assigns higher probability to extreme values. This makes it much more tolerant of outliers.\n",
    "\n",
    "### Why Student-T for Robust Regression?\n",
    "\n",
    "The Student-T distribution has three parameters:\n",
    "- **Location ($\\mu$)**: The center, similar to the mean\n",
    "- **Scale ($\\sigma$)**: The spread, similar to standard deviation\n",
    "- **Degrees of freedom ($\\nu$)**: Controls tail heaviness\n",
    "\n",
    "The degrees of freedom parameter is key:\n",
    "- Small $\\nu$ (e.g., 2-5) â†’ very heavy tails, very robust to outliers\n",
    "- Large $\\nu$ (> 30) â†’ approaches normal distribution\n",
    "\n",
    "Since we need `pm.gp.Latent` for non-Gaussian likelihoods, let's see robust regression in action with Student-T likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Data with Outliers\n",
    "\n",
    "Let's generate data where most observations are clean, but a few are extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with outliers\n",
    "n_robust = 100\n",
    "X_robust = np.linspace(0, 10, n_robust)[:, None]\n",
    "\n",
    "# True function\n",
    "f_true_robust = 2 * np.sin(1.5 * X_robust.flatten())\n",
    "\n",
    "# Most observations have small Gaussian noise\n",
    "y_robust = f_true_robust + 0.3 * rng.standard_normal(n_robust)\n",
    "\n",
    "# Add outliers: replace 10% of points with extreme values\n",
    "n_outliers = 10\n",
    "outlier_indices = rng.choice(n_robust, size=n_outliers, replace=False)\n",
    "y_robust[outlier_indices] += rng.choice([-1, 1], size=n_outliers) * rng.uniform(3, 6, size=n_outliers)\n",
    "\n",
    "df_robust = pl.DataFrame({\n",
    "    'x': X_robust.flatten(),\n",
    "    'y': y_robust,\n",
    "    'f_true': f_true_robust,\n",
    "    'is_outlier': [i in outlier_indices for i in range(n_robust)]\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "# Regular points\n",
    "df_regular = df_robust.filter(pl.col('is_outlier') == False)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_regular['x'],\n",
    "    y=df_regular['y'],\n",
    "    mode='markers',\n",
    "    name='Regular observations',\n",
    "    marker=dict(size=5, color='black')\n",
    "))\n",
    "\n",
    "# Outliers\n",
    "df_outlier = df_robust.filter(pl.col('is_outlier') == True)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_outlier['x'],\n",
    "    y=df_outlier['y'],\n",
    "    mode='markers',\n",
    "    name='Outliers',\n",
    "    marker=dict(size=8, color='red', symbol='x')\n",
    "))\n",
    "\n",
    "# True function\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_robust['x'],\n",
    "    y=df_robust['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function',\n",
    "    line=dict(color='dodgerblue', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Data with Outliers',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Outliers\n",
    "\n",
    "The red X markers show outliersâ€”observations that are far from the true function (blue line). If we used a Gaussian likelihood, these outliers would pull our posterior predictions toward them, distorting our estimate of the true function.\n",
    "\n",
    "Let's build a robust model using a Student-T likelihood with `pm.gp.Latent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as robust_model:\n",
    "    # Hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=3)\n",
    "    \n",
    "    # Covariance function\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    # Latent GP\n",
    "    gp = pm.gp.Latent(cov_func=cov_func)\n",
    "    f = gp.prior('f', X=X_robust)\n",
    "    \n",
    "    # Student-T likelihood for robustness\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    nu = pm.Gamma('nu', alpha=2, beta=0.1)  # Prior favors low nu (heavy tails)\n",
    "    \n",
    "    y_obs = pm.StudentT(\n",
    "        'y',\n",
    "        mu=f,\n",
    "        sigma=sigma,\n",
    "        nu=nu,\n",
    "        observed=y_robust\n",
    "    )\n",
    "    \n",
    "    # Sample\n",
    "    robust_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='numpyro',\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Robust Model\n",
    "\n",
    "The key difference from our earlier Gaussian models is the likelihood:\n",
    "\n",
    "```python\n",
    "y_obs = pm.StudentT('y', mu=f, sigma=sigma, nu=nu, observed=y_robust)\n",
    "```\n",
    "\n",
    "We're learning the degrees of freedom parameter `nu` from the data. The prior `Gamma(2, 0.1)` suggests we expect heavy tails (small nu), but leaves room for the data to tell us otherwise.\n",
    "\n",
    "Let's see how well this handles the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the learned nu parameter\n",
    "nu_summary = az.summary(robust_trace, var_names=['nu'])\n",
    "print(\"Degrees of freedom posterior:\")\n",
    "print(nu_summary)\n",
    "print(f\"\\nMean nu: {nu_summary['mean'].values[0]:.2f}\")\n",
    "print(\"(Small nu indicates heavy tails were needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Robust Predictions\n",
    "\n",
    "Now let's predict and see if the model successfully ignored the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X_pred_robust = np.linspace(-1, 11, 300)[:, None]\n",
    "\n",
    "with robust_model:\n",
    "    f_pred_robust = gp.conditional('f_pred', X_pred_robust)\n",
    "    ppc_robust = pm.sample_posterior_predictive(\n",
    "        robust_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=rng\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract and visualize\n",
    "f_pred_robust_samples = az.extract(\n",
    "    ppc_robust.posterior_predictive,\n",
    "    var_names=['f_pred']\n",
    ")['f_pred'].values\n",
    "\n",
    "f_pred_robust_mean = f_pred_robust_samples.mean(axis=1)\n",
    "f_pred_robust_lower = np.percentile(f_pred_robust_samples, 2.5, axis=1)\n",
    "f_pred_robust_upper = np.percentile(f_pred_robust_samples, 97.5, axis=1)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_robust.flatten(),\n",
    "    y=f_pred_robust_upper,\n",
    "    mode='lines',\n",
    "    line=dict(width=0),\n",
    "    showlegend=False\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_robust.flatten(),\n",
    "    y=f_pred_robust_lower,\n",
    "    mode='lines',\n",
    "    fill='tonexty',\n",
    "    fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "    line=dict(width=0),\n",
    "    name='95% CI'\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=X_pred_robust.flatten(),\n",
    "    y=f_pred_robust_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior mean',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# True function\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_robust['x'],\n",
    "    y=df_robust['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function',\n",
    "    line=dict(color='dodgerblue', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Regular data\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_regular['x'],\n",
    "    y=df_regular['y'],\n",
    "    mode='markers',\n",
    "    name='Regular observations',\n",
    "    marker=dict(size=5, color='black')\n",
    "))\n",
    "\n",
    "# Outliers\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_outlier['x'],\n",
    "    y=df_outlier['y'],\n",
    "    mode='markers',\n",
    "    name='Outliers',\n",
    "    marker=dict(size=8, color='red', symbol='x')\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Robust GP with Student-T Likelihood',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Robust Results\n",
    "\n",
    "Excellent! The posterior mean (red line) closely follows the true function (blue dashed line), essentially ignoring the outliers (red X markers). The Student-T likelihood has successfully downweighted the extreme observations.\n",
    "\n",
    "Compare this to what would happen with a Gaussian likelihoodâ€”the outliers would pull the predicted function toward them, creating bulges in the estimate. The Student-T likelihood's heavy tails allow the model to say \"these observations are just noise from the tail of the distribution,\" rather than trying to fit them.\n",
    "\n",
    "**When to use robust likelihoods**:\n",
    "- When you suspect your data contains outliers\n",
    "- When measurement errors might occasionally be much larger than usual\n",
    "- When you want your model to be less sensitive to a few extreme values\n",
    "\n",
    "The cost is slower sampling (since we use `pm.gp.Latent`), but the benefit is much more reliable inference in the presence of outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE 3: Compare Marginal vs Latent GPs\n",
    "\n",
    "Your final exercise: implement both approaches on the same dataset and compare their speed and accuracy.\n",
    "\n",
    "### Task\n",
    "\n",
    "Complete the function below that:\n",
    "1. Fits the same data with both `pm.gp.Marginal` (Gaussian likelihood)\n",
    "2. And `pm.gp.Latent` (also with Gaussian likelihood for fair comparison)\n",
    "3. Reports fit time and predictive quality (e.g., RMSE on held-out data)\n",
    "\n",
    "**Prompt suggestion**: \"Help me implement two GP fits on the same data: one using pm.gp.Marginal and one using pm.gp.Latent with Normal likelihood. Compare their sampling time and posterior predictive performance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_marginal_vs_latent(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Fit the same dataset with pm.gp.Marginal and pm.gp.Latent,\n",
    "    report fit time and predictive quality.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X_train, y_train : arrays\n",
    "        Training data\n",
    "    X_test, y_test : arrays\n",
    "        Test data for evaluation\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    results : dict\n",
    "        Dictionary with timing and performance metrics\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    # Hints:\n",
    "    # - Use time.time() to measure sampling duration\n",
    "    # - For Latent model, use pm.Normal likelihood with same parameters as Marginal\n",
    "    # - Compute RMSE on test set for both models\n",
    "    # - Return dict with keys: 'marginal_time', 'latent_time', 'marginal_rmse', 'latent_rmse'\n",
    "    pass\n",
    "\n",
    "# Test on the training data from earlier\n",
    "# Split into train/test\n",
    "# n_test = 20\n",
    "# test_indices = rng.choice(len(X_train), size=n_test, replace=False)\n",
    "# train_mask = np.ones(len(X_train), dtype=bool)\n",
    "# train_mask[test_indices] = False\n",
    "#\n",
    "# results = compare_marginal_vs_latent(\n",
    "#     X_train[train_mask], y_train[train_mask],\n",
    "#     X_train[test_indices], y_train[test_indices]\n",
    "# )\n",
    "# print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.7: Summary and Next Steps\n",
    "\n",
    "Congratulations! You've now mastered the core modeling choices in Gaussian process regression:\n",
    "\n",
    "### Key Concepts from Session 2\n",
    "\n",
    "**The Kernel Zoo**: We explored how different kernels encode different assumptions:\n",
    "- ExpQuad, MatÃ©rn 5/2, and MatÃ©rn 3/2 for varying smoothness\n",
    "- Periodic for seasonal patterns\n",
    "- Linear for trends and non-stationary behavior\n",
    "\n",
    "**Marginal vs Latent**: You learned when to use each approach:\n",
    "- `pm.gp.Marginal`: Fast, efficient, limited to Gaussian likelihoods\n",
    "- `pm.gp.Latent`: Flexible, works with any likelihood, slower\n",
    "\n",
    "**Kernel Composition**: The power of combining kernels:\n",
    "- Addition for independent effects (trend + seasonality)\n",
    "- Multiplication for interactions (modulating amplitude)\n",
    "\n",
    "**Non-Gaussian Likelihoods**: How to handle:\n",
    "- Binary outcomes with Bernoulli likelihood\n",
    "- Outliers with Student-T likelihood\n",
    "- Any distribution supported by PyMC\n",
    "\n",
    "### Practical Guidance\n",
    "\n",
    "When building your own GP models:\n",
    "\n",
    "1. **Start simple**: Begin with a single kernel (usually MatÃ©rn 5/2) and Gaussian likelihood\n",
    "2. **Examine residuals**: Plot what your model gets wrongâ€”this often reveals missing structure\n",
    "3. **Add complexity**: If you see periodic patterns, add a Periodic kernel. If you see outliers, consider Student-T\n",
    "4. **Compare models**: Use LOO-CV (with ArviZ) to compare different kernel choices\n",
    "5. **Check diagnostics**: Always verify R-hat, ESS, and look for divergences\n",
    "\n",
    "### What's Next: Session 3\n",
    "\n",
    "Everything we've done so far has a major limitation: computational cost. Exact GP inference is $O(n^3)$ in the number of data points, making it impractical for datasets with thousands of observations.\n",
    "\n",
    "In Session 3, we'll tackle this head-on:\n",
    "- **Sparse GPs** with inducing points that approximate the full GP\n",
    "- **Hilbert Space GPs (HSGP)** that use basis function expansions\n",
    "- **Practical strategies** for choosing approximation parameters\n",
    "- **When approximations are (and aren't) appropriate**\n",
    "\n",
    "These techniques will let you scale GPs to large datasets while maintaining the flexibility we've explored today. See you in Session 3!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
