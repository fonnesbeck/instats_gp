{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_2.ipynb)\n",
    "\n",
    "# Session 2: Kernels, Likelihoods, and Model Building\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will:\n",
    "\n",
    "- Understand different kernel families and their properties\n",
    "- Learn kernel composition via addition and multiplication\n",
    "- Understand marginal vs latent GP formulations\n",
    "- Work with non-Gaussian likelihoods\n",
    "- Build additive and multiplicative models for real-world data\n",
    "\n",
    "This session builds directly on Session 1's foundations. We'll move beyond basic GP models to explore the rich toolkit of covariance functions and likelihood choices that make Gaussian processes so flexible for real-world problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pymc as pm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:= 8675309)\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "OUTPUT_DIR = \"../output/\"\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Session 1, we built intuition about Gaussian processes by connecting multivariate normal distributions to functions. We learned that the **covariance function** (or kernel) is the heart of a GPâ€”it encodes our assumptions about how similar function values should be at different input locations. We also saw how to build simple GP models in PyMC using `pm.gp.Marginal` with Gaussian noise.\n",
    "\n",
    "This session takes us deeper. Real-world problems rarely involve perfectly smooth functions with Gaussian noise. You might encounter:\n",
    "\n",
    "- **Periodic patterns** that repeat over time (think seasonal sales, daily temperatures)\n",
    "- **Long-term trends** which require longer length scales\n",
    "- **Binary outcomes** where we need classification, not regression\n",
    "- **Heavy-tailed noise** that makes outliers more likely than a normal distribution suggests\n",
    "\n",
    "Fortunately, the GP framework is remarkably flexible. By choosing appropriate kernels and likelihoods, we can handle all these scenarios. Let's see how."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.1: The Kernel Zoo\n",
    "\n",
    "Kernels are the building blocks that define the structure of our GP. Different kernels encode different assumptions about function smoothness, periodicity, and behavior. Let's explore the most important kernel families and develop intuition for when to use each one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_prior_samples(X, cov_func, n_samples=5, title=\"GP Prior Samples\"):\n",
    "    \"\"\"\n",
    "    Plot samples from a GP prior to visualize kernel behavior.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array\n",
    "        Input locations (1D array, will be reshaped to column vector)\n",
    "    cov_func : PyMC covariance function\n",
    "        The kernel to visualize\n",
    "    n_samples : int\n",
    "        Number of function samples to draw\n",
    "    title : str\n",
    "        Plot title\n",
    "    \"\"\"\n",
    "    X = X.reshape(-1, 1)\n",
    "    \n",
    "    # Compute covariance matrix and add jitter for numerical stability\n",
    "    K = cov_func(X).eval() + 1e-6 * np.eye(len(X))\n",
    "    \n",
    "    # Draw samples from the GP prior\n",
    "    samples = RNG.multivariate_normal(np.zeros(len(X)), K, size=n_samples)\n",
    "    \n",
    "    # Create plotly figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=X.flatten(),\n",
    "            y=samples[i],\n",
    "            mode='lines',\n",
    "            name=f'Sample {i+1}',\n",
    "            opacity=0.7,\n",
    "            line=dict(width=2)\n",
    "        ))\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=title,\n",
    "        xaxis_title='X',\n",
    "        yaxis_title='f(x)',\n",
    "        hovermode='x unified',\n",
    "        width=900,\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MatÃ©rn Family\n",
    "\n",
    "The MatÃ©rn kernels generalize the ExpQuad by controlling smoothness through a parameter $\\nu$. Smaller $\\nu$ means less smooth. PyMC provides three variants:\n",
    "\n",
    "- **MatÃ©rn 1/2**: Continuous but not differentiable (roughest)\n",
    "- **MatÃ©rn 3/2**: Once differentiable (medium smoothness)\n",
    "- **MatÃ©rn 5/2**: Twice differentiable (smooth, but not as extreme as ExpQuad)\n",
    "\n",
    "Use MatÃ©rn when you want control over smoothness or when your function might not be infinitely smooth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create input points\n",
    "X_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "# Fixed parameters for fair comparison\n",
    "lengthscale = 1.0\n",
    "amplitude = 1.0\n",
    "\n",
    "# Define three kernels with different smoothness properties\n",
    "kernels = {\n",
    "    'ExpQuad (RBF)': amplitude**2 * pm.gp.cov.ExpQuad(1, lengthscale),\n",
    "    'Matern52': amplitude**2 * pm.gp.cov.Matern52(1, lengthscale),\n",
    "    'Matern32': amplitude**2 * pm.gp.cov.Matern32(1, lengthscale),\n",
    "}\n",
    "\n",
    "# Plot prior samples for each kernel\n",
    "for name, cov in kernels.items():\n",
    "    fig = plot_gp_prior_samples(X_grid, cov, n_samples=5, title=f'{name} Kernel - Prior Samples')\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the functions become progressively smoother as we go from MatÃ©rn 1/2 to 5/2. The MatÃ©rn 1/2 (also called Exponential kernel) produces jagged, continuous but non-differentiable functionsâ€”perfect for modeling rough phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Rational Quadratic Kernel: Modeling Multiple Scales\n",
    "\n",
    "So far, we've seen kernels with a single lengthscale parameter that controls how quickly correlations decay with distance. But what if your data has variation at multiple timescales simultaneously? Imagine a phenomenon with both short-term fluctuations and long-term trends that can't be cleanly decomposed into additive components.\n",
    "\n",
    "The **Rational Quadratic (RQ) kernel** addresses this by acting as a **scale mixture**â€”essentially combining many squared exponential kernels with different lengthscales. Mathematically:\n",
    "\n",
    "$$\n",
    "k_{RQ}(x, x') = \\sigma^2 \\left(1 + \\frac{(x - x')^2}{2\\alpha\\ell^2}\\right)^{-\\alpha}\n",
    "$$\n",
    "\n",
    "The key parameter is $\\alpha$ (alpha), which controls the mixture:\n",
    "- **Small Î±** (e.g., 0.5-2): Heavy-tailed behavior, more variation across scales\n",
    "- **Large Î±** (e.g., 100+): Approaches the squared exponential kernel\n",
    "- **Î± â†’ âˆž**: Becomes exactly the ExpQuad kernel\n",
    "\n",
    "Think of Î± as controlling how \"multi-scale\" your function is. When you're not sure whether your phenomenon has one dominant lengthscale or many, the RQ kernel lets the data decide.\n",
    "\n",
    "Let's visualize how Î± affects the prior samples to build intuition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "lengthscale = 1.0\n",
    "amplitude = 1.0\n",
    "\n",
    "alpha_values = [0.5, 2.0, 10.0, 100.0]\n",
    "\n",
    "for alpha in alpha_values:\n",
    "    cov = amplitude**2 * pm.gp.cov.RatQuad(1, ls=lengthscale, alpha=alpha)\n",
    "    fig = plot_gp_prior_samples(\n",
    "        X_grid,\n",
    "        cov,\n",
    "        n_samples=5,\n",
    "        title=f'Rational Quadratic Kernel (Î± = {alpha})'\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Rational Quadratic Through Its Samples\n",
    "\n",
    "Looking at these prior samples across different Î± values reveals important patterns:\n",
    "\n",
    "**With Î± = 0.5**: The functions are quite rough and can change behavior dramatically over short distances. This heavy-tailed behavior allows for surprising local variations while still maintaining some global structure. You'd use this when you expect your data to have unpredictable multi-scale variation.\n",
    "\n",
    "**With Î± = 2.0**: We see moderate smoothness with the ability to capture both rapid local changes and slower global trends. This is often a good middle ground when you're unsure about the scale structure of your data.\n",
    "\n",
    "**With Î± = 10.0**: The functions become noticeably smoother and more predictable, approaching the behavior of our MatÃ©rn kernels. The multi-scale mixing is still present but less pronounced.\n",
    "\n",
    "**With Î± = 100.0**: These functions are nearly identical to what we'd get from an ExpQuad kernelâ€”silky smooth and infinitely differentiable. At this point, there's effectively only one dominant lengthscale.\n",
    "\n",
    "**Practical guidance**: Start with MatÃ©rn 5/2 for most problems. If you find yourself torn between multiple lengthscale values (the data seems to have both fine details and broad trends), try RQ with Î± ~ 2-5. If posterior estimates push Î± very high (> 50), you might as well use ExpQuad. If Î± stays very low (< 1), your data truly has multi-scale structure that can't be captured by a single lengthscale.\n",
    "\n",
    "**Important caveat**: The RQ kernel can be more challenging for inference because it has one more hyperparameter than ExpQuad or MatÃ©rn. Use it when the extra flexibility is genuinely needed, not as a default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Periodic Kernels for Seasonal Patterns\n",
    "\n",
    "Many real-world phenomena repeat: daily temperature cycles, weekly sales patterns, annual seasonal effects. For these, we need a kernel that encodes periodicity. The Periodic kernel is perfect for this.\n",
    "\n",
    "The Periodic kernel has two key parameters beyond amplitude:\n",
    "- **period**: The length of one complete cycle\n",
    "- **lengthscale**: Controls smoothness *within* each period (how similar nearby points within a cycle should be)\n",
    "\n",
    "Let's visualize periodic patterns and see how they differ from the stationary kernels we've explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate periodic kernel with annual period\n",
    "X_grid = np.linspace(0, 20, 400)  # Extended range to see multiple periods\n",
    "\n",
    "period = 5.0  # Complete cycle every 5 units\n",
    "lengthscale = 1.0\n",
    "amplitude = 1.0\n",
    "\n",
    "cov_periodic = amplitude**2 * pm.gp.cov.Periodic(1, period=period, ls=lengthscale)\n",
    "\n",
    "fig = plot_gp_prior_samples(\n",
    "    X_grid, \n",
    "    cov_periodic, \n",
    "    n_samples=5,\n",
    "    title=f'Periodic Kernel (period={period}, lengthscale={lengthscale})'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Periodic Patterns\n",
    "\n",
    "Notice how the functions repeat with the specified period. The pattern from $x=0$ to $x=5$ is similar to the pattern from $x=5$ to $x=10$, and so on. This is exactly what we want for seasonal data.\n",
    "\n",
    "The lengthscale parameter controls smoothness *within* each period. A smaller lengthscale would allow more wiggly behavior within each cycle, while a larger lengthscale would make each cycle smoother.\n",
    "\n",
    "**Real-world example**: If you're modeling daily data with weekly patterns, you'd set `period=7`. The GP would then learn that Mondays tend to be similar to other Mondays, Tuesdays to Tuesdays, and so forth, while still being flexible about the exact pattern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Linear Kernel for Trends\n",
    "\n",
    "Sometimes your data contains linear or polynomial trends. The Linear kernel captures this by computing covariance proportional to the inner product of input locations. Unlike the stationary kernels we've seen, the Linear kernel is *non-stationary*â€”the covariance depends on the absolute position, not just the distance between points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate linear kernel\n",
    "X_grid = np.linspace(0, 10, 200)\n",
    "\n",
    "c = 1.0  # Center point\n",
    "variance = 1.0\n",
    "\n",
    "cov_linear = variance * pm.gp.cov.Linear(1, c=c)\n",
    "\n",
    "fig = plot_gp_prior_samples(\n",
    "    X_grid,\n",
    "    cov_linear,\n",
    "    n_samples=5,\n",
    "    title='Linear Kernel - Prior Samples'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Linear Kernel\n",
    "\n",
    "These functions are predominantly linear, though you'll notice they're not perfectly straight linesâ€”there's some flexibility around the linear trend. The Linear kernel alone is rarely sufficient for real data, but it becomes powerful when *combined* with other kernels (which we'll explore in Section 2.4).\n",
    "\n",
    "The parameter `c` acts as a center point or offset. The covariance between two points $x$ and $x'$ is proportional to $(x - c)(x' - c)$. This makes the variance grow as you move away from $c$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the Right Kernel: A Practical Guide\n",
    "\n",
    "Now that we've explored the kernel zoo, you might be wondering: \"How do I choose the right kernel for my data?\" The choice of kernel determines almost all the generalization properties of your GP model, so this decision matters immensely.\n",
    "\n",
    "Here's a practical decision framework based on what you know (or suspect) about your data:\n",
    "\n",
    "#### Start With These Questions\n",
    "\n",
    "**Is your function smooth everywhere, or does it have sharp features?**\n",
    "- Smooth everywhere â†’ ExpQuad or MatÃ©rn 5/2\n",
    "- Occasional sharp features or kinks â†’ MatÃ©rn 3/2\n",
    "- Very rough or irregular â†’ MatÃ©rn 1/2 (rarely used)\n",
    "\n",
    "**Does your data have obvious periodic patterns?**\n",
    "- Clear, exact repetition â†’ Periodic kernel (or composed with others)\n",
    "- Periodic but changing over time â†’ Locally Periodic (ExpQuad Ã— Periodic)\n",
    "- No periodicity â†’ Skip Periodic entirely\n",
    "\n",
    "**Are there multiple timescales or characteristic lengths?**\n",
    "- Single dominant scale â†’ MatÃ©rn 5/2 or ExpQuad\n",
    "- Multiple scales that can be separated â†’ Additive composition (kâ‚ + kâ‚‚)\n",
    "- Multiple scales that interact â†’ Rational Quadratic or multiplicative composition\n",
    "\n",
    "**Is there a trend component?**\n",
    "- Linear trend â†’ Include Linear kernel (usually composed with others)\n",
    "- Polynomial trend â†’ Linear Ã— Linear for quadratic, etc.\n",
    "- Complex trend â†’ Use smooth kernel with long lengthscale\n",
    "\n",
    "#### The Conservative Default\n",
    "\n",
    "**When in doubt, start with MatÃ©rn 5/2.** It's smooth enough for most applications without being overly smooth like ExpQuad, and it generalizes well across problem domains. This has become the de facto recommendation in modern GP practice.\n",
    "\n",
    "#### Warning Signs of Poor Kernel Choice\n",
    "\n",
    "Watch for these indicators that your kernel might be misspecified:\n",
    "\n",
    "**Shrinking lengthscale with more data**: If your lengthscale posterior keeps getting smaller as you add more observations, your kernel is likely too smooth for the data. The model is trying to capture roughness by shrinking the lengthscale indefinitely. Consider switching to a rougher kernel (e.g., MatÃ©rn 5/2 â†’ MatÃ©rn 3/2) or adding complexity through composition.\n",
    "\n",
    "**Very wide posterior distributions**: If hyperparameter posteriors are extremely broad even with substantial data, the kernel might not match the data structure. Try a different kernel family or consider composition.\n",
    "\n",
    "**Systematic residual patterns**: If your posterior predictive checks reveal clear patterns in the residuals (e.g., unmodeled periodicity, unmodeled trend), your kernel is missing structure. Add appropriate components through composition.\n",
    "\n",
    "Let's move on to see these kernels in action through model building."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Workflow for Kernel Selection\n",
    "\n",
    "Rather than agonizing over the perfect kernel upfront, follow this iterative workflow:\n",
    "\n",
    "**Step 1: Start Simple**\n",
    "\n",
    "Begin with a single MatÃ©rn 5/2 kernel with weakly informative priors. This gives you a baseline and helps you understand the data's basic structure.\n",
    "\n",
    "**Step 2: Examine Residuals**\n",
    "\n",
    "After fitting, plot the difference between your observations and posterior mean predictions. Look for patterns:\n",
    "- Regular oscillations â†’ Consider adding Periodic\n",
    "- Long-term drift â†’ Consider adding Linear or smooth kernel with long lengthscale\n",
    "- Different behavior in different regions â†’ Consider multiplicative compositions or change-point models\n",
    "- Outliers â†’ Consider robust likelihoods (Student-T)\n",
    "\n",
    "**Step 3: Add Complexity Incrementally**\n",
    "\n",
    "Based on residual patterns, add kernel components one at a time. Refit after each addition and verify the new component improves the model.\n",
    "\n",
    "This workflow prevents both underfitting (too simple) and overfitting (too complex) while maintaining computational efficiency. Now let's see how to actually build and fit these models in PyMC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.2: Marginal Likelihood with pm.gp.Marginal\n",
    "\n",
    "When your data are continuous observations with Gaussian noise, `pm.gp.Marginal` provides the most efficient implementation. It analytically integrates out the latent GP function, directly computing the marginal likelihood $p(y | x)$ without sampling the intermediate function values.\n",
    "\n",
    "### Mathematical Intuition\n",
    "\n",
    "Recall that a GP models a function $f(x)$, and our observations are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x) &\\sim \\mathcal{GP}(m(x), k(x, x')) \\\\\n",
    "y &= f(x) + \\epsilon \\\\\n",
    "\\epsilon &\\sim \\mathcal{N}(0, \\sigma^2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Because both the GP prior and the Gaussian noise are normal distributions, we can analytically integrate out $f$ to get:\n",
    "\n",
    "$$\n",
    "y \\sim \\mathcal{N}(m(X), K(X, X) + \\sigma^2 I)\n",
    "$$\n",
    "\n",
    "This marginalization is exact and efficientâ€”we don't need to sample the potentially high-dimensional latent function. The `marginal_likelihood` method implements this approach.\n",
    "\n",
    "### When to Use pm.gp.Marginal\n",
    "\n",
    "Choose `pm.gp.Marginal` when:\n",
    "- Your likelihood is Gaussian (normal)\n",
    "- You have continuous-valued observations\n",
    "- You want the fastest inference possible\n",
    "- You don't need samples from the latent function itself\n",
    "\n",
    "Let's work through a complete example using real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Recovering a Known Latent Function\n",
    "\n",
    "We'll start with a controlled example where we know the true underlying function. This lets us see exactly how well the GP recovers the latent function from noisy observationsâ€”a perfect way to build intuition before tackling real-world data with multiple patterns.\n",
    "\n",
    "We'll generate synthetic data by:\n",
    "1. Sampling a single function from a MatÃ©rn 5/2 GP prior (our \"ground truth\")\n",
    "2. Adding Gaussian noise to create observations\n",
    "3. Using `pm.gp.Marginal` to recover the latent function from the noisy data\n",
    "\n",
    "This approach removes domain complexity and lets us focus purely on GP mechanics: how the model learns lengthscales, amplitudes, and noise levels from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simulated data from a known GP\n",
    "np.random.seed(RANDOM_SEED)  # For reproducibility\n",
    "\n",
    "n = 100  # Number of data points\n",
    "X = np.linspace(0, 10, n)[:, None]  # Inputs arranged as column vector\n",
    "\n",
    "# Define the true covariance function and parameters\n",
    "ell_true = 1.0\n",
    "eta_true = 3.0\n",
    "cov_func_true = eta_true**2 * pm.gp.cov.Matern52(1, ell_true)\n",
    "\n",
    "# Zero mean function\n",
    "mean_func = pm.gp.mean.Zero()\n",
    "\n",
    "# Draw one sample from the GP prior - this is our \"true\" latent function\n",
    "K_true = cov_func_true(X).eval() + 1e-8 * np.eye(n)  # Add jitter for numerical stability\n",
    "f_true = RNG.multivariate_normal(mean_func(X).eval(), K_true)\n",
    "\n",
    "# Add Gaussian noise to create observations\n",
    "sigma_true = 2.0\n",
    "y = f_true + sigma_true * RNG.standard_normal(n)\n",
    "\n",
    "# Create polars dataframe for plotting\n",
    "df_sim = pl.DataFrame({\n",
    "    'x': X.flatten(),\n",
    "    'y': y,\n",
    "    'f_true': f_true\n",
    "})\n",
    "\n",
    "# Visualize the data\n",
    "fig = go.Figure()\n",
    "\n",
    "# True latent function\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_sim['x'],\n",
    "    y=df_sim['f_true'],\n",
    "    mode='lines',\n",
    "    name='True latent function f(x)',\n",
    "    line=dict(color='dodgerblue', width=3)\n",
    "))\n",
    "\n",
    "# Noisy observations\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_sim['x'],\n",
    "    y=df_sim['y'],\n",
    "    mode='markers',\n",
    "    name='Observed data y = f(x) + noise',\n",
    "    marker=dict(size=5, color='black', opacity=0.6)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Simulated GP Data: Known Ground Truth',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Data Structure\n",
    "\n",
    "The blue line shows our true latent function $f(x)$â€”a single draw from a MatÃ©rn 5/2 GP prior. Notice its smooth, gradually varying behavior with occasional changes in direction. This is characteristic of the MatÃ©rn 5/2 kernel: twice differentiable but not infinitely smooth.\n",
    "\n",
    "The black points show our noisy observations $y = f(x) + \\epsilon$, where $\\epsilon \\sim \\mathcal{N}(0, 2^2)$. The noise obscures the true function, but there's clearly structure in the dataâ€”points nearby in $x$ tend to have similar $y$ values.\n",
    "\n",
    "Our task: recover the blue line (or something close to it) from only the black points. Can a GP learn the appropriate lengthscale, amplitude, and noise level to reconstruct the latent function? Let's find out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Model\n",
    "\n",
    "We'll specify priors for the hyperparameters (lengthscale, amplitude, and noise level), then use `pm.gp.Marginal` to efficiently compute the marginal likelihood. Since we generated this data, we know the true parameter values:\n",
    "\n",
    "- True lengthscale: $\\ell = 1.0$\n",
    "- True amplitude: $\\eta = 3.0$\n",
    "- True noise: $\\sigma = 2.0$\n",
    "\n",
    "We'll use weakly informative priors that don't reveal these valuesâ€”the model must learn them from data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as marginal_model:\n",
    "    # Priors for hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy('eta', beta=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    # Create the GP (mean function defaults to zero)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    \n",
    "    # Noise level\n",
    "    sigma = pm.HalfCauchy('sigma', beta=5)\n",
    "    \n",
    "    # Marginal likelihood\n",
    "    y_obs = gp.marginal_likelihood('y', X=X, y=y, sigma=sigma)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    marginal_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='nutpie',\n",
    "        chains=2,\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Model Components\n",
    "\n",
    "Let's break down what we specified:\n",
    "\n",
    "**Lengthscale prior (`ell`)**: Gamma(2, 1) has most mass between 0.5 and 5. This weakly suggests moderate correlation distances without being too informative. The true value (1.0) falls comfortably within this range.\n",
    "\n",
    "**Amplitude prior (`eta`)**: HalfCauchy(5) has heavy tails, allowing for a wide range of vertical scales. It's weakly informativeâ€”it won't force a particular amplitude but prevents unreasonably extreme values.\n",
    "\n",
    "**Noise prior (`sigma`)**: Another HalfCauchy(5), placing weak prior mass on moderate noise levels. The data will strongly inform this parameter since we have 100 observations.\n",
    "\n",
    "The key method call is `gp.marginal_likelihood`, which analytically integrates out the latent function $f$ to directly compute $p(y | \\theta)$. This is much faster than sampling $f$ explicitly (which we'd need with `pm.gp.Latent`).\n",
    "\n",
    "Let's check if inference succeeded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Inference Diagnostics\n",
    "\n",
    "Before trusting our results, let's verify that sampling worked well. We'll check R-hat values and examine trace plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check R-hat values and ESS\n",
    "summary = az.summary(marginal_trace, var_names=['ell', 'eta', 'sigma'])\n",
    "print(summary)\n",
    "print(f\"\\nTrue parameter values:\")\n",
    "print(f\"  ell_true  = {ell_true}\")\n",
    "print(f\"  eta_true  = {eta_true}\")\n",
    "print(f\"  sigma_true = {sigma_true}\")\n",
    "\n",
    "# Check for divergences\n",
    "divergences = marginal_trace.sample_stats['diverging'].sum().item()\n",
    "print(f\"\\nNumber of divergences: {divergences}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Diagnostics\n",
    "\n",
    "Excellent! Our R-hat values are all close to 1.0, indicating convergence. The effective sample sizes (ESS) are in the hundreds, giving us reliable posterior estimates. Most importantly, zero divergences means NUTS had no trouble exploring the posterior.\n",
    "\n",
    "Looking at the posterior means, notice how close they are to the true values:\n",
    "- The posterior mean for `ell` should be near 1.0\n",
    "- The posterior mean for `eta` should be near 3.0\n",
    "- The posterior mean for `sigma` should be near 2.0\n",
    "\n",
    "The GP has successfully learned the hyperparameters from data alone! The posterior distributions capture our uncertaintyâ€”we're not perfectly certain about the true values (especially with only 100 data points), but the data provide strong evidence about the characteristic scales.\n",
    "\n",
    "Let's visualize the posterior to see this more clearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize posterior distributions\n",
    "az.plot_trace(marginal_trace, var_names=['ell', 'eta', 'sigma'], backend_kwargs={'constrained_layout': True});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions\n",
    "\n",
    "Now comes the exciting part: using our trained GP to make predictions at new input locations. We'll use the `conditional` method to compute the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New X locations for prediction (including extrapolation)\n",
    "X_new = np.linspace(-1, 12, 300)[:, None]\n",
    "\n",
    "with marginal_model:\n",
    "    # Conditional distribution for the latent function\n",
    "    f_pred = gp.conditional('f_pred', X_new)\n",
    "    \n",
    "    # Sample from the posterior predictive\n",
    "    marginal_ppc = pm.sample_posterior_predictive(\n",
    "        marginal_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Posterior Predictions\n",
    "\n",
    "Now comes the exciting part: can the GP recover the true latent function from noisy observations? Let's plot the posterior mean along with credible intervals to show our uncertainty. We'll also overlay the true function to see how close we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gp_posterior(ppc, X_pred, X, y, var_name='f_pred',\n",
    "                      title='GP Posterior Predictions', xaxis_title='X', yaxis_title='Y',\n",
    "                      show_training_range=True):\n",
    "\n",
    "    f_pred_samples = ppc.posterior_predictive[var_name].values\n",
    "\n",
    "    f_pred_mean = f_pred_samples.mean(axis=(0, 1))\n",
    "    f_pred_lower = np.percentile(f_pred_samples, 2.5, axis=(0, 1))\n",
    "    f_pred_upper = np.percentile(f_pred_samples, 97.5, axis=(0, 1))\n",
    "\n",
    "    # Calculate y-axis range from data\n",
    "    y_min = y.min()\n",
    "    y_max = y.max()\n",
    "    y_range = y_max - y_min\n",
    "    y_padding = 0.1 * y_range  # 10% padding\n",
    "\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=X_pred.flatten(),\n",
    "        y=f_pred_upper,\n",
    "        mode='lines',\n",
    "        line=dict(width=0),\n",
    "        showlegend=False,\n",
    "        hoverinfo='skip'\n",
    "    )).add_trace(go.Scatter(\n",
    "        x=X_pred.flatten(),\n",
    "        y=f_pred_lower,\n",
    "        mode='lines',\n",
    "        fill='tonexty',\n",
    "        fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "        line=dict(width=0),\n",
    "        name='95% Credible Interval'\n",
    "    )).add_trace(go.Scatter(\n",
    "        x=X_pred.flatten(),\n",
    "        y=f_pred_mean,\n",
    "        mode='lines',\n",
    "        name='Posterior mean',\n",
    "        line=dict(color='red', width=2)\n",
    "    )).add_trace(go.Scatter(\n",
    "        x=X.flatten(),\n",
    "        y=y,\n",
    "        mode='markers',\n",
    "        name='Observed data',\n",
    "        marker=dict(size=4, color='black')\n",
    "    )).update_layout(\n",
    "        title=title,\n",
    "        xaxis_title=xaxis_title,\n",
    "        yaxis_title=yaxis_title,\n",
    "        yaxis=dict(range=[y_min - y_padding, y_max + y_padding]),\n",
    "        hovermode='x unified',\n",
    "        width=900,\n",
    "        height=500,\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_gp_posterior(\n",
    "    marginal_ppc,\n",
    "    X_new,\n",
    "    X,\n",
    "    y,\n",
    "    title='GP Marginal: Posterior Predictions with Uncertainty',\n",
    "    xaxis_title='Years since 1990',\n",
    "    yaxis_title='COâ‚‚ (ppm)'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Predictions\n",
    "\n",
    "Remarkable! The GP has successfully recovered the true latent function (blue line) from noisy observations:\n",
    "\n",
    "**Within the training region** ($0 < x < 10$): The posterior mean (red line) tracks the true function closely. The 95% credible interval (shaded) captures the true function almost everywhere, reflecting appropriate uncertainty. Where observations are dense and informative, the credible interval is narrow. This is Bayesian inference at workâ€”the data strongly constrain what the latent function could be.\n",
    "\n",
    "**In extrapolation regions** ($x < 0$ or $x > 10$): Uncertainty grows rapidly. Without data to anchor predictions, the GP reverts to its prior belief: functions should stay near zero (the mean function) but could wander. Notice how the credible interval widensâ€”this is the model honestly admitting \"I don't know what happens out here.\"\n",
    "\n",
    "**Key insight**: The GP didn't just fit a curve through the pointsâ€”it learned the underlying **process** that generated the data. It inferred the lengthscale (how quickly correlations decay), the amplitude (vertical scale of variation), and the noise level (observation uncertainty). With these learned parameters, it reconstructed the latent function.\n",
    "\n",
    "This is the power of `pm.gp.Marginal` for Gaussian likelihood problems: fast, exact inference that properly quantifies uncertainty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE: Explore Different Kernels\n",
    "\n",
    "Now that you've seen a successful GP fit, try experimenting with different kernel choices!\n",
    "\n",
    "### Your Task\n",
    "\n",
    "Modify the model above to use different kernels and observe how they affect the fit. Try:\n",
    "\n",
    "1. **ExpQuad kernel**: Replace `Matern52` with `ExpQuad`â€”does it over-smooth or under-smooth?\n",
    "2. **MatÃ©rn 3/2 kernel**: Replace with `Matern32`â€”does it capture sharper features?\n",
    "3. **Misspecified prior**: Change the lengthscale prior to something that is too tight around an inappropriate valueâ€”does it still recover the truth?\n",
    "\n",
    "Use your LLM to help you make these modifications and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the kernel and re-fit\n",
    "\n",
    "with pm.Model() as exercise_model:\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfCauchy('eta', beta=5)\n",
    "    \n",
    "    # MODIFY THIS LINE - try different kernels\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    sigma = pm.HalfCauchy('sigma', beta=5)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X, y=y, sigma=sigma)\n",
    "    \n",
    "    exercise_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='nutpie',\n",
    "        chains=2,\n",
    "        random_seed=RNG\n",
    "    )\n",
    "\n",
    "# Visualize results\n",
    "az.plot_trace(exercise_trace, var_names=['ell', 'eta', 'sigma'], backend_kwargs={'constrained_layout': True});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the posterior predictions to see how your kernel choice affected the fit!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with exercise_model:\n",
    "    f_pred = gp.conditional('f_pred', X_new)\n",
    "    exercise_ppc = pm.sample_posterior_predictive(\n",
    "        exercise_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RNG\n",
    "    )\n",
    "\n",
    "# Plot using the same function\n",
    "plot_gp_posterior(\n",
    "    exercise_ppc,\n",
    "    X_new,\n",
    "    X,\n",
    "    y,\n",
    "    title='GP with Alternative Kernel',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.3: Latent GPs with pm.gp.Latent\n",
    "\n",
    "While `pm.gp.Marginal` is efficient for Gaussian likelihoods, many real-world problems don't produce continuous measurements with Gaussian noise. Consider:\n",
    "\n",
    "- **Classification**: Predicting whether a patient has a disease (binary outcome)\n",
    "- **Count data**: Number of events in a time period (discrete, non-negative)\n",
    "- **Survival analysis**: Time until an event occurs (censored data)\n",
    "\n",
    "For these cases, we need `pm.gp.Latent`, which keeps the GP function as a latent variable in the model and allows arbitrary likelihood functions.\n",
    "\n",
    "### The Latent GP Approach\n",
    "\n",
    "Instead of marginalizing out the GP, we explicitly sample it:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f &\\sim \\mathcal{GP}(m, k) \\\\\n",
    "y_i &\\sim \\text{Likelihood}(g(f(x_i)), \\theta)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $g$ is a link function (like logistic for binary outcomes) and Likelihood can be any distribution (Bernoulli, Poisson, Student-T, etc.).\n",
    "\n",
    "### Trade-offs: Latent vs Marginal\n",
    "\n",
    "**pm.gp.Marginal**:\n",
    "- âœ“ Fast: analytically integrates out the GP\n",
    "- âœ“ Efficient: fewer parameters to sample\n",
    "- âœ— Limited: only works with Gaussian likelihoods\n",
    "\n",
    "**pm.gp.Latent**:\n",
    "- âœ“ Flexible: works with any likelihood\n",
    "- âœ“ Includes GP samples in posterior\n",
    "- âœ— Slower: must sample high-dimensional latent function\n",
    "- âœ— Can have sampling challenges\n",
    "\n",
    "Let's work through a classification example to see `pm.gp.Latent` in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Poisson Regression for Count Data\n",
    "\n",
    "Many real-world applications involve count dataâ€”discrete, non-negative outcomes like the number of events occurring in a time period. The **Poisson distribution** is the natural choice for modeling counts, and when we expect the rate to vary smoothly over time, a GP with Poisson likelihood becomes a powerful tool.\n",
    "\n",
    "We'll work with the famous British coal mining disasters dataset, which records 191 mining disasters from 1851 to 1962. Our goal: model how the disaster rate changes over this period using a GP to capture smooth temporal trends without imposing a rigid parametric form.\n",
    "\n",
    "The Poisson likelihood requires a positive rate parameter $\\lambda$, so we'll use a **log link function**: the GP models $\\log(\\lambda)$, ensuring $\\lambda > 0$. Let's load the data and bin it into annual counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load coal mining disasters data\n",
    "coal_data_path = pm.get_data('coal.csv')\n",
    "coal_df_raw = pl.read_csv(coal_data_path, has_header=False, new_columns=['year'])\n",
    "\n",
    "print(f\"Total disasters recorded: {len(coal_df_raw)}\")\n",
    "print(f\"Time period: {coal_df_raw['year'].min():.1f} to {coal_df_raw['year'].max():.1f}\")\n",
    "\n",
    "# Bin into annual counts\n",
    "# Create bins for each year from 1851 to 1963 (to capture all disasters)\n",
    "year_min = int(np.floor(coal_df_raw['year'].min()))\n",
    "year_max = int(np.ceil(coal_df_raw['year'].max()))\n",
    "\n",
    "# Count disasters per year\n",
    "year_counts = []\n",
    "years = []\n",
    "for year in range(year_min, year_max + 1):\n",
    "    count = coal_df_raw.filter(\n",
    "        (pl.col('year') >= year) & (pl.col('year') < year + 1)\n",
    "    ).height\n",
    "    year_counts.append(count)\n",
    "    years.append(year)\n",
    "\n",
    "# Create DataFrame with annual counts\n",
    "df_coal = pl.DataFrame({\n",
    "    'year': years,\n",
    "    'count': year_counts,\n",
    "    'year_scaled': [(y - year_min) for y in years]  # Years since 1851\n",
    "})\n",
    "\n",
    "print(f\"\\nAnnual counts range: {min(year_counts)} to {max(year_counts)}\")\n",
    "print(f\"\\nFirst few years:\")\n",
    "print(df_coal.head(10))\n",
    "\n",
    "# Visualize the annual disaster counts\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_coal['year_scaled'],\n",
    "    y=df_coal['count'],\n",
    "    mode='markers',\n",
    "    name='Annual disaster count',\n",
    "    marker=dict(size=6, color='black', opacity=0.7)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='British Coal Mining Disasters (1851-1962)',\n",
    "    xaxis_title='Years since 1851',\n",
    "    yaxis_title='Number of disasters per year',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Coal Mining Data\n",
    "\n",
    "Looking at the plot, we see clear temporal patterns in the annual disaster counts:\n",
    "\n",
    "**Early period (1851-1890s)**: Disaster rates are notably higher, with several years experiencing 5-6 disasters. This reflects the dangerous conditions of 19th-century miningâ€”inadequate safety regulations, primitive equipment, and poor ventilation.\n",
    "\n",
    "**Later period (1900s onward)**: The disaster rate appears to decline, with most years having 0-2 disasters. This likely reflects improving safety standards, better mining technology, and stricter regulations following major disasters.\n",
    "\n",
    "**Key observation**: The rate appears to change smoothly over time rather than in discrete jumps. This makes the coal mining dataset perfect for GP modelingâ€”we want to capture this smooth temporal trend without assuming a specific parametric form (like exponential decay).\n",
    "\n",
    "Notice these are **count data**â€”non-negative integers, not continuous values. A Gaussian likelihood would be inappropriate since it could predict negative disaster counts. The Poisson distribution is designed exactly for this type of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Poisson GP Model\n",
    "\n",
    "Now we'll use `pm.gp.Latent` with a Poisson likelihood to model the disaster rate. The key modeling choices:\n",
    "\n",
    "**Link function**: The Poisson distribution parameterizes the rate $\\lambda > 0$, but our GP can range over all real numbers. We use the **log link**: $\\log(\\lambda) = f(t)$, which means $\\lambda = \\exp(f(t))$. This ensures the rate is always positive regardless of what the GP predicts.\n",
    "\n",
    "**Kernel choice**: We'll use MatÃ©rn 5/2, which allows smooth but not infinitely smooth variationsâ€”appropriate for a disaster rate that might change gradually as mining practices evolve. A lengthscale prior that's weakly informative over the ~110-year timespan makes sense.\n",
    "\n",
    "**Prior considerations**: We need to be thoughtful about the GP prior on the log-scale. Since disasters are relatively rare events (at most 6 per year), $\\log(\\lambda)$ should be modestâ€”probably between -2 (â‰ˆ0.14 disasters/year) and 2 (â‰ˆ7.4 disasters/year)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "X_coal = df_coal['year_scaled'].to_numpy().reshape(-1, 1)\n",
    "y_coal = df_coal['count'].to_numpy()\n",
    "\n",
    "with pm.Model() as coal_model:\n",
    "    # Hyperparameter priors\n",
    "    # Lengthscale: expect variation on the scale of 10-30 years\n",
    "    ell = pm.Gamma('ell', alpha=3, beta=0.2)\n",
    "    \n",
    "    # Amplitude: controls variation in log-rate\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "    \n",
    "    # Covariance function - MatÃ©rn 5/2 for smooth but flexible trends\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    # Latent GP\n",
    "    gp = pm.gp.Latent(cov_func=cov_func)\n",
    "    \n",
    "    # GP prior - this is the log-rate\n",
    "    f = gp.prior('f', X=X_coal)\n",
    "    \n",
    "    # Transform to rate via exponential (inverse of log link)\n",
    "    lam = pm.Deterministic('lambda', pm.math.exp(f))\n",
    "    \n",
    "    # Poisson likelihood\n",
    "    y_obs = pm.Poisson('y', mu=lam, observed=y_coal)\n",
    "    \n",
    "    # Sample from posterior\n",
    "    coal_trace = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        nuts_sampler='nutpie',\n",
    "        chains=2,\n",
    "        target_accept=0.9,\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Poisson GP Structure\n",
    "\n",
    "The critical components of this model differ from our earlier examples:\n",
    "\n",
    "**`gp.prior('f', X=X_coal)`**: This explicitly samples the latent GP as a variable in our model. The vector `f` represents $\\log(\\lambda)$ at each yearâ€”the log-rate of disasters.\n",
    "\n",
    "**`pm.Deterministic('lambda', pm.math.exp(f))`**: This is the **inverse link function**, transforming from log-rate to actual rate. By tracking it as a Deterministic variable, we can examine the disaster rate $\\lambda$ directly in our posterior, not just the log-rate. This makes interpretation much easier.\n",
    "\n",
    "**`pm.Poisson('y', mu=lam, observed=y_coal)`**: The Poisson likelihood connects our latent rate to the observed counts. This is fundamentally different from Gaussian or Bernoulliâ€”the Poisson naturally handles non-negative integer counts and has variance equal to its mean.\n",
    "\n",
    "**Why we can't use `pm.gp.Marginal`**: There's no way to analytically marginalize a Poisson likelihood. We must explicitly sample the latent function `f`, which is what `pm.gp.Latent` does. This makes inference slower but enables the flexibility to use non-Gaussian likelihoods.\n",
    "\n",
    "Let's check how well sampling worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check convergence\n",
    "summary_coal = az.summary(coal_trace, var_names=['ell', 'eta'])\n",
    "print(summary_coal)\n",
    "\n",
    "# Check divergences\n",
    "divergences_coal = coal_trace.sample_stats['diverging'].sum().item()\n",
    "print(f\"\\nNumber of divergences: {divergences_coal}\")\n",
    "\n",
    "if divergences_coal == 0:\n",
    "    print(\"âœ“ No divergences - sampling worked well!\")\n",
    "else:\n",
    "    print(f\"âš  Warning: {divergences_coal} divergences detected. Consider reparameterization.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Posterior Disaster Rate\n",
    "\n",
    "Now let's see how well our GP captured the temporal trend in disaster rates. We'll plot the posterior mean rate $\\lambda(t)$ along with credible intervals, overlaying the observed annual counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax=ax,\n",
    "    x=df_coal['year_scaled'].to_numpy(), \n",
    "    samples=coal_trace.posterior['lambda'].values.reshape(-1, len(df_coal)),\n",
    "    plot_samples=False, \n",
    "    palette='Reds'\n",
    ")\n",
    "\n",
    "ax.plot(\n",
    "    df_coal['year_scaled'].to_numpy(),\n",
    "    df_coal['count'].to_numpy(),\n",
    "    'ko',\n",
    "    markersize=4,\n",
    "    alpha=0.6,\n",
    "    label='Observed annual counts'\n",
    ")\n",
    "\n",
    "ax.set_xlabel('Years since 1851')\n",
    "ax.set_ylabel('Disaster rate (disasters/year)')\n",
    "ax.set_title('GP Poisson Regression: Coal Mining Disaster Rate Over Time')\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Poisson GP Results\n",
    "\n",
    "This is a beautiful demonstration of GP flexibility with non-Gaussian likelihoods. The posterior mean rate (red line) captures the historical trend in coal mining disasters:\n",
    "\n",
    "**Early period (1851-1890s)**: The estimated disaster rate is high, around 3-4 disasters per year. The model has learned that Victorian-era mining was substantially more dangerous, reflected in the elevated rate during this period.\n",
    "\n",
    "**Transition period (~1890-1900)**: The rate begins to decline as safety practices gradually improved. The GP smoothly captures this transition without us imposing any particular functional formâ€”no exponential decay, no linear trend, just letting the data speak.\n",
    "\n",
    "**Later period (1900-1962)**: The disaster rate settles to a lower level, around 1-2 disasters per year. While still tragic, this represents a substantial improvement from the earlier period.\n",
    "\n",
    "**Uncertainty quantification**: The credible interval (shaded region) reflects our uncertainty about the true underlying rate. Notice it's wider in periods with high variability in the observed countsâ€”the Poisson variance equals its mean, so high-rate periods naturally have more scatter.\n",
    "\n",
    "**The power of the log link**: By modeling $\\log(\\lambda)$ with a GP and exponentiating, we guaranteed $\\lambda > 0$ throughout. This is essential for count dataâ€”we never predict negative disaster rates, even though our GP can range over all real numbers.\n",
    "\n",
    "Now let's move on to one of the most powerful features of GPs: kernel composition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE: Fitting a GP to Atmospheric COâ‚‚ Data\n",
    "\n",
    "Now it's your turn to fit a GP model to real-world data. We'll use the famous Mauna Loa COâ‚‚ datasetâ€”the \"Keeling Curve\"â€”which has become one of the most important scientific time series in climate science.\n",
    "\n",
    "### Understanding the Mauna Loa COâ‚‚ Dataset\n",
    "\n",
    "Since 1958, researchers at the Mauna Loa Observatory in Hawaii have continuously measured atmospheric COâ‚‚ concentrations. This remote location in the middle of the Pacific Ocean provides measurements relatively free from local pollution sources, making it an ideal monitoring station for global atmospheric composition.\n",
    "\n",
    "The Keeling Curve exhibits two striking patterns that you'll notice when you plot the data:\n",
    "\n",
    "1. **Long-term increasing trend** (decades-scale) due to cumulative anthropogenic emissions from fossil fuels, deforestation, and industrial activity\n",
    "2. **Annual seasonal cycles** (yearly oscillations) due to Northern Hemisphere vegetation dynamicsâ€”plants absorb COâ‚‚ during spring/summer growth and release it during fall/winter decay\n",
    "\n",
    "For this exercise, we'll focus on a 10-year window (1990-2000) to keep the computation manageable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mauna Loa CO2 data from PyMC datasets\n",
    "co2_file = pm.get_data(\"monthly_in_situ_co2_mlo.csv\")\n",
    "\n",
    "df_raw = pl.read_csv(\n",
    "    co2_file,\n",
    "    skip_rows=57,  # Skip metadata header\n",
    "    has_header=False,\n",
    "    null_values=[\"-99.99\"],  # Handle missing value indicator\n",
    "    schema_overrides={\n",
    "        \"column_1\": pl.Int64,    # year\n",
    "        \"column_2\": pl.Int64,    # month\n",
    "        \"column_3\": pl.Float64,  # decimal_date\n",
    "        \"column_4\": pl.Float64,  # monthly_avg\n",
    "        \"column_5\": pl.Float64,  # CO2\n",
    "        \"column_6\": pl.Float64,  # seasonally_adjusted\n",
    "        \"column_7\": pl.Float64,  # fit\n",
    "        \"column_8\": pl.Float64,  # seasonally_adjusted_fit\n",
    "        \"column_9\": pl.Float64,  # CO2_filled\n",
    "        \"column_10\": pl.Float64, # seasonally_adjusted_filled\n",
    "    }\n",
    ")\n",
    "\n",
    "df_raw.columns = [\n",
    "    \"year\", \"month\", \"decimal_date\", \"monthly_avg\", \"CO2\", \"seasonally_adjusted\",\n",
    "    \"fit\", \"seasonally_adjusted_fit\", \"CO2_filled\", \"seasonally_adjusted_filled\"\n",
    "]\n",
    "\n",
    "df_co2 = (\n",
    "    df_raw\n",
    "    .filter((pl.col(\"year\") >= 1990) & (pl.col(\"year\") < 2000))\n",
    "    .with_columns(\n",
    "        (pl.col(\"year\") + (pl.col(\"month\") - 1) / 12 - 1990).alias(\"time\")\n",
    "    )\n",
    "    .select([\"time\", \"CO2\"])\n",
    "    .drop_nulls()\n",
    ")\n",
    "\n",
    "X_co2 = df_co2.select(\"time\").to_numpy().reshape(-1, 1)\n",
    "y_co2 = df_co2.select(\"CO2\").to_numpy().flatten()\n",
    "\n",
    "print(f\"Data shape: {X_co2.shape[0]} monthly observations from 1990-2000\")\n",
    "print(f\"CO2 range: {y_co2.min():.2f} to {y_co2.max():.2f} ppm\")\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=df_co2[\"time\"].to_numpy(),\n",
    "    y=df_co2[\"CO2\"].to_numpy(),\n",
    "    mode=\"markers\",\n",
    "    marker=dict(size=4, color=\"steelblue\"),\n",
    "    name=\"Observed COâ‚‚\"\n",
    ")).update_layout(\n",
    "    title=\"Atmospheric COâ‚‚ at Mauna Loa (1990-2000)\",\n",
    "    xaxis_title=\"Years since 1990\",\n",
    "    yaxis_title=\"COâ‚‚ Concentration (ppm)\",\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task\n",
    "\n",
    "Build a simple GP model using **either `pm.gp.Marginal` or `pm.gp.Latent`** to fit the COâ‚‚ data. You'll need to:\n",
    "\n",
    "1. Load and prepare the data (code provided below)\n",
    "2. Choose an appropriate kernel (think about what patterns you're trying to capture)\n",
    "3. Define priors for the kernel hyperparameters\n",
    "4. Fit the model and generate predictions\n",
    "5. Visualize the fit\n",
    "\n",
    "Don't worry if your fit doesn't capture all the patterns perfectlyâ€”that's intentional! In the next section, we'll see how to improve this model dramatically using additive kernels.\n",
    "\n",
    "**Prompt suggestion**: \"Help me implement a GP regression model in PyMC to fit atmospheric COâ‚‚ data. Use either Marginal or Latent formulation with an appropriate kernel, priors for lengthscale and amplitude, and generate posterior predictions with uncertainty bands.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR LLM-ASSISTED CODE HERE\n",
    "# Build your GP model below. Consider:\n",
    "# - Which kernel captures smooth variation?\n",
    "# - What are reasonable lengthscale values for data spanning 10 years?\n",
    "# - Should you use Marginal (Gaussian likelihood) or Latent?\n",
    "# - How will you handle the observation noise?\n",
    "\n",
    "def fit_simple_co2_gp(X, y):\n",
    "    \"\"\"\n",
    "    Fit a GP model to CO2 data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : array, shape (n, 1)\n",
    "        Time points (years since 1990)\n",
    "    y : array, shape (n,)\n",
    "        CO2 concentrations (ppm)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    model : pm.Model\n",
    "        The fitted PyMC model\n",
    "    trace : az.InferenceData\n",
    "        Posterior samples\n",
    "    \"\"\"\n",
    "    # YOUR CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementation\n",
    "# model_co2, trace_co2 = fit_simple_co2_gp(X_co2, y_co2)\n",
    "# az.summary(trace_co2, var_names=['ell', 'eta', 'sigma'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflecting on Your Simple GP Fit\n",
    "\n",
    "Once you've fit your model, take a moment to examine the results carefully. If you used a single smooth kernel like ExpQuad or MatÃ©rn, you likely noticed something important: **the model had to make a compromise**.\n",
    "\n",
    "**If you chose a short lengthscale**: The GP might have captured some of the seasonal oscillations, but at the cost of creating unrealistic wiggles in the long-term trend. The model is trying to fit both patterns with a single smoothness parameter, and it's struggling.\n",
    "\n",
    "**If you chose a long lengthscale**: The GP might have captured the decade-scale upward trend beautifully, giving you a smooth increasing curve. But look closely at the residualsâ€”you'll see a clear annual pattern that the model completely missed. The seasonal cycle is still there in the data; your model just couldn't see it.\n",
    "\n",
    "**This is the fundamental limitation**: A single kernel with one lengthscale can only capture variation at one characteristic timescale. The COâ‚‚ data has variation at **two very different timescales**â€”years (trend) and months (seasonality)â€”and they arise from completely independent physical processes.\n",
    "\n",
    "Here's the key insight: these patterns don't interact; they simply add together. The long-term trend from fossil fuel emissions doesn't change the seasonal amplitude, and the seasonal cycle from vegetation doesn't affect the long-term trend. They're independent sources of variation.\n",
    "\n",
    "This is exactly the scenario where **additive kernels** shine. In the next section, we'll see how combining kernels through addition allows us to model both patterns simultaneously, each with its own characteristic lengthscale. The improvement will be dramatic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Section 2.4: Additive and Multiplicative Kernels\n",
    "\n",
    "One of the most powerful features of GPs is that we can **compose kernels** through addition and multiplication. This allows us to build sophisticated models that capture multiple patterns simultaneouslyâ€”exactly what we need for the COâ‚‚ data you just worked with.\n",
    "\n",
    "### The Algebra of Kernels: AND vs OR\n",
    "\n",
    "If $k_1$ and $k_2$ are valid covariance functions, then both $k_1 + k_2$ and $k_1 \\times k_2$ are also valid covariance functions. But they mean very different things:\n",
    "\n",
    "**Addition (OR operation): $k_1 + k_2$**\n",
    "\n",
    "Think of addition as saying \"the function can vary in ways explained by kernel 1 **OR** kernel 2 (or both).\" The total covariance is the sum of covariances from each kernel:\n",
    "\n",
    "$$\n",
    "k_{\\text{sum}}(x, x') = k_1(x, x') + k_2(x, x')\n",
    "$$\n",
    "\n",
    "Covariance is high if **either kernel** suggests that $f(x)$ and $f(x')$ should be similar. This is perfect for modeling **independent sources of variation** that simply add together without interacting.\n",
    "\n",
    "**Multiplication (AND operation): $k_1 \\times k_2$**\n",
    "\n",
    "Think of multiplication as saying \"the function must satisfy constraints from kernel 1 **AND** kernel 2 simultaneously.\" The total covariance is the product:\n",
    "\n",
    "$$\n",
    "k_{\\text{product}}(x, x') = k_1(x, x') \\times k_2(x, x')\n",
    "$$\n",
    "\n",
    "Covariance is high only when **both kernels** agree that $f(x)$ and $f(x')$ should be similar. This is perfect for modeling situations where **one pattern modulates another**â€”where one kernel acts as an envelope or mask for the other.\n",
    "\n",
    "Let's make this concrete with examples, starting with the additive case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intuitive Examples of Addition vs Multiplication\n",
    "\n",
    "To build intuition, let's think through some real-world scenarios:\n",
    "\n",
    "**Addition (Independent Effects)**\n",
    "\n",
    "*Example 1: Daily Temperature*\n",
    "Temperature has a yearly seasonal cycle (winter is cold, summer is hot) **OR** local weather variations (day-to-day fluctuations). These are independent: the yearly cycle doesn't change the day-to-day weather variance, and local weather doesn't affect the seasonal pattern. Model: $k_{\\text{seasonal}} + k_{\\text{local}}$\n",
    "\n",
    "*Example 2: Website Traffic*\n",
    "A website might have a weekly pattern (more traffic on weekdays) **OR** a long-term growth trend (increasing users over months/years). These effects stack independently. Model: $k_{\\text{periodic}} + k_{\\text{trend}}$\n",
    "\n",
    "*Example 3: Mauna Loa COâ‚‚*\n",
    "We saw this in our earlier example: a decade-long upward trend from fossil fuel emissions **OR** annual oscillations from vegetation. Neither affects the otherâ€”they're independent physical processes. Model: $k_{\\text{trend}} + k_{\\text{seasonal}}$\n",
    "\n",
    "**Multiplication (Modulation and Interaction)**\n",
    "\n",
    "*Example 1: Seasonal Sales Patterns During Business Growth*\n",
    "A retail business has weekly sales patterns (higher on weekends) **AND** those patterns only emerged as the business maturedâ€”they weren't present in year 1. The growth process modulates the strength of the weekly pattern. Model: $k_{\\text{linear}} \\times k_{\\text{periodic}}$\n",
    "\n",
    "*Example 2: Equipment Vibrations*\n",
    "A machine might vibrate periodically (due to rotating parts) **AND** those vibrations only occur when the machine is running at certain speeds (captured by a smooth function of operating conditions). The operating regime gates whether oscillations exist. Model: $k_{\\text{smooth}} \\times k_{\\text{periodic}}$ (locally periodic)\n",
    "\n",
    "*Example 3: Spatial-Temporal Phenomena*\n",
    "Consider pollution levels that vary both in space and time, but the spatial patterns change slowly over time. The temporal pattern modulates which spatial structures are relevant at any moment. Model: $k_{\\text{spatial}}(x, x') \\times k_{\\text{temporal}}(t, t')$\n",
    "\n",
    "**Mental Model Summary**\n",
    "\n",
    "- **Addition**: \"Stack the effects\" â€” independent contributions that simply superimpose\n",
    "- **Multiplication**: \"Mask or modulate\" â€” one pattern determines where/when the other pattern appears\n",
    "\n",
    "When building models, ask yourself: \"Are these patterns independent sources of variation (addition), or does one pattern control the expression of the other (multiplication)?\" This will guide your kernel composition strategy.\n",
    "\n",
    "Now let's see additive kernels in action by returning to the COâ‚‚ data with a proper decomposition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additive Kernels for the COâ‚‚ Data\n",
    "\n",
    "Let's return to the Mauna Loa COâ‚‚ data from the previous exercise and see how additive kernels solve the problem you likely encountered. Remember, a single kernel had to compromise between capturing the long-term trend and the seasonal oscillations.\n",
    "\n",
    "The solution is remarkably elegant: we'll use **two kernels added together**, each modeling a different physical process:\n",
    "\n",
    "1. **ExpQuad kernel with long lengthscale**: Captures the smooth decade-scale trend from cumulative emissions\n",
    "2. **Periodic kernel with 1-year period**: Captures the annual seasonal cycle from vegetation dynamics\n",
    "\n",
    "When we add these kernels, $k_{\\text{total}} = k_{\\text{trend}} + k_{\\text{seasonal}}$, the resulting GP can simultaneously model both patterns, each with appropriate characteristic scales. Let's build this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the same data you loaded in the previous exercise (`X_co2` and `y_co2`). Now let's build an additive model that combines a smooth trend kernel with a periodic seasonal kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Independent Patterns\n",
    "\n",
    "Looking at the data, two distinct patterns are immediately apparent:\n",
    "\n",
    "**The long-term trend**: COâ‚‚ concentrations rise steadily over the decade, from ~355 ppm in 1990 to ~370 ppm in 2000. This reflects cumulative emissions from fossil fuel combustion and land use changes.\n",
    "\n",
    "**The seasonal cycle**: Regular oscillations repeat annually with an amplitude of about 5-7 ppm. Each spring, COâ‚‚ concentrations drop as Northern Hemisphere plants leaf out and photosynthesize. Each fall, concentrations rise as plants shed leaves and decomposition releases COâ‚‚.\n",
    "\n",
    "The key insight: these patterns arise from **independent physical processes**. The trend results from anthropogenic emissions. The seasonal cycle results from the terrestrial biosphere's annual growth/decay cycle. Neither process modulates the otherâ€”they simply add together.\n",
    "\n",
    "This is exactly when we need an additive kernel: $k_{\\text{total}} = k_{\\text{trend}} + k_{\\text{seasonal}}$. Let's build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Additive GP: Trend + Seasonal Components\n",
    "\n",
    "We'll use an **additive kernel** to model the trend and seasonality as independent components:\n",
    "\n",
    "- **MatÃ©rn 5/2 kernel** with a long lengthscale for the decadal trend\n",
    "- **Periodic kernel** with period=1 year for the seasonal oscillations\n",
    "\n",
    "The mathematical insight: when we add kernels, the resulting GP's covariance function is\n",
    "\n",
    "$$\n",
    "k_{\\text{total}}(x, x') = k_{\\text{trend}}(x, x') + k_{\\text{seasonal}}(x, x')\n",
    "$$\n",
    "\n",
    "This means the **total variance** is the sum of variances from each component. Each kernel can explain different aspects of the data independently. The GP will automatically learn how much variation to attribute to the trend versus the seasonal cycle.\n",
    "\n",
    "Let's implement this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as additive_co2_model:\n",
    "    \n",
    "    # Trend component: smooth, long-term variation\n",
    "    ell_trend = pm.Gamma('ell_trend', alpha=2, beta=0.5)  # Prior favors larger lengthscales\n",
    "    eta_trend = pm.HalfNormal('eta_trend', sigma=10)  # Allow for substantial trend magnitude\n",
    "    cov_trend = eta_trend**2 * pm.gp.cov.Matern52(1, ell_trend)\n",
    "    \n",
    "    # Seasonal component: periodic with yearly period\n",
    "    ell_seasonal = pm.Gamma('ell_seasonal', alpha=2, beta=2)\n",
    "    eta_seasonal = pm.HalfNormal('eta_seasonal', sigma=5)  # Seasonal amplitude ~3-7 ppm\n",
    "    period = 1.0  # One year\n",
    "    cov_seasonal = eta_seasonal**2 * pm.gp.cov.Periodic(1, period=period, ls=ell_seasonal)\n",
    "    \n",
    "    # ADDITIVE KERNEL: sum of trend and seasonal\n",
    "    cov_total = cov_trend + cov_seasonal\n",
    "    \n",
    "    # GP with additive kernel\n",
    "    gp = pm.gp.Marginal(cov_func=cov_total)\n",
    "    \n",
    "    # Likelihood\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)  # Measurement noise\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_co2, y=y_co2, sigma=sigma)\n",
    "    \n",
    "    # Sample\n",
    "    additive_co2_trace = pm.sample(\n",
    "        500,\n",
    "        tune=1000,\n",
    "        nuts_sampler='nutpie',\n",
    "        chains=2,\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Additive Model Structure\n",
    "\n",
    "The critical line is `cov_total = cov_trend + cov_seasonal`. This creates a new covariance function where:\n",
    "\n",
    "- The **trend component** can vary slowly over years, capturing the decadal increase in COâ‚‚\n",
    "- The **seasonal component** can oscillate with a one-year period, capturing the annual vegetation cycle\n",
    "- The **total variation** is the sum of bothâ€”they contribute independently\n",
    "\n",
    "We gave each component its own hyperparameters:\n",
    "- `ell_trend`, `eta_trend`: Control how smooth and how large the long-term trend is\n",
    "- `ell_seasonal`, `eta_seasonal`: Control how smooth each annual cycle is and how large the seasonal amplitude is\n",
    "\n",
    "The model will learn these parameters from the data, effectively **decomposing** the COâ‚‚ signal into its constituent physical processes.\n",
    "\n",
    "Let's visualize the results to see how well the additive kernel captures both patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions (including slight extrapolation)\n",
    "X_pred_co2 = np.linspace(-0.5, 11, 400)[:, None]\n",
    "\n",
    "with additive_co2_model:\n",
    "    f_pred_co2 = gp.conditional('f_pred', X_pred_co2)\n",
    "    ppc_co2 = pm.sample_posterior_predictive(\n",
    "        additive_co2_trace,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax=ax,\n",
    "    x=X_pred_co2.flatten(),\n",
    "    samples=ppc_co2.posterior_predictive['f_pred'].values.reshape(-1, len(X_pred_co2)),\n",
    "    plot_samples=False, \n",
    "    palette='Reds',   \n",
    ")\n",
    "\n",
    "ax.scatter(\n",
    "    df_co2['time'],\n",
    "    df_co2['CO2'],\n",
    "    s=20,\n",
    "    c='black',\n",
    "    alpha=0.6,\n",
    "    label='Observed COâ‚‚',\n",
    "    zorder=10\n",
    ")\n",
    "\n",
    "ax.set_title('Additive GP: Trend + Seasonal Components', fontsize=12)\n",
    "ax.set_xlabel('Years since 1990', fontsize=10)\n",
    "ax.set_ylabel('COâ‚‚ concentration (ppm)', fontsize=10)\n",
    "ax.legend(loc='upper left')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Additive Model Results\n",
    "\n",
    "Beautiful! The additive GP has successfully decomposed the Mauna Loa COâ‚‚ data into its constituent patterns:\n",
    "\n",
    "**The trend component** captures the steady decade-long increase from fossil fuel emissions. The model learned that this variation happens on a time scale of years, with a large lengthscale parameter.\n",
    "\n",
    "**The seasonal component** captures the annual oscillations from vegetation dynamics. The model learned that this pattern repeats exactly every year (period=1.0) with an amplitude of several ppm.\n",
    "\n",
    "**The combined fit** (dark red line) shows how these independent patterns add together to explain the observations. The credible intervals (shown as progressively lighter shades of red for 68%, 95%, and wider intervals) reflect our uncertaintyâ€”notice how the uncertainty bands grow slightly when extrapolating beyond year 10.\n",
    "\n",
    "This is the power of additive models: when your data contains multiple **independent** sources of variation, each kernel explains a different aspect. The GP automatically learns how to decompose the signal based on the different correlation structures (long-term smooth trend vs. yearly periodic cycle).\n",
    "\n",
    "**Key intuition**: Addition works when patterns don't interactâ€”when one doesn't modulate the other. The COâ‚‚ trend doesn't change the seasonal amplitude, and the seasonal cycle doesn't affect the long-term trend. They're independent physical processes that simply superimpose.\n",
    "\n",
    "Now let's see what happens when patterns **do** interactâ€”that's where multiplicative kernels come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Locally Periodic Patterns: When Seasonality Isn't Global\n",
    "\n",
    "The additive model we just built assumed that both the trend and seasonal patterns exist throughout the entire time series. But what if your periodic pattern only appears in certain regions, or gradually fades in and out?\n",
    "\n",
    "Consider these scenarios:\n",
    "- **Ecological data**: Animal behavior might be seasonal in temperate regions but constant in tropical regions\n",
    "- **Economic data**: Holiday sales patterns might emerge over time as a business matures\n",
    "- **Sensor data**: Periodic noise might only appear when equipment is under certain operating conditions\n",
    "\n",
    "For these cases, we need a **locally periodic kernel**â€”a periodic pattern that's modulated by a smooth envelope. The key insight: this requires **multiplication**, not addition.\n",
    "\n",
    "Recall the algebra of kernel composition:\n",
    "- **Addition** (kâ‚ + kâ‚‚): Independent sources of variationâ€”\"this OR that\"\n",
    "- **Multiplication** (kâ‚ Ã— kâ‚‚): One pattern modulates anotherâ€”\"this AND that\"\n",
    "\n",
    "A **locally periodic kernel** multiplies a Periodic kernel with a smooth kernel (typically ExpQuad or MatÃ©rn):\n",
    "\n",
    "$$\n",
    "k_{\\text{local-periodic}}(x, x') = k_{\\text{smooth}}(x, x') \\times k_{\\text{periodic}}(x, x')\n",
    "$$\n",
    "\n",
    "This creates periodic oscillations that are strong where the smooth kernel has high covariance, and weak (or absent) where it doesn't. Let's see this in action with synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with localized periodic pattern\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "X_local_per = np.linspace(0, 15, 250)[:, None]\n",
    "\n",
    "# Create a Gaussian envelope centered at x=7.5\n",
    "center = 7.5\n",
    "envelope_width = 3.0\n",
    "envelope = np.exp(-0.5 * ((X_local_per.flatten() - center) / envelope_width)**2)\n",
    "\n",
    "# Periodic component with annual period\n",
    "periodic_component = np.sin(2 * np.pi * X_local_per.flatten() / 2.0)\n",
    "\n",
    "f_true_local = 2.0 * envelope * periodic_component\n",
    "\n",
    "y_local_per = f_true_local + 0.3 * RNG.standard_normal(len(X_local_per))\n",
    "\n",
    "df_local_per = pl.DataFrame({\n",
    "    'x': X_local_per.flatten(),\n",
    "    'y': y_local_per,\n",
    "    'f_true': f_true_local,\n",
    "    'envelope': 2.0 * envelope,\n",
    "    'periodic': 2.0 * periodic_component\n",
    "})\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=df_local_per['x'],\n",
    "    y=df_local_per['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function (localized periodic)',\n",
    "    line=dict(color='dodgerblue', width=2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=df_local_per['x'],\n",
    "    y=df_local_per['envelope'],\n",
    "    mode='lines',\n",
    "    name='Envelope (Gaussian)',\n",
    "    line=dict(color='green', width=2, dash='dash')\n",
    ")).add_trace(go.Scatter(\n",
    "    x=df_local_per['x'],\n",
    "    y=df_local_per['y'],\n",
    "    mode='markers',\n",
    "    name='Observed data',\n",
    "    marker=dict(size=4, color='black', opacity=0.6)\n",
    ")).update_layout(\n",
    "    title='Data with Localized Periodic Pattern',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll fit three models to compare approaches:\n",
    "\n",
    "1. Pure Periodic (wrong - assumes global periodicity)\n",
    "2. ExpQuad + Periodic (wrong - independent components)\n",
    "3. ExpQuad Ã— Periodic (correct - modulated periodicity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Pure Periodic\n",
    "with pm.Model() as model_pure_periodic:\n",
    "    ell_per = pm.Gamma('ell_per', alpha=2, beta=2)\n",
    "    eta_per = pm.HalfNormal('eta_per', sigma=3)\n",
    "    cov_per = eta_per**2 * pm.gp.cov.Periodic(1, period=2.0, ls=ell_per)\n",
    "\n",
    "    gp = pm.gp.Marginal(cov_func=cov_per)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_local_per, y=y_local_per, sigma=sigma)\n",
    "\n",
    "    trace_pure_per = pm.sample(500, tune=500, nuts_sampler='nutpie', chains=2, random_seed=RNG)\n",
    "\n",
    "X_pred_local = np.linspace(-1, 16, 400)[:, None]\n",
    "\n",
    "with model_pure_periodic:\n",
    "    \n",
    "    f_pred_pure = gp.conditional('f_pred', X_pred_local)\n",
    "    ppc_pure = pm.sample_posterior_predictive(trace_pure_per, var_names=['f_pred'], random_seed=RNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Additive (ExpQuad + Periodic)\n",
    "with pm.Model() as model_additive:\n",
    "    # Smooth component\n",
    "    ell_smooth = pm.Gamma('ell_smooth', alpha=2, beta=0.5)\n",
    "    eta_smooth = pm.HalfNormal('eta_smooth', sigma=3)\n",
    "    cov_smooth = eta_smooth**2 * pm.gp.cov.ExpQuad(1, ell_smooth)\n",
    "\n",
    "    # Periodic component\n",
    "    ell_per = pm.Gamma('ell_per', alpha=2, beta=2)\n",
    "    eta_per = pm.HalfNormal('eta_per', sigma=3)\n",
    "    cov_per = eta_per**2 * pm.gp.cov.Periodic(1, period=2.0, ls=ell_per)\n",
    "\n",
    "    # Addition\n",
    "    cov_add = cov_smooth + cov_per\n",
    "\n",
    "    gp = pm.gp.Marginal(cov_func=cov_add)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_local_per, y=y_local_per, sigma=sigma)\n",
    "\n",
    "    trace_add = pm.sample(500, tune=500, nuts_sampler='nutpie', chains=2, random_seed=RNG)\n",
    "\n",
    "with model_additive:\n",
    "    \n",
    "    f_pred_add = gp.conditional('f_pred', X_pred_local)\n",
    "    ppc_add = pm.sample_posterior_predictive(trace_add, var_names=['f_pred'], random_seed=RNG)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 3: Multiplicative (ExpQuad Ã— Periodic) - LOCALLY PERIODIC\n",
    "with pm.Model() as model_locally_periodic:\n",
    "    # Smooth component\n",
    "    ell_smooth = pm.Gamma('ell_smooth', alpha=2, beta=0.5)\n",
    "    eta_smooth = pm.HalfNormal('eta_smooth', sigma=3)\n",
    "    cov_smooth = eta_smooth**2 * pm.gp.cov.ExpQuad(1, ell_smooth)\n",
    "\n",
    "    # Periodic component\n",
    "    ell_per = pm.Gamma('ell_per', alpha=2, beta=2)\n",
    "    eta_per = pm.HalfNormal('eta_per', sigma=3)\n",
    "    cov_per = eta_per**2 * pm.gp.cov.Periodic(1, period=2.0, ls=ell_per)\n",
    "\n",
    "    # Multiplication - this creates local periodicity\n",
    "    cov_mult = cov_smooth * cov_per\n",
    "\n",
    "    gp = pm.gp.Marginal(cov_func=cov_mult)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = gp.marginal_likelihood('y', X=X_local_per, y=y_local_per, sigma=sigma)\n",
    "\n",
    "    trace_mult = pm.sample(500, tune=500, nuts_sampler='nutpie', chains=2, random_seed=RNG)\n",
    "\n",
    "with model_locally_periodic:\n",
    "    \n",
    "    f_pred_mult = gp.conditional('f_pred', X_pred_local)\n",
    "    ppc_mult = pm.sample_posterior_predictive(trace_mult, var_names=['f_pred'], random_seed=RNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize all three fits side-by-side:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=('Pure Periodic', 'Additive', 'Locally Periodic'),\n",
    "    horizontal_spacing=0.1,\n",
    "    shared_yaxes=True\n",
    ")\n",
    "\n",
    "def extract_predictions(ppc):\n",
    "    samples = ppc.posterior_predictive['f_pred'].values\n",
    "    mean = samples.mean(axis=(0, 1))\n",
    "    lower = np.percentile(samples, 2.5, axis=(0, 1))\n",
    "    upper = np.percentile(samples, 97.5, axis=(0, 1))\n",
    "    return mean, lower, upper\n",
    "\n",
    "for idx, (ppc, title) in enumerate([(ppc_pure, 'Pure'), (ppc_add, 'Additive'), (ppc_mult, 'Multiplicative')], start=1):\n",
    "    mean, lower, upper = extract_predictions(ppc)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=X_pred_local.flatten(), y=upper, mode='lines',\n",
    "                            line=dict(width=0), showlegend=False, hoverinfo='skip'),\n",
    "                 row=1, col=idx)\n",
    "    fig.add_trace(go.Scatter(x=X_pred_local.flatten(), y=lower, mode='lines',\n",
    "                            fill='tonexty', fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "                            line=dict(width=0), showlegend=False, hoverinfo='skip'),\n",
    "                 row=1, col=idx)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=X_pred_local.flatten(), y=mean, mode='lines',\n",
    "                            name='Posterior mean', line=dict(color='red', width=2),\n",
    "                            showlegend=(idx==1)),\n",
    "                 row=1, col=idx)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df_local_per['x'], y=df_local_per['f_true'],\n",
    "                            mode='lines', name='True function',\n",
    "                            line=dict(color='dodgerblue', width=2, dash='dash'),\n",
    "                            showlegend=(idx==1)),\n",
    "                 row=1, col=idx)\n",
    "\n",
    "    fig.add_trace(go.Scatter(x=df_local_per['x'], y=df_local_per['y'],\n",
    "                            mode='markers', name='Data',\n",
    "                            marker=dict(size=3, color='black', opacity=0.5),\n",
    "                            showlegend=(idx==1)),\n",
    "                 row=1, col=idx)\n",
    "\n",
    "fig.update_yaxes(range=[-5, 5])\n",
    "fig.update_layout(height=400, width=1200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Multiplication Worked Where Addition Failed\n",
    "\n",
    "Looking at the three models side by side reveals the critical difference:\n",
    "\n",
    "**Model 1: Pure Periodic (Left Panel)**\n",
    "This model assumes the periodic pattern exists everywhere with constant amplitude. Outside the central region where the true periodicity exists, the model confidently predicts oscillations that aren't there. This is exactly wrongâ€”it extrapolates periodic behavior globally when the data only shows it locally.\n",
    "\n",
    "**Model 2: Additive (Center Panel)**\n",
    "The additive model tried to explain the data with two independent components: a smooth function plus a global periodic function. While it might capture some structure, it fundamentally misunderstands the data generation process. The smooth component can't \"turn off\" the periodic component in regions where it shouldn't exist. This model will likely predict some oscillations outside the central region (though less extreme than Model 1) and may underestimate the periodic amplitude in the central region where it's strong.\n",
    "\n",
    "**Model 3: Locally Periodic (Right Panel)**\n",
    "The multiplicative model correctly captures the localized periodicity. The posterior mean (red line) closely follows the true function (blue dashed line), showing strong oscillations in the central region and smooth, non-oscillatory behavior outside. The credible interval grows appropriately in regions far from the data, but it doesn't confidently predict spurious oscillations.\n",
    "\n",
    "**The mathematical insight**: When you multiply kernels, the covariance $k_1(x, x') \\times k_2(x, x')$ is high only when **both** kernels have high covariance. The ExpQuad kernel creates an envelope of high covariance around the central region. The Periodic kernel creates high covariance at regular intervals. Their product creates periodic covariance **only within the envelope**â€”exactly what we wanted.\n",
    "\n",
    "**When to use locally periodic kernels**:\n",
    "- Seasonal patterns that emerge or fade over time\n",
    "- Periodic phenomena that only occur in specific regions of input space\n",
    "- Oscillatory behavior that's modulated by another process\n",
    "\n",
    "**Key takeaway**: Multiplication is not just for \"growing amplitude\"â€”it's the right tool whenever one pattern should **gate** or **localize** another. The order doesn't matter in multiplication (kâ‚ Ã— kâ‚‚ = kâ‚‚ Ã— kâ‚), but the interpretation does: think of one kernel as the \"envelope\" and the other as the \"carrier signal.\"\n",
    "\n",
    "Now let's continue exploring multiplicative patterns with another example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ¤– EXERCISE: Multiplicative Kernels with Air Passengers Data\n",
    "\n",
    "You've seen how additive kernels work for the COâ‚‚ data, where trend and seasonality are independent. Now let's explore **multiplicative seasonality**, where seasonal amplitude grows over time.\n",
    "\n",
    "### Understanding Multiplicative Seasonality\n",
    "\n",
    "The Air Passengers dataset shows monthly totals from 1949-1960. Notice how seasonal oscillations grow larger over timeâ€”early years have small swings (Â±10-20), late years have large swings (Â±100-200). The seasonal amplitude is **proportional to the trend level**.\n",
    "\n",
    "This is different from the COâ‚‚ data, where seasonality has constant amplitude. Here, one pattern (the trend) **modulates** another pattern (the seasonality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Air Passengers data from PyMC\n",
    "df_air = pl.read_csv(pm.get_data('AirPassengers.csv'))\n",
    "\n",
    "# Create time index in years\n",
    "df_air = df_air.with_columns([\n",
    "    pl.Series('time_years', [i/12.0 for i in range(len(df_air))])\n",
    "])\n",
    "\n",
    "X_air = df_air['time_years'].to_numpy()[:, None]\n",
    "y_air = df_air['#Passengers'].to_numpy().astype(float)\n",
    "\n",
    "print(f\"Data: {len(X_air)} monthly observations from 1949-1960\")\n",
    "print(f\"Passenger range: {y_air.min():.0f} to {y_air.max():.0f}\")\n",
    "\n",
    "# Visualize\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=X_air.flatten(), y=y_air,\n",
    "    mode='markers+lines',\n",
    "    marker=dict(size=3, color='darkblue'),\n",
    "    line=dict(width=1, color='steelblue')\n",
    ")).update_layout(\n",
    "    title='Air Passengers (1949-1960)',\n",
    "    xaxis_title='Years since 1949',\n",
    "    yaxis_title='Passengers',\n",
    "    height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Your Task: Modeling Growing Seasonality\n",
    "\n",
    "Build a GP model using **multiplicative kernels** to capture the growing seasonal amplitude in the Air Passengers data.\n",
    "\n",
    "Notice how the seasonal fluctuations grow larger over timeâ€”the swings in 1949 are much smaller than in 1960. This requires **multiplication** of kernels, not addition.\n",
    "\n",
    "**Think about**:\n",
    "- Which kernels capture trend and seasonality? (ExpQuad Ã— Periodic)\n",
    "- What lengthscales make sense for 12 years of data?\n",
    "- What is the period for yearly cycles?\n",
    "\n",
    "**Success**: Your model should show small seasonal swings early and large swings later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR LLM-ASSISTED CODE HERE\n",
    "# Build your GP model below. Consider:\n",
    "# - Which kernel captures trend and seasonality?\n",
    "# - What are reasonable lengthscale values for data spanning 12 years?\n",
    "# - How will you handle the observation noise?\n",
    "\n",
    "# Test your implementation\n",
    "\n",
    "\n",
    "# ====================\n",
    "# PLOTTING CODE\n",
    "# ====================\n",
    "# Uncomment after building your model (assumes: model, gp defined)\n",
    "\n",
    "# X_pred = np.linspace(-1, 13, 300)[:, None]\n",
    "# \n",
    "# with model:\n",
    "#     f_pred = gp.conditional('f_pred', X_pred)\n",
    "# \n",
    "# mu = f_pred.eval()\n",
    "# \n",
    "# # Plot with credible intervals\n",
    "# fig = go.Figure()\n",
    "# \n",
    "# for n_std, alpha in [(3, 0.1), (2, 0.15), (1, 0.25)]:\n",
    "#     sigma = np.sqrt(gp.conditional('f_pred', X_pred).eval())\n",
    "#     fig.add_trace(go.Scatter(\n",
    "#         x=np.concatenate([X_pred.flatten(), X_pred.flatten()[::-1]]),\n",
    "#         y=np.concatenate([mu + n_std*sigma, (mu - n_std*sigma)[::-1]]),\n",
    "#         fill='toself', fillcolor=f'rgba(255,100,100,{alpha})',\n",
    "#         line=dict(width=0), showlegend=False, hoverinfo='skip'\n",
    "#     ))\n",
    "# \n",
    "# fig.add_trace(go.Scatter(x=X_pred.flatten(), y=mu, \n",
    "#                          line=dict(color='darkred', width=2), name='GP Mean'))\n",
    "# fig.add_trace(go.Scatter(x=X_air.flatten(), y=y_air, mode='markers',\n",
    "#                          marker=dict(color='darkblue', size=4), name='Data'))\n",
    "# \n",
    "# fig.update_layout(title='Multiplicative GP: Growing Seasonal Amplitude',\n",
    "#                   xaxis_title='Years since 1949', yaxis_title='Passengers',\n",
    "#                   height=450)\n",
    "# fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding Domain Knowledge Through Kernel Design\n",
    "\n",
    "Beyond choosing from standard kernels, you can encode specific domain knowledge directly into kernel structure. Here are a few advanced patterns:\n",
    "\n",
    "**Symmetry**\n",
    "\n",
    "If you know your function has reflective symmetry around the origin ($f(x) = f(-x)$), you can enforce this:\n",
    "\n",
    "$$\n",
    "k_{\\text{symmetric}}(x, x') = k(x, x') + k(-x, x')\n",
    "$$\n",
    "\n",
    "This doubles the effective data by using the symmetry constraint. For example, if you're modeling physical phenomena that must be symmetric (like the electric field around a charged sphere), enforcing this constraint improves efficiency and prevents the model from learning spurious asymmetries.\n",
    "\n",
    "**Low-Dimensional Structure**\n",
    "\n",
    "If you suspect your high-dimensional input actually varies along a low-dimensional manifold, project before applying the kernel:\n",
    "\n",
    "$$\n",
    "k_{\\text{low-dim}}(\\mathbf{x}, \\mathbf{x}') = k(\\mathbf{A}\\mathbf{x}, \\mathbf{A}\\mathbf{x}')\n",
    "$$\n",
    "\n",
    "where $\\mathbf{A}$ is a low-rank matrix (e.g., $m \\times d$ where $m \\ll d$). This is like doing dimensionality reduction before GP regression. You can learn $\\mathbf{A}$ jointly with the GP, creating a model that discovers the relevant low-dimensional structure.\n",
    "\n",
    "**Categorical Variables**\n",
    "\n",
    "For categorical inputs (e.g., \"red\", \"blue\", \"green\"), use one-hot encoding with product kernels:\n",
    "\n",
    "```python\n",
    "# For 2D input: continuous variable and 3-level categorical\n",
    "# Encode categorical as [1,0,0], [0,1,0], [0,0,1]\n",
    "with pm.Model() as model:\n",
    "    # Continuous dimension\n",
    "    ell_cont = pm.Gamma('ell_cont', alpha=2, beta=1)\n",
    "    cov_continuous = pm.gp.cov.Matern52(1, ls=ell_cont)\n",
    "    \n",
    "    # Categorical dimension (3 levels encoded as 3D one-hot)\n",
    "    ell_cat = pm.Gamma('ell_cat', alpha=2, beta=1, shape=3)\n",
    "    cov_categorical = pm.gp.cov.ExpQuad(3, ls=ell_cat)\n",
    "    \n",
    "    # Combine multiplicatively\n",
    "    cov_total = cov_continuous * cov_categorical\n",
    "```\n",
    "\n",
    "The model learns lengthscales for the categorical encoding, effectively learning whether categories should share information. Small lengthscales mean the categories are very different; large lengthscales mean they're similar.\n",
    "\n",
    "**Periodic Boundary Conditions**\n",
    "\n",
    "For phenomena defined on a circle (e.g., compass directions, time of day), the input space wraps around. Instead of treating 359Â° and 1Â° as distant, encode the input as $(\\cos(\\theta), \\sin(\\theta))$ and use a standard kernel on this 2D representation.\n",
    "\n",
    "**When to use these patterns**: Only when you have strong domain knowledge that justifies the constraint. Imposing wrong symmetry or structure will harm performance. When in doubt, stick with simpler kernels and let the data speak.\n",
    "\n",
    "These advanced patterns are mostly relevant for specialized applications. For most practical GP modeling, you'll succeed with the standard kernels and composition rules we've explored. The key is to start simple and add complexity only when data or domain knowledge clearly indicates it's needed.\n",
    "\n",
    "Now that we have a comprehensive understanding of kernels and their properties, let's see how to handle non-Gaussian data through flexible likelihood choices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2.5: Non-Gaussian Likelihoods - Robust Regression with Student-T\n",
    "\n",
    "So far, we've used Gaussian noise (in `pm.gp.Marginal`) or Bernoulli outcomes (in classification). But real data often contains **outliers**â€”observations that don't fit the typical pattern. Gaussian distributions are not robust to outliers: a single extreme value can dramatically affect inference.\n",
    "\n",
    "The **Student-T distribution** offers a robust alternative. It has heavier tails than a Gaussian, meaning it assigns higher probability to extreme values. This makes it much more tolerant of outliers.\n",
    "\n",
    "### Why Student-T for Robust Regression?\n",
    "\n",
    "The Student-T distribution has three parameters:\n",
    "- **Location ($\\mu$)**: The center, similar to the mean\n",
    "- **Scale ($\\sigma$)**: The spread, similar to standard deviation\n",
    "- **Degrees of freedom ($\\nu$)**: Controls tail heaviness\n",
    "\n",
    "The degrees of freedom parameter is key:\n",
    "- Small $\\nu$ (e.g., 2-5) â†’ very heavy tails, very robust to outliers\n",
    "- Large $\\nu$ (> 30) â†’ approaches normal distribution\n",
    "\n",
    "Since we need `pm.gp.Latent` for non-Gaussian likelihoods, let's see robust regression in action with Student-T likelihood."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Data with Outliers\n",
    "\n",
    "Let's generate data where most observations are clean, but a few are extreme outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data with outliers\n",
    "n_robust = 100\n",
    "X_robust = np.linspace(0, 10, n_robust)[:, None]\n",
    "\n",
    "# True function\n",
    "f_true_robust = 2 * np.sin(1.5 * X_robust.flatten())\n",
    "\n",
    "# Most observations have small Gaussian noise\n",
    "y_robust = f_true_robust + 0.3 * RNG.standard_normal(n_robust)\n",
    "\n",
    "# Add outliers: replace 10% of points with extreme values\n",
    "n_outliers = 10\n",
    "outlier_indices = RNG.choice(n_robust, size=n_outliers, replace=False)\n",
    "y_robust[outlier_indices] += RNG.choice([-1, 1], size=n_outliers) * RNG.uniform(3, 6, size=n_outliers)\n",
    "\n",
    "df_robust = pl.DataFrame({\n",
    "    'x': X_robust.flatten(),\n",
    "    'y': y_robust,\n",
    "    'f_true': f_true_robust,\n",
    "    'is_outlier': [i in outlier_indices for i in range(n_robust)]\n",
    "})\n",
    "\n",
    "# Visualize\n",
    "fig = go.Figure()\n",
    "\n",
    "# Regular points\n",
    "df_regular = df_robust.filter(pl.col('is_outlier') == False)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_regular['x'],\n",
    "    y=df_regular['y'],\n",
    "    mode='markers',\n",
    "    name='Regular observations',\n",
    "    marker=dict(size=5, color='black')\n",
    "))\n",
    "\n",
    "# Outliers\n",
    "df_outlier = df_robust.filter(pl.col('is_outlier') == True)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_outlier['x'],\n",
    "    y=df_outlier['y'],\n",
    "    mode='markers',\n",
    "    name='Outliers',\n",
    "    marker=dict(size=8, color='red', symbol='x')\n",
    "))\n",
    "\n",
    "# True function\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df_robust['x'],\n",
    "    y=df_robust['f_true'],\n",
    "    mode='lines',\n",
    "    name='True function',\n",
    "    line=dict(color='dodgerblue', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Data with Outliers',\n",
    "    xaxis_title='X',\n",
    "    yaxis_title='Y',\n",
    "    hovermode='x unified',\n",
    "    width=900,\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observing the Outliers\n",
    "\n",
    "The red X markers show outliersâ€”observations that are far from the true function (blue line). If we used a Gaussian likelihood, these outliers would pull our posterior predictions toward them, distorting our estimate of the true function.\n",
    "\n",
    "Let's build a robust model using a Student-T likelihood with `pm.gp.Latent`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as robust_model:\n",
    "    # Hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=3)\n",
    "    \n",
    "    # Covariance function\n",
    "    cov_func = eta**2 * pm.gp.cov.Matern52(1, ell)\n",
    "    \n",
    "    # Latent GP\n",
    "    gp = pm.gp.Latent(cov_func=cov_func)\n",
    "    f = gp.prior('f', X=X_robust)\n",
    "    \n",
    "    # Student-T likelihood for robustness\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    nu = pm.Gamma('nu', alpha=2, beta=0.1)  # Prior favors low nu (heavy tails)\n",
    "    \n",
    "    y_obs = pm.StudentT(\n",
    "        'y',\n",
    "        mu=f,\n",
    "        sigma=sigma,\n",
    "        nu=nu,\n",
    "        observed=y_robust\n",
    "    )\n",
    "    \n",
    "    # Sample\n",
    "    robust_trace = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        nuts_sampler='nutpie',\n",
    "        chains=2,\n",
    "        target_accept=0.9,\n",
    "        random_seed=RNG\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Robust Model\n",
    "\n",
    "The key difference from our earlier Gaussian models is the likelihood:\n",
    "\n",
    "```python\n",
    "y_obs = pm.StudentT('y', mu=f, sigma=sigma, nu=nu, observed=y_robust)\n",
    "```\n",
    "\n",
    "We're learning the degrees of freedom parameter `nu` from the data. The prior `Gamma(2, 0.1)` suggests we expect heavy tails (small nu), but leaves room for the data to tell us otherwise.\n",
    "\n",
    "Let's see how well this handles the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the learned nu parameter\n",
    "nu_summary = az.summary(robust_trace, var_names=['nu'])\n",
    "print(\"Degrees of freedom posterior:\")\n",
    "print(nu_summary)\n",
    "print(f\"\\nMean nu: {nu_summary['mean'].values[0]:.2f}\")\n",
    "print(\"(Small nu indicates heavy tails were needed)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Robust Predictions\n",
    "\n",
    "Now let's predict and see if the model successfully ignored the outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions\n",
    "X_pred_robust = np.linspace(-1, 11, 300)[:, None]\n",
    "\n",
    "USE_CACHED = True\n",
    "\n",
    "if USE_CACHED and os.path.exists(OUTPUT_DIR + \"robust_ppc.nc\"):\n",
    "    ppc_robust = az.from_netcdf(OUTPUT_DIR + \"robust_ppc.nc\")\n",
    "\n",
    "else:\n",
    "\n",
    "    with robust_model:\n",
    "        f_pred_robust = gp.conditional('f_pred', X_pred_robust)\n",
    "        ppc_robust = pm.sample_posterior_predictive(\n",
    "            robust_trace,\n",
    "            var_names=['f_pred'],\n",
    "            random_seed=RNG\n",
    "        )\n",
    "    ppc_robust.to_netcdf(OUTPUT_DIR + \"robust_ppc.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize using PyMC's built-in plotting function\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "# Plot the GP posterior distribution\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax=ax,\n",
    "    x=X_pred_robust.flatten(),\n",
    "    samples=ppc_robust.posterior_predictive['f_pred'].values.reshape(-1, len(X_pred_robust)),\n",
    "    plot_samples=False, \n",
    "    palette='Reds'\n",
    ")\n",
    "\n",
    "# True function\n",
    "ax.plot(\n",
    "    df_robust['x'],\n",
    "    df_robust['f_true'],\n",
    "    color='dodgerblue',\n",
    "    linewidth=2,\n",
    "    linestyle='--',\n",
    "    label='True function'\n",
    ")\n",
    "\n",
    "# Regular observations\n",
    "ax.scatter(\n",
    "    df_regular['x'],\n",
    "    df_regular['y'],\n",
    "    s=25,\n",
    "    color='black',\n",
    "    label='Regular observations',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "# Outliers\n",
    "ax.scatter(\n",
    "    df_outlier['x'],\n",
    "    df_outlier['y'],\n",
    "    s=64,\n",
    "    color='red',\n",
    "    marker='x',\n",
    "    linewidths=2,\n",
    "    label='Outliers',\n",
    "    zorder=5\n",
    ")\n",
    "\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_title('Robust GP with Student-T Likelihood')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Robust Results\n",
    "\n",
    "Excellent! The posterior mean (red line) closely follows the true function (blue dashed line), essentially ignoring the outliers (red X markers). The Student-T likelihood has successfully downweighted the extreme observations.\n",
    "\n",
    "Compare this to what would happen with a Gaussian likelihoodâ€”the outliers would pull the predicted function toward them, creating bulges in the estimate. The Student-T likelihood's heavy tails allow the model to say \"these observations are just noise from the tail of the distribution,\" rather than trying to fit them.\n",
    "\n",
    "**When to use robust likelihoods**:\n",
    "- When you suspect your data contains outliers\n",
    "- When measurement errors might occasionally be much larger than usual\n",
    "- When you want your model to be less sensitive to a few extreme values\n",
    "\n",
    "The cost is slower sampling (since we use `pm.gp.Latent`), but the benefit is much more reliable inference in the presence of outliers."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
