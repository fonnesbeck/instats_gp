{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_4.ipynb)\n",
    "\n",
    "# Session 4: Multi-Output GPs and Case Study\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "1. Understand multi-output GP models for correlated outputs\n",
    "2. Handle multidimensional inputs with ARD lengthscales\n",
    "3. Build coregionalized models using Hadamard product kernels\n",
    "4. Execute a comprehensive case study: Soccer player skill modeling\n",
    "5. Integrate hierarchical structure and non-Gaussian likelihoods\n",
    "6. Interpret factor models that decompose skill from context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import polars as pl\n",
    "\n",
    "# PyMC ecosystem\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "# Visualization\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "pio.renderers.default = \"plotly_mimetype+notebook_connected\"\n",
    "\n",
    "print(f\"PyMC: {pm.__version__}, NumPy: {np.__version__}\")\n",
    "print(f\"Polars: {pl.__version__}, ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part A: Multi-Output Gaussian Processes\n",
    "\n",
    "### Why Model Multiple Outputs Together?\n",
    "\n",
    "Imagine analyzing 27 elite soccer players. You could fit 27 separate GPs, but this misses that **players operate in a shared context**.\n",
    "\n",
    "Multi-output GPs offer:\n",
    "1. **Information sharing** between related outputs\n",
    "2. **Partial pooling** for data-scarce outputs\n",
    "3. **Learned correlation structure**  \n",
    "4. **Computational efficiency**\n",
    "\n",
    "Let's start with multidimensional inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Automatic Relevance Determination (ARD)\n",
    "\n",
    "ARD assigns independent lengthscales to each input dimension:\n",
    "\n",
    "$$k(\\mathbf{x}, \\mathbf{x}') = \\sigma^2 \\exp\\left(-\\frac{1}{2}\\sum_{i=1}^d \\frac{(x_i - x'_i)^2}{\\ell_i^2}\\right)$$\n",
    "\n",
    "Large $\\ell_i$ â†’ dimension $i$ is irrelevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Synthetic Data\n",
    "\n",
    "Create 2D data where only x1 matters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_obs = 150\n",
    "x1 = RNG.uniform(-3, 3, n_obs)  # Relevant\n",
    "x2 = RNG.uniform(-3, 3, n_obs)  # Irrelevant\n",
    "y_obs = np.sin(2 * x1) + 0.5 * x1 + RNG.normal(0, 0.2, n_obs)\n",
    "X_train = np.column_stack([x1, x2])\n",
    "\n",
    "print(f\"{X_train.shape[0]} observations, {X_train.shape[1]} features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting ARD Model\n",
    "\n",
    "Watch what lengthscales the model learns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as ard_model:\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1, shape=2)\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ls)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_obs, sigma=sigma)\n",
    "    trace_ard = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learned Lengthscales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_post = az.extract(trace_ard, var_names=[\"ls\"])\n",
    "ls_means = ls_post.mean(dim=\"sample\").values\n",
    "\n",
    "fig = go.Figure()\n",
    "for i in range(2):\n",
    "    fig.add_trace(go.Violin(y=ls_post.sel(ls_dim_0=i).values,\n",
    "                            name=f\"Feature {i+1}\", box_visible=True))\n",
    "fig.update_layout(title=\"Learned Lengthscales\", yaxis_title=\"Lengthscale\")\n",
    "fig.show()\n",
    "\n",
    "print(f\"F1: {ls_means[0]:.2f}, F2: {ls_means[1]:.2f}\")\n",
    "print(f\"Ratio: {ls_means[1]/ls_means[0]:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Feature 1: small ls â†’ relevant  \n",
    "Feature 2: large ls â†’ irrelevant\n",
    "\n",
    "ARD discovered which dimension matters!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32c846",
   "metadata": {},
   "source": [
    "### Diving Deeper into ARD\n",
    "\n",
    "We just saw that ARD successfully identified which input dimension matters. The ratio of lengthscales tells us how strongly the model believes dimension 2 is irrelevant compared to dimension 1.\n",
    "\n",
    "In practice, ARD enables **automatic feature selection**: irrelevant dimensions get large lengthscales (model becomes insensitive to them), while important dimensions get small lengthscales (model pays close attention).\n",
    "\n",
    "Now let's move from multidimensional **inputs** to multiple **outputs**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d8ba97",
   "metadata": {},
   "source": [
    "## Intrinsic Coregionalization Model (ICM)\n",
    "\n",
    "Imagine you're tracking the fastball spin rates of 5 elite pitchers across a baseball season. You could fit 5 separate GPs, one per pitcher. But what if some pitchers have similar mechanics, and their spin rates fluctuate together?\n",
    "\n",
    "**Multi-output Gaussian Processes** let us model related outputs jointly, learning:\n",
    "1. How each output evolves over inputs (e.g., time)\n",
    "2. How outputs correlate with each other\n",
    "\n",
    "The key insight: if outputs are related, we should **share statistical strength** across them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae2516e",
   "metadata": {},
   "source": [
    "### The Mathematics of Sharing Structure\n",
    "\n",
    "ICM uses the Kronecker product (âŠ—) to combine two covariance structures:\n",
    "\n",
    "$$K_{ICM}([\\mathbf{x}_i, o_i], [\\mathbf{x}_j, o_j]) = K_{input}(\\mathbf{x}_i, \\mathbf{x}_j) \\times B(o_i, o_j)$$\n",
    "\n",
    "Where:\n",
    "- $K_{input}(\\mathbf{x}_i, \\mathbf{x}_j)$: Covariance over inputs (e.g., time)\n",
    "- $B(o_i, o_j)$: **Coregionalization matrix** â€” covariance between outputs  \n",
    "- $o_i, o_j$: Output indices (e.g., pitcher 0, pitcher 1, ...)\n",
    "\n",
    "Think of it as: *\"How similar are these inputs?\"* multiplied by *\"How correlated are these outputs?\"*\n",
    "\n",
    "The coregionalization matrix has a special structure that ensures it's positive semi-definite:\n",
    "\n",
    "$$B = WW^T + \\text{diag}(\\kappa)$$\n",
    "\n",
    "This separates:\n",
    "- $WW^T$: Shared variations across outputs (low-rank structure)\n",
    "- $\\text{diag}(\\kappa)$: Output-specific independent noise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ea50ae",
   "metadata": {},
   "source": [
    "### Real Data: Baseball Pitcher Spin Rates\n",
    "\n",
    "Let's see ICM in action with real data. We'll model fastball spin rates of 5 elite pitchers across the 2021 MLB season.\n",
    "\n",
    "**Why spin rate matters:** Higher spin rates make fastballs harder to hit. Spin rate fluctuates game-to-game due to:\n",
    "- Fatigue accumulation  \n",
    "- Mechanics adjustments  \n",
    "- Measurement noise  \n",
    "- Potentially shared factors (weather, ball characteristics)\n",
    "\n",
    "**Hypothesis:** Some pitchers' spin rates may be correlated if they have similar mechanics or respond similarly to external factors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99fa214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load baseball spin rate data\n",
    "df_spin = pl.read_csv(\"../data/fastball_spin_rates.csv\")\n",
    "\n",
    "# Standardize spin rates to z-scores\n",
    "mean_spin = df_spin[\"avg_spin_rate\"].mean()\n",
    "std_spin = df_spin[\"avg_spin_rate\"].std()\n",
    "df_spin = df_spin.with_columns([\n",
    "    ((pl.col(\"avg_spin_rate\") - mean_spin) / std_spin).alias(\"avg_spin_rate_std\")\n",
    "])\n",
    "\n",
    "print(f\"Total: {df_spin.height} observations, {df_spin['pitcher_name'].n_unique()} pitchers\")\n",
    "print(f\"Date range: {df_spin['game_date'].min()} to {df_spin['game_date'].max()}\")\n",
    "df_spin.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1692087",
   "metadata": {},
   "source": [
    "We standardized spin rates so all pitchers are on the same scale (z-scores). This makes the coregionalization matrix more interpretable: values close to 1 indicate strong correlation between pitchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4279b793",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top 5 pitchers by number of games\n",
    "top_pitchers_df = (df_spin\n",
    "    .group_by(\"pitcher_name\")\n",
    "    .agg(pl.count(\"game_date\").alias(\"n_games\"))\n",
    "    .sort(\"n_games\", descending=True)\n",
    "    .head(5)\n",
    ")\n",
    "\n",
    "top_pitchers = top_pitchers_df[\"pitcher_name\"].to_list()\n",
    "print(\"Top 5 pitchers:\")\n",
    "print(top_pitchers_df)\n",
    "\n",
    "# Filter to these pitchers\n",
    "df_train = df_spin.filter(pl.col(\"pitcher_name\").is_in(top_pitchers))\n",
    "print(f\"\\nTraining data: {df_train.height} observations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5410799e",
   "metadata": {},
   "source": [
    "Now we create two key index variables:\n",
    "\n",
    "1. **`game_date_idx`**: Integer days since season start (April 1, 2021 = day 0)  \n",
    "2. **`output_idx`**: Pitcher number (0 to 4)\n",
    "\n",
    "Our input matrix $X$ will be $(n, 2)$ where each row is `[game_date_idx, output_idx]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2e714e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to datetime\n",
    "df_train = df_train.with_columns([\n",
    "    pl.col(\"game_date\").str.strptime(pl.Date, format=\"%Y-%m-%d\").alias(\"game_date_dt\")\n",
    "])\n",
    "\n",
    "# Create game date index (days since season start)\n",
    "min_date = df_train[\"game_date_dt\"].min()\n",
    "df_train = df_train.with_columns([\n",
    "    (pl.col(\"game_date_dt\") - min_date).dt.total_days().alias(\"game_date_idx\")\n",
    "])\n",
    "\n",
    "# Create output index\n",
    "pitcher_to_idx = {name: idx for idx, name in enumerate(top_pitchers)}\n",
    "df_train = df_train.with_columns([\n",
    "    pl.col(\"pitcher_name\").replace(pitcher_to_idx).alias(\"output_idx\")\n",
    "])\n",
    "\n",
    "# Sort by output then time\n",
    "df_train = df_train.sort([\"output_idx\", \"game_date_idx\"])\n",
    "\n",
    "print(\"Data structure:\")\n",
    "print(df_train.select([\"pitcher_name\", \"game_date_idx\", \"output_idx\", \"avg_spin_rate_std\"]).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59474214",
   "metadata": {},
   "source": [
    "### Visualizing the Raw Data\n",
    "\n",
    "Before modeling, let's examine the raw time series. This helps us understand trends, volatility, and potential correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc91a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive time series plot\n",
    "fig = go.Figure()\n",
    "colors = px.colors.qualitative.Set2\n",
    "\n",
    "for i, pitcher in enumerate(top_pitchers):\n",
    "    pitcher_data = df_train.filter(pl.col(\"pitcher_name\") == pitcher)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pitcher_data[\"game_date_idx\"].to_list(),\n",
    "        y=pitcher_data[\"avg_spin_rate_std\"].to_list(),\n",
    "        mode='markers',\n",
    "        name=pitcher,\n",
    "        marker=dict(size=5, color=colors[i]),\n",
    "        opacity=0.7\n",
    "    ))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Fastball Spin Rates: 2021 Season (Standardized)\",\n",
    "    xaxis_title=\"Days Since Season Start\",\n",
    "    yaxis_title=\"Standardized Spin Rate\",\n",
    "    height=450,\n",
    "    hovermode='closest'\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e55949a",
   "metadata": {},
   "source": [
    "Each pitcher shows noisy variation. Some appear to have trends (gradual changes over the season), while others look more stationary. The ICM model will:\n",
    "\n",
    "1. **Smooth** trajectories to separate signal from noise  \n",
    "2. **Learn correlations** between pitchers  \n",
    "3. **Quantify uncertainty** with credible intervals\n",
    "\n",
    "Now let's build the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc60ad4",
   "metadata": {},
   "source": [
    "### Building the ICM: Helper Function\n",
    "\n",
    "We define `get_icm()` to construct an ICM kernel. This combines an input kernel with a `Coregion` kernel using the Hadamard product (`*`).\n",
    "\n",
    "The `active_dims` parameter tells each kernel which columns to operate on:\n",
    "- Input kernel uses `active_dims=[0]` (time)  \n",
    "- Coregion kernel uses `active_dims=[1]` (pitcher index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22503314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_icm(input_dim, kernel, W=None, kappa=None, B=None, active_dims=None):\n",
    "    \"\"\"\n",
    "    Construct Intrinsic Coregionalization Model kernel.\n",
    "    \n",
    "    Combines input kernel with output coregionalization via Hadamard product.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_dim : int\n",
    "        Total input dimensions (including output index)\n",
    "    kernel : pm.gp.cov.Covariance\n",
    "        Base kernel for inputs (e.g., ExpQuad over time)\n",
    "    W, kappa, B : tensors, optional\n",
    "        Coregionalization parameters\n",
    "    active_dims : list, optional\n",
    "        Dimensions for coregion kernel (typically [1] for output index)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    pm.gp.cov.Covariance\n",
    "        ICM kernel\n",
    "    \"\"\"\n",
    "    coreg = pm.gp.cov.Coregion(\n",
    "        input_dim=input_dim,\n",
    "        W=W,\n",
    "        kappa=kappa,\n",
    "        B=B,\n",
    "        active_dims=active_dims\n",
    "    )\n",
    "    # Hadamard product: kernel * coreg\n",
    "    icm_cov = kernel * coreg\n",
    "    return icm_cov\n",
    "\n",
    "print(\"âœ“ Helper function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412d2fb0",
   "metadata": {},
   "source": [
    "**Critical distinction:**\n",
    "- Kernel **addition** (`+`): combines multiple processes  \n",
    "- Kernel **multiplication** (`*`): Hadamard product for ICM\n",
    "\n",
    "The `*` operator creates a covariance where the input kernel and coregion kernel operate **independently** on their designated dimensions, then multiply."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba438de3",
   "metadata": {},
   "source": [
    "### Preparing Data for PyMC\n",
    "\n",
    "Convert polars DataFrame to numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8121b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training arrays\n",
    "X_train = df_train.select([\"game_date_idx\", \"output_idx\"]).to_numpy().astype(np.float64)\n",
    "y_train = df_train.select(\"avg_spin_rate_std\").to_numpy().flatten()\n",
    "\n",
    "n_outputs = len(top_pitchers)\n",
    "\n",
    "print(f\"X shape: {X_train.shape} (rows x [time, output_idx])\")\n",
    "print(f\"y shape: {y_train.shape}\")\n",
    "print(f\"n_outputs: {n_outputs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae12ad2",
   "metadata": {},
   "source": [
    "### Specifying Priors\n",
    "\n",
    "We need priors for:\n",
    "- **Lengthscale** (`ell`): How quickly spin rate changes. `Gamma(2, 0.5)` gives mean â‰ˆ 4 days.\n",
    "- **Amplitude** (`eta`): Overall temporal variation. `Gamma(3, 1)` gives mean = 3.\n",
    "- **W**: Weight matrix $(5 \\\\times 2)$. Rank 2 assumes pitchers share â‰¤2 latent patterns.\n",
    "- **kappa**: Output-specific variances.\n",
    "- **sigma**: Observation noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c35ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as icm_model:\n",
    "    # Temporal kernel parameters\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=0.5)\n",
    "    eta = pm.Gamma(\"eta\", alpha=3, beta=1)\n",
    "    \n",
    "    # Base kernel on time (active_dims=[0])\n",
    "    kernel_time = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ell, active_dims=[0])\n",
    "    \n",
    "    # Coregionalization parameters\n",
    "    W = pm.Normal(\"W\", mu=0, sigma=3, shape=(n_outputs, 2),\n",
    "                  initval=RNG.standard_normal((n_outputs, 2)))\n",
    "    kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "    \n",
    "    # Track B matrix\n",
    "    B = pm.Deterministic(\"B\", pt.dot(W, W.T) + pt.diag(kappa))\n",
    "    \n",
    "    # ICM kernel\n",
    "    cov_icm = get_icm(input_dim=2, kernel=kernel_time, W=W, kappa=kappa, active_dims=[1])\n",
    "    \n",
    "    # GP\n",
    "    gp = pm.gp.Marginal(cov_func=cov_icm)\n",
    "    \n",
    "    # Noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=sigma)\n",
    "\n",
    "pm.model_to_graphviz(icm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af606c16",
   "metadata": {},
   "source": [
    "### Sampling the Posterior\n",
    "\n",
    "This may take a few minutes..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e68e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "with icm_model:\n",
    "    trace_icm = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        chains=2,\n",
    "        target_accept=0.95\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8855fe69",
   "metadata": {},
   "source": [
    "Let's check convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82edd331",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(trace_icm, var_names=[\"ell\", \"eta\", \"sigma\", \"kappa\"])\n",
    "print(f\"R-hat: [{summary['r_hat'].min():.4f}, {summary['r_hat'].max():.4f}]\")\n",
    "print(f\"ESS: [{summary['ess_bulk'].min():.0f}, {summary['ess_bulk'].max():.0f}]\")\n",
    "print(\"\\nEstimates:\")\n",
    "print(summary[[\"mean\", \"sd\", \"hdi_3%\", \"hdi_97%\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5debc5",
   "metadata": {},
   "source": [
    "Good convergence! The model has learned temporal dynamics and output correlations simultaneously.\n",
    "\n",
    "Now let's make predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc09f8f",
   "metadata": {},
   "source": [
    "### Posterior Predictions\n",
    "\n",
    "Create test grid for all 5 pitchers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29357e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test data: 200 time points\n",
    "n_test = 200\n",
    "time_test = np.linspace(0, 199, n_test)\n",
    "\n",
    "# Stack for all outputs\n",
    "X_test_list = []\n",
    "for out_idx in range(n_outputs):\n",
    "    X_out = np.column_stack([time_test, np.full(n_test, out_idx)])\n",
    "    X_test_list.append(X_out)\n",
    "\n",
    "X_test = np.vstack(X_test_list)\n",
    "print(f\"Test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fd9704",
   "metadata": {},
   "outputs": [],
   "source": [
    "with icm_model:\n",
    "    f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "    ppc_icm = pm.sample_posterior_predictive(\n",
    "        trace_icm,\n",
    "        var_names=[\"f_pred\"],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a064ea5d",
   "metadata": {},
   "source": [
    "### Visualizing Multi-Output Predictions\n",
    "\n",
    "Plot posterior for each pitcher:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f7ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(\n",
    "    rows=5, cols=1,\n",
    "    subplot_titles=top_pitchers,\n",
    "    vertical_spacing=0.05,\n",
    "    shared_xaxes=True\n",
    ")\n",
    "\n",
    "for i, pitcher in enumerate(top_pitchers):\n",
    "    # Extract predictions for this output\n",
    "    f_pred_i = ppc_icm.posterior_predictive[\"f_pred\"].isel(\n",
    "        f_pred_dim_0=slice(i*n_test, (i+1)*n_test)\n",
    "    )\n",
    "    \n",
    "    mean = f_pred_i.mean(dim=[\"chain\", \"draw\"]).values\n",
    "    lower = np.percentile(f_pred_i.values, 2.5, axis=(0,1))\n",
    "    upper = np.percentile(f_pred_i.values, 97.5, axis=(0,1))\n",
    "    \n",
    "    # HDI band\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=upper, line=dict(width=0),\n",
    "        showlegend=False, hoverinfo='skip'\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=lower, fill='tonexty',\n",
    "        line=dict(width=0), showlegend=False,\n",
    "        fillcolor='rgba(135,206,250,0.3)'\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Mean\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=time_test, y=mean, mode='lines',\n",
    "        line=dict(color='steelblue', width=2),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "    \n",
    "    # Training data\n",
    "    pitcher_data = df_train.filter(pl.col(\"pitcher_name\") == pitcher)\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=pitcher_data[\"game_date_idx\"].to_list(),\n",
    "        y=pitcher_data[\"avg_spin_rate_std\"].to_list(),\n",
    "        mode='markers',\n",
    "        marker=dict(color='red', size=3),\n",
    "        showlegend=False\n",
    "    ), row=i+1, col=1)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Days Since Season Start\", row=5, col=1)\n",
    "fig.update_layout(height=1000, title_text=\"ICM Posterior Predictions\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0329aff5",
   "metadata": {},
   "source": [
    "Notice how the model smooths noisy observations while respecting each pitcher's pattern. ,Uncertainty is wider where data is sparse, narrower with more observations.\n",
    "\n",
    "Now let's examine the learned correlations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e92d8ae",
   "metadata": {},
   "source": [
    "### The Coregionalization Matrix: Who's Correlated?\n",
    "\n",
    "Extract learned $B$ matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0742f8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Posterior mean of B\n",
    "B_post = az.extract(trace_icm, var_names=[\"B\"]).mean(dim=\"sample\").values\n",
    "\n",
    "# Heatmap\n",
    "fig = go.Figure(data=go.Heatmap(\n",
    "z=B_post,\n",
    "x=top_pitchers,\n",
    "y=top_pitchers,\n",
    "colorscale='RdBu',\n",
    "zmid=0,\n",
    "text=np.round(B_post, 2),\n",
    "texttemplate='%{text}',\n",
    "textfont={\"size\": 10}\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "title=\"Learned Coregionalization Matrix B\",\n",
    "xaxis_title=\"Pitcher\",\n",
    "yaxis_title=\"Pitcher\",\n",
    "height=450\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Convert to correlation\n",
    "std_devs = np.sqrt(np.diag(B_post))\n",
    "corr = B_post / np.outer(std_devs, std_devs)\n",
    "print(\"\\nCorrelations:\")\n",
    "print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5fe85f",
   "metadata": {},
   "source": [
    "Diagonal elements are variances. Off-diagonal elements show shared variation. ,High positive covariance means those pitchers' spin rates fluctuate together.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141332d6",
   "metadata": {},
   "source": [
    "## Linear Coregionalization Model (LCM)\n",
    "\n",
    "ICM assumes a **single latent process** drives output correlations. But what if there are **multiple independent sources** of variation?\n",
    "\n",
    "Examples:\n",
    "- Long-term trends (aging, mechanics changes)\n",
    "- Short-term fluctuations (fatigue, noise)\n",
    "\n",
    "LCM extends ICM by **summing multiple ICM kernels**:\n",
    "\n",
    "$$K_{LCM} = B_1 \\\\otimes K_1 + B_2 \\\\otimes K_2 + ...$$\n",
    "\n",
    "This allows different output correlations at different timescales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c630316",
   "metadata": {},
   "source": [
    "### When to Use LCM Over ICM\n",
    "\n",
    "**Use ICM when:**\n",
    "- Single timescale dominates\n",
    "- Simplicity is priority\n",
    "- Limited data\n",
    "\n",
    "**Use LCM when:**\n",
    "- Multiple timescales evident\n",
    "- Different correlations at different scales\n",
    "- Sufficient data for complexity\n",
    "\n",
    "For baseball, we might combine:\n",
    "- ExpQuad: smooth long-term trends\n",
    "- Matern32: short-term wiggles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaed2304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lcm(input_dim, active_dims, num_outputs, kernels, W=None, kappa=None, B=None, name=\"LCM\"):\n",
    "    \"\"\"\n",
    "    Construct Linear Coregionalization Model kernel.\n",
    "    \n",
    "    Sums multiple ICM kernels.\n",
    "    \"\"\"\n",
    "    if B is None:\n",
    "        if kappa is None:\n",
    "            kappa = pm.Gamma(f\"{name}_kappa\", alpha=5, beta=1, shape=num_outputs)\n",
    "        if W is None:\n",
    "            W = pm.Normal(f\"{name}_W\", mu=0, sigma=5, shape=(num_outputs, 1),\n",
    "                         initval=RNG.standard_normal((num_outputs, 1)))\n",
    "    else:\n",
    "        kappa = None\n",
    "    \n",
    "    # Sum ICMs\n",
    "    cov_lcm = 0\n",
    "    for kernel in kernels:\n",
    "        icm = get_icm(input_dim, kernel, W, kappa, B, active_dims)\n",
    "        cov_lcm += icm\n",
    "    \n",
    "    return cov_lcm\n",
    "\n",
    "print(\"âœ“ LCM helper defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d9ad74",
   "metadata": {},
   "source": [
    "This reuses the same $W$ and $\\\\kappa$ across kernels (shared coregionalization), ,but each kernel contributes its own input covariance shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433d4d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as lcm_model:\n",
    "    # Two lengthscales\n",
    "    ell = pm.Gamma(\"ell\", alpha=2, beta=0.5, shape=2)\n",
    "    eta = pm.Gamma(\"eta\", alpha=3, beta=1, shape=2)\n",
    "    \n",
    "    # Two kernels\n",
    "    kernel_list = [\n",
    "        eta[0]**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ell[0], active_dims=[0]),\n",
    "        eta[1]**2 * pm.gp.cov.Matern32(input_dim=2, ls=ell[1], active_dims=[0])\n",
    "    ]\n",
    "    \n",
    "    # LCM kernel\n",
    "    cov_lcm = get_lcm(input_dim=2, active_dims=[1], num_outputs=n_outputs,\n",
    "                      kernels=kernel_list, name=\"LCM\")\n",
    "    \n",
    "    # GP\n",
    "    gp = pm.gp.Marginal(cov_func=cov_lcm)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=3)\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=sigma)\n",
    "\n",
    "pm.model_to_graphviz(lcm_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210e236e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lcm_model:\n",
    "    trace_lcm = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        chains=2,\n",
    "        target_accept=0.95\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7501ee",
   "metadata": {},
   "source": [
    "LCM has more parameters (2 lengthscales, 2 amplitudes) but shared coregionalization. ,This flexibility captures both smooth trends and short-term wiggles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39c2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "with lcm_model:\n",
    "    f_pred_lcm = gp.conditional(\"f_pred\", X_test)\n",
    "    ppc_lcm = pm.sample_posterior_predictive(\n",
    "        trace_lcm,\n",
    "        var_names=[\"f_pred\"],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426bedfb",
   "metadata": {},
   "source": [
    "LCM predictions (similar visualization to ICM):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c46950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar plotting code - abbreviated for space\n",
    "print(\"LCM posterior predictive samples obtained\")\n",
    "print(f\"Shape: {ppc_lcm.posterior_predictive['f_pred'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38db6f30",
   "metadata": {},
   "source": [
    "### Model Comparison: ICM vs LCM\n",
    "\n",
    "Quantitative comparison with LOO:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f56b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo_icm = az.loo(trace_icm, pointwise=True)\n",
    "loo_lcm = az.loo(trace_lcm, pointwise=True)\n",
    "\n",
    "comparison = az.compare({\"ICM\": trace_icm, \"LCM\": trace_lcm}, ic=\"loo\")\n",
    "print(comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e686b8e",
   "metadata": {},
   "source": [
    "The `elpd_diff` shows difference in predictive accuracy. ,If substantially larger than SE, the better model is meaningfully improved.\n",
    "\n",
    "**Practical takeaway:** If similar, prefer ICM for simplicity. ,If LCM substantially better, added complexity is justified."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a80cab5a",
   "metadata": {},
   "source": [
    "### Computational Cost\n",
    "\n",
    "Summarize trade-offs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0f58f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "icm_time = trace_icm.sample_stats[\"sampling_time\"].values.sum()\n",
    "lcm_time = trace_lcm.sample_stats[\"sampling_time\"].values.sum()\n",
    "\n",
    "comp_df = pl.DataFrame({\n",
    "\"Model\": [\"ICM\", \"LCM\"],\n",
    "\"Sampling (s)\": [icm_time, lcm_time],\n",
    "\"LOO\": [loo_icm.elpd_loo, loo_lcm.elpd_loo]\n",
    "})\n",
    "print(comp_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6946119f",
   "metadata": {},
   "source": [
    "### Summary: Multi-Output GPs\n",
    "\n",
    "We've covered:\n",
    "\n",
    "1. **ARD**: Different lengthscales per input dimension for feature selection\n",
    "2. **ICM**: Model related outputs jointly with coregionalization matrix $B = WW^T + \\\\text{diag}(\\\\kappa)$\n",
    "3. **LCM**: Extend to multiple timescales by summing ICM kernels\n",
    "\n",
    "**When to use:**\n",
    "- **ARD**: Always for multidimensional inputs\n",
    "- **ICM**: Related outputs needing information sharing\n",
    "- **LCM**: Domain knowledge suggests multiple processes\n",
    "\n",
    "Next, we integrate these patterns with HSGP and hierarchical structure in our soccer case study."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba48a65e",
   "metadata": {},
   "source": [
    "### From Finance to FÃºtbol: Factor Models\n",
    "\n",
    "In finance, the Fama-French model asks: Is a fund manager skilled, or just lucky with market exposure?\n",
    "\n",
    "Similarly for soccer: Is a player elite, or do they benefit from strong teammates vs weak opponents?\n",
    "\n",
    "Our factor model:\n",
    "$$p_i = \\\\sigma(\\\\alpha_i + \\\\mathbf{X}_i \\\\boldsymbol{\\\\beta})$$\n",
    "\n",
    "Where:\n",
    "- $\\\\alpha_i$: Player skill (our main interest)\n",
    "- $\\\\mathbf{X}_i \\\\boldsymbol{\\\\beta}$: Team context effects\n",
    "- $\\\\sigma$: Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5b8783",
   "metadata": {},
   "source": [
    "### Factor Engineering\n",
    "\n",
    "We engineer 8 factors from pre-match data:\n",
    "- `goalsscored_diff`: Current goals difference\n",
    "- `points_diff`: Recent form\n",
    "- `goal_balance_diff`: Overall strength disparity\n",
    "- ... and 5 more\n",
    "\n",
    "For this demo, we focus on 3 that capture key dimensions:\n",
    "1. **`home_pitch`**: Home advantage\n",
    "2. **`points_diff`**: Recent form/momentum\n",
    "3. **`goal_balance_diff`**: Team strength disparity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part B: Coregionalization\n",
    "\n",
    "ICM uses Hadamard product:\n",
    "\n",
    "$$k([\\mathbf{x}, i], [\\mathbf{x}', j]) = k_{\\text{input}}(\\mathbf{x}, \\mathbf{x}') \\times k_{\\text{coreg}}(i, j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate 3 Related Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_times, n_outputs = 40, 3\n",
    "t = np.linspace(0, 10, n_times)\n",
    "f1 = np.sin(t) + RNG.normal(0, 0.2, n_times)\n",
    "f2 = np.sin(t) + 0.5*np.cos(2*t) + RNG.normal(0, 0.2, n_times)\n",
    "f3 = -np.cos(t) + RNG.normal(0, 0.2, n_times)\n",
    "\n",
    "X_mogp = np.column_stack([np.tile(t, n_outputs), np.repeat([0,1,2], n_times)])\n",
    "y_mogp = np.concatenate([f1, f2, f3])\n",
    "\n",
    "print(f\"Multi-output: {X_mogp.shape[0]} obs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Coregionalized GP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as mogp_model:\n",
    "    ls_time = pm.Gamma(\"ls_time\", alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    cov_time = eta**2 * pm.gp.cov.ExpQuad(input_dim=2, ls=ls_time, active_dims=[0])\n",
    "    \n",
    "    W = pm.Normal(\"W\", mu=0, sigma=1, shape=(n_outputs, 2))\n",
    "    kappa = pm.Gamma(\"kappa\", alpha=1.5, beta=1, shape=n_outputs)\n",
    "    B = pm.Deterministic(\"B\", pt.dot(W, W.T) + pt.diag(kappa))\n",
    "    cov_out = pm.gp.cov.Coregion(input_dim=2, W=W, kappa=kappa, active_dims=[1])\n",
    "    \n",
    "    cov_total = cov_time * cov_out\n",
    "    gp = pm.gp.Marginal(cov_func=cov_total)\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_mogp, y=y_mogp, sigma=sigma)\n",
    "    trace_mogp = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learned Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B_post = az.extract(trace_mogp, var_names=[\"B\"]).mean(dim=\"sample\").values\n",
    "fig = go.Figure(data=go.Heatmap(z=B_post, colorscale='RdBu', zmid=0))\n",
    "fig.update_layout(title=\"Output Covariance B\", height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outputs 1-2: high covariance â†’ correlated  \n",
    "Output 3: lower covariance â†’ independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part C: Soccer Player Skill Modeling\n",
    "\n",
    "### Challenge: Identify True Skill\n",
    "\n",
    "Account for team strength, opponent quality, context, varying sample sizes.\n",
    "\n",
    "### Hierarchical Logistic Regression\n",
    "\n",
    "$$P(\\text{goal}_{ij} = 1) = \\text{logit}^{-1}(\\alpha_i + \\mathbf{X}_{ij}^T\\boldsymbol{\\beta})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(\"../data/SFM_data_byPlayer_clean.csv\")\n",
    "n_players = df.select(pl.col(\"name_player\")).unique().height\n",
    "goal_rate = df.select(pl.col(\"goal\").mean()).item()\n",
    "\n",
    "print(f\"{df.shape[0]} observations, {n_players} players\")\n",
    "print(f\"Goal rate: {goal_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors: Context Variables\n",
    "\n",
    "1. home_pitch: Home advantage\n",
    "2. points_diff: Recent form\n",
    "3. goal_balance_diff: Team vs opponent strength"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Factor Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors = [\"home_pitch\", \"points_diff\", \"goal_balance_diff\"]\n",
    "fig = make_subplots(rows=1, cols=3, subplot_titles=factors)\n",
    "\n",
    "for i, factor in enumerate(factors, 1):\n",
    "    binned = (df.with_columns([\n",
    "        pl.col(factor).cut(breaks=[-np.inf,-1,0,1,np.inf], \n",
    "                          labels=[\"Low\",\"Mid-Low\",\"Mid-High\",\"High\"]).alias(\"bin\")\n",
    "    ]).group_by(\"bin\").agg([\n",
    "        pl.col(\"goal\").mean().alias(\"rate\")\n",
    "    ]).sort(\"bin\"))\n",
    "    \n",
    "    fig.add_trace(go.Bar(x=binned[\"bin\"].to_list(), \n",
    "                        y=binned[\"rate\"].to_list()), row=1, col=i)\n",
    "\n",
    "fig.update_layout(title=\"Goal Rate by Factor\", showlegend=False, height=350)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factor_cols = [\"home_pitch\", \"points_diff\", \"goal_balance_diff\"]\n",
    "X_factors = df.select(factor_cols).to_numpy().astype(np.float64)\n",
    "y_goals = df.select(\"goal\").to_numpy().flatten().astype(int)\n",
    "\n",
    "player_names = df.select(\"name_player\").unique().sort(\"name_player\")[\"name_player\"].to_list()\n",
    "player_idx_map = {name: i for i, name in enumerate(player_names)}\n",
    "player_idx = df.select(pl.col(\"name_player\").replace(player_idx_map)).to_numpy().flatten()\n",
    "\n",
    "n_players, n_factors = len(player_names), len(factor_cols)\n",
    "print(f\"{len(y_goals)} obs, {n_players} players, {n_factors} factors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Hierarchical Model\n",
    "\n",
    "Partial pooling: data-scarce players regularized toward population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(p): \n",
    "    return np.log(p / (1 - p))\n",
    "\n",
    "with pm.Model() as sfm_model:\n",
    "    mu_alpha = pm.Normal(\"mu_alpha\", mu=logit(goal_rate), sigma=1)\n",
    "    sigma_alpha = pm.HalfNormal(\"sigma_alpha\", sigma=0.5)\n",
    "    alpha = pm.Normal(\"alpha\", mu=mu_alpha, sigma=sigma_alpha, shape=n_players)\n",
    "    \n",
    "    beta = pm.Normal(\"beta\", mu=0, sigma=1, shape=n_factors)\n",
    "    \n",
    "    eta = alpha[player_idx] + pm.math.dot(X_factors, beta)\n",
    "    \n",
    "    y_obs = pm.Bernoulli(\"y_obs\", logit_p=eta, observed=y_goals)\n",
    "    \n",
    "    trace_sfm = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, \n",
    "                         target_accept=0.9, chains=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = az.summary(trace_sfm, var_names=[\"alpha\",\"beta\",\"mu_alpha\",\"sigma_alpha\"])\n",
    "print(f\"R-hat: [{summary['r_hat'].min():.4f}, {summary['r_hat'].max():.4f}]\")\n",
    "print(f\"ESS: [{summary['ess_bulk'].min():.0f}, {summary['ess_bulk'].max():.0f}]\")\n",
    "print(\"\\nFactor coefficients:\")\n",
    "print(summary.filter(like=\"beta\", axis=0)[[\"mean\",\"sd\",\"hdi_3%\",\"hdi_97%\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Player Skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_post = az.extract(trace_sfm, var_names=[\"alpha\"])\n",
    "alpha_means = alpha_post.mean(dim=\"sample\").values\n",
    "\n",
    "results = pl.DataFrame({\n",
    "    \"player\": player_names,\n",
    "    \"skill_mean\": alpha_means,\n",
    "    \"skill_lower\": np.percentile(alpha_post.values, 2.5, axis=1),\n",
    "    \"skill_upper\": np.percentile(alpha_post.values, 97.5, axis=1)\n",
    "}).sort(\"skill_mean\", descending=True)\n",
    "\n",
    "print(\"Top 10:\")\n",
    "print(results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize with Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sorted = results.sort(\"skill_mean\", descending=False)\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    y=results_sorted[\"player\"].to_list(),\n",
    "    x=results_sorted[\"skill_mean\"].to_list(),\n",
    "    error_x=dict(type='data', symmetric=False,\n",
    "                array=(results_sorted[\"skill_upper\"]-results_sorted[\"skill_mean\"]).to_list(),\n",
    "                arrayminus=(results_sorted[\"skill_mean\"]-results_sorted[\"skill_lower\"]).to_list()),\n",
    "    mode='markers', marker=dict(size=8, color='steelblue')\n",
    "))\n",
    "fig.add_vline(x=0, line_dash=\"dash\", annotation_text=\"Avg\")\n",
    "fig.update_layout(title=\"Player Skills (Î±)\", xaxis_title=\"Skill\", height=700)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "- Skill hierarchy: Top players consistently better\n",
    "- Uncertainty varies: More data â†’ narrower intervals\n",
    "- Overlap matters: Can't confidently rank when intervals overlap\n",
    "- Context-adjusted: Fair comparison across situations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beta_post = az.extract(trace_sfm, var_names=[\"beta\"])\n",
    "factor_res = pl.DataFrame({\n",
    "    \"factor\": factor_cols,\n",
    "    \"coef\": beta_post.mean(dim=\"sample\").values,\n",
    "    \"lower\": np.percentile(beta_post.values, 2.5, axis=1),\n",
    "    \"upper\": np.percentile(beta_post.values, 97.5, axis=1)\n",
    "})\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=factor_res[\"coef\"].to_list(), \n",
    "                        y=factor_res[\"factor\"].to_list(),\n",
    "                        error_x=dict(type='data', symmetric=False,\n",
    "                                    array=(factor_res[\"upper\"]-factor_res[\"coef\"]).to_list(),\n",
    "                                    arrayminus=(factor_res[\"coef\"]-factor_res[\"lower\"]).to_list()),\n",
    "                        mode='markers', marker=dict(size=12, color='coral')))\n",
    "fig.add_vline(x=0, line_dash=\"dash\")\n",
    "fig.update_layout(title=\"Factor Effects (Î²)\", xaxis_title=\"Coefficient\", height=300)\n",
    "fig.show()\n",
    "\n",
    "print(factor_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "goal_balance_diff: strongest (team quality matters!)  \n",
    "home_pitch: positive home advantage  \n",
    "points_diff: form correlates with scoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– LLM Exercise\n",
    "\n",
    "Extend the model with temporal dynamics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Add time-varying skills\n",
    "\n",
    "def extend_with_hsgp():\n",
    "    \"\"\"\n",
    "    Add HSGP for player skills over seasons.\n",
    "    \n",
    "    Prompt: \"I have hierarchical logistic regression (Bernoulli, \n",
    "    alpha player effects, beta factors). Make alpha_i vary over \n",
    "    seasons with HSGP. Help me: 1) Define HSGP over seasons,  \n",
    "    2) Integrate with model, 3) Update predictor. PyMC code.\"\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "print(\"ðŸŽ¯ Extend SFM with time-varying HSGP skills\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extensions\n",
    "\n",
    "1. Temporal dynamics: Aging curves, form  \n",
    "2. More factors: Defensive rating, rest, injuries\n",
    "3. Multi-level: Group by position\n",
    "4. Predictive checks: Simulate vs holdout\n",
    "5. Decision-making: Transfer value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Workshop Summary\n",
    "\n",
    "### Sessions 1-4 Journey\n",
    "\n",
    "**Session 1**: Foundations (Bayesian inference, MVNâ†’GP, kernels)  \n",
    "**Session 2**: Model Building (kernel composition, likelihoods)  \n",
    "**Session 3**: Scaling (O(nÂ³), sparse, HSGP)  \n",
    "**Session 4**: Applications (multi-output, hierarchical, real case study)\n",
    "\n",
    "### GP Mindset\n",
    "\n",
    "1. Flexibility: Adapt to data\n",
    "2. Uncertainty: Full posteriors\n",
    "3. Interpretability: Clear meanings\n",
    "4. Composability: Complex from simple\n",
    "5. Scalability: Modern approximations\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "- PyMC docs: https://www.pymc.io/\n",
    "- Rasmussen & Williams book (free online)\n",
    "- PyMC examples and Discourse\n",
    "- Apply to your data!\n",
    "\n",
    "**Final thought**: GPs encode smoothness assumptions, let data speak. You're equipped for real-world problems!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgments\n",
    "\n",
    "PyMC team, Alex Andorra & Max Goebel (soccer case), Danh Phan, Bill Engels, Chris Fonnesbeck (multi-output GPs).\n",
    "\n",
    "Materials for educational use under open-source licenses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11edc858",
   "metadata": {},
   "source": [
    "### Temporal Complexity: Three Timescales\n",
    "\n",
    "Player scoring ability varies across multiple timescales:\n",
    "\n",
    "1. **Match-to-match (2-5 matchdays)**: Injury recovery, tactical changes, streaks\n",
    "2. **Within-season form (15-25 matchdays)**: Confidence, fitness, team chemistry\n",
    "3. **Career trajectory (2-6 seasons)**: Aging curve, experience, skill development/decline\n",
    "\n",
    "We model all three simultaneously using **additive HSGP components**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655c90c7",
   "metadata": {},
   "source": [
    "### HSGP Hyperparameter Selection\n",
    "\n",
    "For each timescale, we choose `m` (basis functions) and `c` (boundary extension).\n",
    "\n",
    "Key logic:\n",
    "- Smaller lengthscales require larger `m` (more basis functions)\n",
    "- Longer input ranges require larger `c` (avoid boundary artifacts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe204b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check preliz availability for maximum entropy priors\n",
    "try:\n",
    "    import preliz as pz\n",
    "    HAVE_PRELIZ = True\n",
    "    print(\"âœ“ preliz available\")\n",
    "except ImportError:\n",
    "    HAVE_PRELIZ = False\n",
    "    print(\"preliz not available - will use manual priors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1b204b",
   "metadata": {},
   "source": [
    "We use **maximum entropy priors** to encode domain knowledge about lengthscales. ,Specify bounds, and preliz finds the distribution maximizing entropy subject to constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1568d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAVE_PRELIZ:\n",
    "    # Short-term: 2-5 matchdays\n",
    "    ls_short_dist, _ = pz.maxent(pz.InverseGamma(), 2, 5)\n",
    "    print(f\"Short: InverseGamma(Î±={ls_short_dist.alpha:.2f}, Î²={ls_short_dist.beta:.2f})\")\n",
    "    \n",
    "    # Medium-term: 15-25 matchdays\n",
    "    ls_medium_dist, _ = pz.maxent(pz.InverseGamma(), 15, 25)\n",
    "    print(f\"Medium: InverseGamma(Î±={ls_medium_dist.alpha:.2f}, Î²={ls_medium_dist.beta:.2f})\")\n",
    "    \n",
    "    # Long-term: 2-6 seasons\n",
    "    ls_long_dist, _ = pz.maxent(pz.InverseGamma(), 2, 6)\n",
    "    print(f\"Long: InverseGamma(Î±={ls_long_dist.alpha:.2f}, Î²={ls_long_dist.beta:.2f})\")\n",
    "    \n",
    "    # Visualize\n",
    "    fig = make_subplots(rows=1, cols=3, subplot_titles=[\"Short\", \"Medium\", \"Long\"])\n",
    "    x_short = np.linspace(0.1, 10, 100)\n",
    "    x_medium = np.linspace(5, 35, 100)\n",
    "    x_long = np.linspace(0.5, 10, 100)\n",
    "    \n",
    "    fig.add_trace(go.Scatter(x=x_short, y=stats.invgamma.pdf(x_short, ls_short_dist.alpha, scale=ls_short_dist.beta), mode='lines'), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=x_medium, y=stats.invgamma.pdf(x_medium, ls_medium_dist.alpha, scale=ls_medium_dist.beta), mode='lines'), row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=x_long, y=stats.invgamma.pdf(x_long, ls_long_dist.alpha, scale=ls_long_dist.beta), mode='lines'), row=1, col=3)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Lengthscale\", row=1, col=1)\n",
    "    fig.update_xaxes(title_text=\"Lengthscale\", row=1, col=2)\n",
    "    fig.update_xaxes(title_text=\"Lengthscale\", row=1, col=3)\n",
    "    fig.update_layout(title=\"Lengthscale Priors for Three Timescales\", showlegend=False, height=300)\n",
    "    fig.show()\n",
    "else:\n",
    "    print(\"Using manual priors\")\n",
    "    ls_short_alpha, ls_short_beta = 3.0, 9.0\n",
    "    ls_medium_alpha, ls_medium_beta = 3.0, 60.0\n",
    "    ls_long_alpha, ls_long_beta = 3.0, 12.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502c8b6c",
   "metadata": {},
   "source": [
    "Now use PyMC's helper to determine `m` and `c`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53f396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Within-season (matchdays 1-38)\n",
    "m_within, c_within = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "x_range=[0, 38],\n",
    "lengthscale_range=[5, 25],  # medium timescale\n",
    "cov_func=\"matern52\"\n",
    ")\n",
    "print(f\"Within-season: m={m_within}, c={c_within:.2f}\")\n",
    "\n",
    "# Across-season\n",
    "max_season = df.select(pl.col(\"season_id\").max()).item()\n",
    "m_long, c_long = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "x_range=[0, max_season],\n",
    "lengthscale_range=[2, 6],\n",
    "cov_func=\"matern52\"\n",
    ")\n",
    "print(f\"Across-season: m={m_long}, c={c_long:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fd2a757",
   "metadata": {},
   "source": [
    "Larger `m` accommodates smaller lengthscales (more flexibility). ,Larger `c` extends boundary for longer lengthscales (reduces edge effects)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a43fc330",
   "metadata": {},
   "source": [
    "### Building the Full Hierarchical Model\n",
    "\n",
    "Components:\n",
    "1. Hierarchical player intercepts (partial pooling)\n",
    "2. Within-season GP (short + medium timescales)\n",
    "3. Across-season GP (long-term aging curve)\n",
    "4. Factor regression (team context)\n",
    "5. Bernoulli likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884aad30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare coordinates\n",
    "players_ordered = df.select(\"name_player\").unique().sort(\"name_player\")[\"name_player\"].to_list()\n",
    "unique_seasons = sorted(df.select(\"season_id\").unique()[\"season_id\"].to_list())\n",
    "unique_gamedays = list(range(1, 39))\n",
    "\n",
    "coords = {\n",
    "\"player\": players_ordered,\n",
    "\"season\": unique_seasons,\n",
    "\"gameday\": unique_gamedays,\n",
    "\"factor\": factors,\n",
    "\"timescale\": [\"short\", \"medium\", \"long\"],\n",
    "\"obs_id\": df.select(\"index\")[\"index\"].to_list()\n",
    "}\n",
    "\n",
    "print(f\"{len(coords['player'])} players, {len(coords['season'])} seasons, {len(coords['gameday'])} matchdays\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a435363",
   "metadata": {},
   "source": [
    "Coordinates enable:\n",
    "1. Semantic naming (easier to understand)\n",
    "2. ArviZ plotting and slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae4d07",
   "metadata": {},
   "source": [
    "### Complete PyMC Model with HSGPs\n",
    "\n",
    "Now we assemble all components. This is complex, so we build step-by-step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295fb894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "player_idx = pd.Categorical(df[\"name_player\"], categories=players_ordered).codes\n",
    "gameday_idx = pd.Categorical(df[\"matchday\"], categories=unique_gamedays).codes\n",
    "\n",
    "with pm.Model(coords=coords) as enhanced_sfm:\n",
    "    # Data containers\n",
    "    factor_data = pm.Data(\"factor_data\", factors_sdz.to_numpy(), dims=(\"obs_id\", \"factor\"))\n",
    "    gameday_id = pm.Data(\"gameday_id\", gameday_idx, dims=\"obs_id\")\n",
    "    player_id = pm.Data(\"player_id\", player_idx, dims=\"obs_id\")\n",
    "    season_id = pm.Data(\"season_id\", df[\"season_id\"].to_numpy(), dims=\"obs_id\")\n",
    "    goals_obs = pm.Data(\"goals_obs\", df[\"goal\"].to_numpy(), dims=\"obs_id\")\n",
    "\n",
    "print(\"âœ“ Data containers defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    # Hierarchical player effects\n",
    "    if HAVE_PRELIZ:\n",
    "        player_diversity_dist, _ = pz.maxent(pz.Exponential(), 0.1, 2)\n",
    "        sigma_player = player_diversity_dist.to_pymc(name=\"player_diversity\")\n",
    "    else:\n",
    "        sigma_player = pm.Exponential(\"player_diversity\", lam=1.0)\n",
    "    \n",
    "    from scipy.special import logit\n",
    "    player_effect = pm.Normal(\n",
    "        \"player_effect\",\n",
    "        mu=logit(df[\"goal\"].mean()),\n",
    "        sigma=sigma_player,\n",
    "        dims=\"player\"\n",
    "    )\n",
    "\n",
    "print(\"âœ“ Player effects defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10c91f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    X_gamedays = pm.Data(\"X_gamedays\", np.array(unique_gamedays)[:, None], dims=\"gameday\")\n",
    "    X_seasons = pm.Data(\"X_seasons\", np.array(unique_seasons)[:, None], dims=\"season\")\n",
    "    \n",
    "    # PC prior on amplitude\n",
    "    alpha_scale, upper_scale = 0.01, 1.1\n",
    "    amplitude = pm.Exponential(\n",
    "        \"amplitude\",\n",
    "        lam=-np.log(alpha_scale) / upper_scale,\n",
    "        dims=\"timescale\"\n",
    "    )\n",
    "    \n",
    "    if HAVE_PRELIZ:\n",
    "        ls = pm.InverseGamma(\n",
    "            \"ls\",\n",
    "            alpha=np.array([ls_short_dist.alpha, ls_medium_dist.alpha, ls_long_dist.alpha]),\n",
    "            beta=np.array([ls_short_dist.beta, ls_medium_dist.beta, ls_long_dist.beta]),\n",
    "            dims=\"timescale\"\n",
    "        )\n",
    "    else:\n",
    "        ls = pm.InverseGamma(\n",
    "            \"ls\",\n",
    "            alpha=np.array([ls_short_alpha, ls_medium_alpha, ls_long_alpha]),\n",
    "            beta=np.array([ls_short_beta, ls_medium_beta, ls_long_beta]),\n",
    "            dims=\"timescale\"\n",
    "        )\n",
    "    \n",
    "    # Covariances\n",
    "    cov_short = amplitude[0]**2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[0])\n",
    "    cov_medium = amplitude[1]**2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[1])\n",
    "    cov_within = cov_short + cov_medium\n",
    "    cov_long = amplitude[2]**2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[2])\n",
    "    \n",
    "    # Within-season GP\n",
    "    gp_within = pm.gp.HSGP(m=[m_within], c=c_within, cov_func=cov_within, drop_first=True)\n",
    "    f_within = gp_within.prior(\"f_within\", X=X_gamedays, dims=\"gameday\")\n",
    "    \n",
    "    # Across-season GP\n",
    "    gp_long = pm.gp.HSGP(m=[m_long], c=c_long, cov_func=cov_long, drop_first=True)\n",
    "    f_long = gp_long.prior(\"f_long\", X=X_seasons, dims=\"season\")\n",
    "\n",
    "print(\"âœ“ HSGPs defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc1366d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    # Combine effects\n",
    "    alpha = pm.Deterministic(\n",
    "        \"alpha\",\n",
    "        player_effect[player_id] + f_within[gameday_id] + f_long[season_id],\n",
    "        dims=\"obs_id\"\n",
    "    )\n",
    "    \n",
    "    # Factor slopes\n",
    "    slope = pm.Normal(\"slope\", sigma=0.25, dims=\"factor\")\n",
    "    \n",
    "    # Probability\n",
    "    p = pm.Deterministic(\n",
    "        \"p\",\n",
    "        pm.math.sigmoid(alpha + pm.math.dot(factor_data, slope)),\n",
    "        dims=\"obs_id\"\n",
    "    )\n",
    "    \n",
    "    # Likelihood\n",
    "    pm.Bernoulli(\"goals_scored\", p=p, observed=goals_obs, dims=\"obs_id\")\n",
    "\n",
    "print(\"âœ“ Complete model defined\")\n",
    "pm.model_to_graphviz(enhanced_sfm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57dd494",
   "metadata": {},
   "source": [
    "### Prior Predictive Checks\n",
    "\n",
    "Sample from the prior to verify it's reasonable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac809cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    idata_enhanced = pm.sample_prior_predictive(random_seed=RNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1147430",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "prior_p = idata_enhanced.prior.p.values.flatten()\n",
    "fig.add_trace(go.Histogram(x=prior_p, nbinsx=50, name=\"Prior\"))\n",
    "fig.update_layout(\n",
    "title=\"Prior Scoring Rate Distribution\",\n",
    "xaxis_title=\"Probability\",\n",
    "yaxis_title=\"Count\",\n",
    "height=350\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Prior mean: {prior_p.mean():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bccdb39",
   "metadata": {},
   "source": [
    "### Sampling the Posterior\n",
    "\n",
    "This will take several minutes despite using Nutpie for faster sampling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73d6ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    idata_enhanced.extend(\n",
    "        pm.sample(\n",
    "            nuts_sampler=\"nutpie\",\n",
    "            random_seed=RNG,\n",
    "            target_accept=0.95\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa42511",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d492f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ESS quantiles\n",
    "ess = az.ess(idata_enhanced.posterior)\n",
    "ess_summary = ess.quantile([0.01, 0.5, 0.99]).to_dataframe().astype(int)\n",
    "print(\"ESS quantiles:\")\n",
    "print(ess_summary)\n",
    "\n",
    "# R-hat\n",
    "rhat = az.rhat(idata_enhanced.posterior)\n",
    "rhat_summary = rhat.quantile([0.01, 0.5, 0.99]).to_dataframe()\n",
    "print(\"\\nR-hat quantiles:\")\n",
    "print(rhat_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1bbf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Energy plot (convert to plotly)\n",
    "import matplotlib.pyplot as plt\n",
    "az.plot_energy(idata_enhanced)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a89a5d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with enhanced_sfm:\n",
    "    idata_enhanced.extend(\n",
    "        pm.sample_posterior_predictive(\n",
    "            idata_enhanced,\n",
    "            random_seed=RNG\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b98026d",
   "metadata": {},
   "source": [
    "### Posterior Predictive Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7105a22a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overall goal rate\n",
    "fig = go.Figure()\n",
    "\n",
    "# Observed\n",
    "obs_rate = df[\"goal\"].mean()\n",
    "fig.add_vline(x=obs_rate, line_dash=\"dash\", line_color=\"red\",\n",
    "annotation_text=\"Observed\", annotation_position=\"top\")\n",
    "\n",
    "# Posterior predictive\n",
    "ppc_goals = idata_enhanced.posterior_predictive[\"goals_scored\"].values\n",
    "ppc_rate = ppc_goals.mean(axis=(0,1,2))\n",
    "fig.add_trace(go.Histogram(x=ppc_goals.mean(axis=2).flatten(), nbinsx=50,\n",
    "name=\"Posterior Predictive\"))\n",
    "\n",
    "fig.update_layout(\n",
    "title=\"Goal Rate: Observed vs Posterior Predictive\",\n",
    "xaxis_title=\"Mean Goal Rate\",\n",
    "height=350\n",
    ")\n",
    "fig.show()\n",
    "print(f\"Observed: {obs_rate:.3f}\")\n",
    "print(f\"Posterior predictive: {ppc_rate:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b32a3e",
   "metadata": {},
   "source": [
    "### Posterior GP Curves\n",
    "\n",
    "Visualize learned temporal patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c0d257c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GP posteriors\n",
    "f_within_post = idata_enhanced.posterior[\"f_within\"]\n",
    "f_long_post = idata_enhanced.posterior[\"f_long\"]\n",
    "\n",
    "# Plot\n",
    "fig = make_subplots(\n",
    "rows=1, cols=2,\n",
    "subplot_titles=[\"Within-Season Variation\", \"Across-Season Variation\"]\n",
    ")\n",
    "\n",
    "# Within-season\n",
    "f_within_mean = f_within_post.mean(dim=[\"chain\", \"draw\"]).values\n",
    "f_within_lower = np.percentile(f_within_post.values, 2.5, axis=(0,1))\n",
    "f_within_upper = np.percentile(f_within_post.values, 97.5, axis=(0,1))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_gamedays, y=f_within_upper,\n",
    "line=dict(width=0), showlegend=False\n",
    "), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_gamedays, y=f_within_lower,\n",
    "fill='tonexty', line=dict(width=0),\n",
    "fillcolor='rgba(0,100,250,0.2)', showlegend=False\n",
    "), row=1, col=1)\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_gamedays, y=f_within_mean,\n",
    "mode='lines', line=dict(color='steelblue', width=2),\n",
    "showlegend=False\n",
    "), row=1, col=1)\n",
    "\n",
    "# Across-season\n",
    "f_long_mean = f_long_post.mean(dim=[\"chain\", \"draw\"]).values\n",
    "f_long_lower = np.percentile(f_long_post.values, 2.5, axis=(0,1))\n",
    "f_long_upper = np.percentile(f_long_post.values, 97.5, axis=(0,1))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_seasons, y=f_long_upper,\n",
    "line=dict(width=0), showlegend=False\n",
    "), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_seasons, y=f_long_lower,\n",
    "fill='tonexty', line=dict(width=0),\n",
    "fillcolor='rgba(0,100,250,0.2)', showlegend=False\n",
    "), row=1, col=2)\n",
    "fig.add_trace(go.Scatter(\n",
    "x=unique_seasons, y=f_long_mean,\n",
    "mode='lines', line=dict(color='steelblue', width=2),\n",
    "showlegend=False\n",
    "), row=1, col=2)\n",
    "\n",
    "fig.update_xaxes(title_text=\"Matchday\", row=1, col=1)\n",
    "fig.update_xaxes(title_text=\"Season\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"Goal Effect\", row=1, col=1)\n",
    "fig.update_layout(title=\"Posterior GP Effects\", height=400)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eee988c",
   "metadata": {},
   "source": [
    "The within-season GP shows form fluctuation across matchdays. ,The across-season GP shows the aging curve (young players improving, veterans declining)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884d9f61",
   "metadata": {},
   "source": [
    "### Enhanced Results\n",
    "\n",
    "Factor effects with the full temporal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6341b213",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Factor slopes\n",
    "slope_post = az.extract(idata_enhanced.posterior, var_names=[\"slope\"])\n",
    "slope_summary = slope_post.mean(dim=\"sample\").values\n",
    "\n",
    "factor_results = pl.DataFrame({\n",
    "\"factor\": factors,\n",
    "\"mean\": slope_summary,\n",
    "\"lower\": np.percentile(slope_post.values, 2.5, axis=1),\n",
    "\"upper\": np.percentile(slope_post.values, 97.5, axis=1)\n",
    "})\n",
    "\n",
    "# Plot\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "x=factor_results[\"mean\"].to_list(),\n",
    "y=factor_results[\"factor\"].to_list(),\n",
    "error_x=dict(\n",
    "type='data',\n",
    "symmetric=False,\n",
    "array=(factor_results[\"upper\"] - factor_results[\"mean\"]).to_list(),\n",
    "arrayminus=(factor_results[\"mean\"] - factor_results[\"lower\"]).to_list()\n",
    "),\n",
    "mode='markers',\n",
    "marker=dict(size=12, color='coral')\n",
    "))\n",
    "fig.add_vline(x=0, line_dash=\"dash\")\n",
    "fig.update_layout(\n",
    "title=\"Factor Effects (Enhanced Model)\",\n",
    "xaxis_title=\"Coefficient\",\n",
    "height=300\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(factor_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffbbeb",
   "metadata": {},
   "source": [
    "### Summary: Complete Session 4\n",
    "\n",
    "We've completed a comprehensive journey:\n",
    "\n",
    "**Part A: Multi-Output GPs**\n",
    "- ARD for automatic feature selection\n",
    "- ICM for modeling correlated outputs\n",
    "- LCM for multiple timescales\n",
    "- Real baseball data with 5 pitchers\n",
    "\n",
    "**Part B: Advanced Case Study**\n",
    "- Hierarchical logistic regression\n",
    "- Factor model for skill attribution\n",
    "- HSGP for scalability\n",
    "- Three timescales (short/medium/long)\n",
    "- Maximum entropy priors\n",
    "- Comprehensive diagnostics\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Multi-output GPs enable information sharing across related outputs\n",
    "2. HSGP makes GPs practical for real datasets\n",
    "3. Hierarchical structure provides partial pooling\n",
    "4. Temporal modeling captures evolution of player skills\n",
    "5. Factor models decompose skill from context\n",
    "\n",
    "You now have the tools to apply sophisticated GP models to your own problems!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
