{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 3: Scaling with HSGP and Sparse Methods\n",
    "\n",
    "**Duration:** 2â€“3 hours\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Understand the computational bottlenecks of standard Gaussian processes\n",
    "- Apply inducing point methods and sparse approximations to scale GPs to larger datasets\n",
    "- Implement Hilbert Space GP (HSGP) approximations in PyMC\n",
    "- Choose appropriate approximation parameters using helper functions and heuristics\n",
    "- Navigate the trade-offs between approximation fidelity and computational efficiency\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sessions, we explored the foundations of Gaussian processes and built models with various kernels and likelihoods. However, you may have noticed that as datasets grow larger, GP computations become increasingly expensive. The standard GP formulation requires inverting an $n \\times n$ covariance matrix, where $n$ is the number of data points. This operation has $\\mathcal{O}(n^3)$ computational complexity and $\\mathcal{O}(n^2)$ memory requirementsâ€”quickly becoming prohibitive for datasets with thousands of observations.\n",
    "\n",
    "In this session, we'll explore two powerful approaches to overcome these computational barriers: sparse GP approximations using inducing points, and the Hilbert Space GP (HSGP) method. These techniques allow us to apply GP models to much larger datasets while maintaining the flexibility and uncertainty quantification that make GPs so valuable.\n",
    "\n",
    "Think of these methods as strategic compromises: we trade away some exactness in our GP representation to gain massive improvements in speed and scalability. The key question we'll answer throughout this session is: *how do we make this trade-off intelligently?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Let's begin by importing our standard libraries and setting up our environment. We'll use the same stack as in previous sessions: PyMC for modeling, Polars for data manipulation, and Plotly for interactive visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Print versions for reproducibility\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Understanding the Computational Challenge\n",
    "\n",
    "Before diving into solutions, let's develop intuition for *why* standard GPs become computationally expensive. The bottleneck lies in computing and inverting the covariance matrix.\n",
    "\n",
    "For a GP with $n$ observations, we need to:\n",
    "\n",
    "1. **Compute** the $n \\times n$ covariance matrix $K$ by evaluating the kernel function at all pairs of data points\n",
    "2. **Invert** this matrix (or equivalently, solve a linear system) to compute the marginal likelihood\n",
    "3. **Repeat** these operations at every step during MCMC sampling as hyperparameters change\n",
    "\n",
    "The matrix inversion step dominates the computational cost, scaling as $\\mathcal{O}(n^3)$. This cubic scaling means that doubling your dataset size increases computation time by roughly 8Ã—. For a dataset with 10,000 points, a full GP could take hours or days to fit, making interactive model development essentially impossible.\n",
    "\n",
    "Let's visualize this by looking at the structure of covariance matrices for different dataset sizes. We'll use a simple squared exponential kernel and observe how the matrices grow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_covariance_matrices():\n",
    "    \"\"\"Visualize how covariance matrix size grows with data.\"\"\"\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=['n=50', 'n=200', 'n=1000'],\n",
    "        horizontal_spacing=0.1\n",
    "    )\n",
    "    \n",
    "    sizes = [50, 200, 1000]\n",
    "    \n",
    "    for idx, n in enumerate(sizes, 1):\n",
    "        # Generate data\n",
    "        x = np.linspace(0, 10, n)[:, None]\n",
    "        \n",
    "        # Create covariance matrix\n",
    "        cov_func = pm.gp.cov.ExpQuad(1, ls=1.0)\n",
    "        K = cov_func(x).eval()\n",
    "        \n",
    "        # Add to subplot\n",
    "        fig.add_trace(\n",
    "            go.Heatmap(\n",
    "                z=K,\n",
    "                colorscale='Viridis',\n",
    "                showscale=(idx == 3),\n",
    "                colorbar=dict(title=\"Covariance\")\n",
    "            ),\n",
    "            row=1, col=idx\n",
    "        )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Data point index\")\n",
    "    fig.update_yaxes(title_text=\"Data point index\", row=1, col=1)\n",
    "    fig.update_layout(\n",
    "        title_text=\"Covariance Matrix Size Grows Quadratically with Data\",\n",
    "        height=400,\n",
    "        showlegend=False\n",
    "    )\n",
    "    \n",
    "    return fig\n",
    "\n",
    "fig = visualize_covariance_matrices()\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Visualization\n",
    "\n",
    "Look at how the covariance matrices expand as we increase $n$ from 50 to 1000. Each pixel represents a covariance calculation between two data points. The bright diagonal shows that each point is perfectly correlated with itself (covariance = 1). The off-diagonal elements show how points are correlated with each other, with the correlation decaying as points become more distant.\n",
    "\n",
    "The key insight: **the number of elements grows quadratically** ($n^2$), but the computational cost of inverting this matrix grows **cubically** ($n^3$). When $n=1000$, we're working with a million-element matrix that requires a billion operations to invertâ€”and we need to do this at every MCMC step!\n",
    "\n",
    "This is where approximation methods become essential. Rather than abandoning GPs for large datasets, we can use clever mathematical tricks to reduce computational complexity while retaining most of the modeling flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.2: Sparse GP Approximations with Inducing Points\n",
    "\n",
    "The first approach to scaling GPs is the **sparse** or **inducing point** method. The central idea is elegant: instead of computing correlations between all $n$ data points, we select a smaller set of $m < n$ \"inducing points\" (also called \"pseudo-inputs\") that act as summary locations. These inducing points capture the essential structure of the function, allowing us to approximate the full GP at a fraction of the computational cost.\n",
    "\n",
    "Think of inducing points like strategic observation posts. If you're trying to understand temperature variations across a city, you don't need thermometers at every houseâ€”carefully placed weather stations at key locations can give you an excellent picture of the temperature field everywhere.\n",
    "\n",
    "Mathematically, the approximation reduces complexity from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(nm^2)$. When $m \\ll n$, this is a massive speedup. The trade-off is that our approximation quality depends on how well the $m$ inducing points can represent the underlying function.\n",
    "\n",
    "PyMC implements three variants of sparse GPs:\n",
    "\n",
    "- **DTC (Deterministic Training Conditional)**: The simplest approach, which can underestimate uncertainty\n",
    "- **FITC (Fully Independent Training Conditional)**: Adds back point-specific noise variance, improving uncertainty estimates\n",
    "- **VFE (Variational Free Energy)**: A variational approach that can optimize inducing point locations\n",
    "\n",
    "For most practical applications, FITC provides a good balance between accuracy and simplicity. Let's see it in action with a moderately large dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data for Sparse GP Demonstration\n",
    "\n",
    "We'll create a dataset with 2000 observationsâ€”large enough to make standard GP inference slow, but small enough to allow us to compare against the exact solution. Our data will be drawn from a GP with a MatÃ©rn 5/2 kernel and moderate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for data generation\n",
    "n = 2000\n",
    "ell_true = 1.0\n",
    "eta_true = 3.0\n",
    "sigma_true = 0.5\n",
    "\n",
    "# Generate input locations\n",
    "x = 10 * np.sort(rng.random(n))\n",
    "\n",
    "# Define true covariance function\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ell_true)\n",
    "\n",
    "# Sample the latent GP function\n",
    "K = cov_func(x[:, None]).eval()\n",
    "K_stable = K + 1e-8 * np.eye(n)  # Add jitter for numerical stability\n",
    "f_true = rng.multivariate_normal(np.zeros(n), K_stable)\n",
    "\n",
    "# Add observation noise\n",
    "y = f_true + sigma_true * rng.standard_normal(n)\n",
    "\n",
    "# Create a Polars DataFrame\n",
    "df = pl.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y,\n",
    "    'f_true': f_true\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(df)} observations\")\n",
    "print(f\"x range: [{df['x'].min():.2f}, {df['x'].max():.2f}]\")\n",
    "print(f\"y range: [{df['y'].min():.2f}, {df['y'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "Let's plot our simulated data. With 2000 points, we'll use transparency to show the density while still seeing the underlying smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the data\n",
    "fig = go.Figure()\n",
    "\n",
    "# Subsample for clearer visualization\n",
    "subsample_idx = rng.choice(len(df), size=500, replace=False)\n",
    "subsample_idx = np.sort(subsample_idx)\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Observed data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['x'],\n",
    "    y=df['f_true'],\n",
    "    mode='lines',\n",
    "    name='True latent function',\n",
    "    line=dict(color='dodgerblue', width=2)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Simulated GP Data (2000 points)',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Inducing Points with K-Means\n",
    "\n",
    "A practical strategy for selecting inducing points is to use K-means clustering. This places inducing points at cluster centers in the input space, naturally concentrating them where we have more data while still covering the entire domain.\n",
    "\n",
    "We'll use $m=20$ inducing pointsâ€”a 100Ã— reduction in effective data size. This gives us $\\mathcal{O}(2000 \\times 20^2) = \\mathcal{O}(800\\text{K})$ operations instead of $\\mathcal{O}(2000^3) = \\mathcal{O}(8\\text{B})$ operationsâ€”roughly a 10,000Ã— speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-means to select inducing points\n",
    "m = 20  # Number of inducing points\n",
    "\n",
    "kmeans = KMeans(n_clusters=m, random_state=RANDOM_SEED, n_init=10)\n",
    "kmeans.fit(x[:, None])\n",
    "Xu = np.sort(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "print(f\"Selected {m} inducing points using K-means\")\n",
    "print(f\"Inducing points span: [{Xu.min():.2f}, {Xu.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where K-means placed our inducing points relative to the data density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Data histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df['x'],\n",
    "    nbinsx=50,\n",
    "    name='Data density',\n",
    "    marker=dict(color='lightblue', opacity=0.6),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# Inducing points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Xu,\n",
    "    y=np.zeros(m),\n",
    "    mode='markers',\n",
    "    name='Inducing points',\n",
    "    marker=dict(size=10, color='cyan', symbol='x', line=dict(width=2))\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Inducing Point Locations from K-Means',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='',\n",
    "    yaxis2=dict(title='Count', overlaying='y', side='right'),\n",
    "    height=300,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Inducing Point Placement\n",
    "\n",
    "Notice how K-means distributes the inducing points fairly evenly across the input domain. Since our data is uniformly distributed, the inducing points spread out to cover the range. This is exactly what we want: the inducing points act as strategic summary locations that can represent the entire function.\n",
    "\n",
    "The key insight is that these $m=20$ points don't need to be at data locationsâ€”they're auxiliary variables that help us approximate the GP efficiently. Think of them as anchor points that define a lower-dimensional representation of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sparse GP Model with FITC\n",
    "\n",
    "Now we'll build our sparse GP model using PyMC's `MarginalApprox` class (the modern replacement for `MarginalSparse`) with the FITC approximation. Notice how the model specification is nearly identical to a standard GPâ€”we just provide the inducing point locations `Xu` and specify the approximation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    # Priors on hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # Sparse GP with FITC approximation\n",
    "    gp = pm.gp.MarginalApprox(cov_func=cov, approx='FITC')\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Marginal likelihood\n",
    "    y_obs = gp.marginal_likelihood(\n",
    "        'y_obs',\n",
    "        X=x[:, None],\n",
    "        Xu=Xu[:, None],\n",
    "        y=y,\n",
    "        sigma=sigma\n",
    "    )\n",
    "    \n",
    "    # Sample posterior\n",
    "    idata_sparse = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Posterior\n",
    "\n",
    "Let's check the posterior distributions of our hyperparameters and verify that sampling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary = az.summary(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plot\n",
    "az.plot_trace(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Look at the trace plots and summary statistics. We should see good mixing (the traces look like \"hairy caterpillars\"), high effective sample sizes (ESS), and $\\hat{R}$ values close to 1.0. These diagnostics tell us that the sampler successfully explored the posterior, despite using only 20 inducing points to represent 2000 data points.\n",
    "\n",
    "The posterior means should be close to our true values (lengthscale=1.0, amplitude=3.0, noise=0.5), though with some uncertainty since we're working with finite data and an approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Sparse GP\n",
    "\n",
    "One of the benefits of the sparse GP approximation is that prediction is also fast. Let's make predictions at a dense grid of test points and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test points\n",
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "# Add conditional distribution to model and sample\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    # Sample posterior predictive\n",
    "    posterior_pred = pm.sample_posterior_predictive(\n",
    "        idata_sparse,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior summary\n",
    "f_pred_mean = posterior_pred.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_pred_std = posterior_pred.posterior_predictive['f_pred'].std(dim=['chain', 'draw']).values\n",
    "\n",
    "# Plot results\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([x_test, x_test[::-1]]),\n",
    "    y=np.concatenate([f_pred_mean + 2*f_pred_std, (f_pred_mean - 2*f_pred_std)[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(255,0,0,0.2)',\n",
    "    line=dict(color='rgba(255,0,0,0)'),\n",
    "    name='95% Credible Interval',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_pred_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior Mean',\n",
    "    line=dict(color='red', width=2)\n",
    "))\n",
    "\n",
    "# True function (interpolated to test points)\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_true_interp,\n",
    "    mode='lines',\n",
    "    name='True Function',\n",
    "    line=dict(color='dodgerblue', width=2, dash='dash')\n",
    "))\n",
    "\n",
    "# Data subsample\n",
    "subsample_idx = rng.choice(len(df), size=200, replace=False)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    "))\n",
    "\n",
    "# Inducing points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Xu,\n",
    "    y=np.ones(len(Xu)) * (f_pred_mean.min() - 1),\n",
    "    mode='markers',\n",
    "    name='Inducing Points',\n",
    "    marker=dict(size=10, color='cyan', symbol='x', line=dict(width=2))\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sparse GP Predictions with FITC Approximation',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Sparse GP Fit\n",
    "\n",
    "This plot reveals several important features of the sparse GP approximation:\n",
    "\n",
    "1. **The posterior mean (red line) closely tracks the true function (blue dashed line)**, demonstrating that just 20 inducing points can effectively represent the smooth underlying pattern in 2000 observations.\n",
    "\n",
    "2. **The credible intervals appropriately capture uncertainty**, widening slightly in regions with fewer nearby inducing points and remaining tight where inducing points are dense.\n",
    "\n",
    "3. **The inducing points (cyan X markers at bottom) are strategically distributed** across the domain, acting as anchor points for the approximation.\n",
    "\n",
    "The key takeaway: we've achieved dramatic computational savings while maintaining excellent approximation quality. For smooth functions and well-placed inducing points, the sparse GP delivers results nearly indistinguishable from the exact GP. The downside of sparse approximations is that they reduce the expressiveness of the GPâ€”reducing the dimension of the covariance matrix effectively reduces the number of eigenvectors that can be used to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Sparse GP with Inducing Points\n",
    "\n",
    "Now it's your turn to experiment with sparse GPs and explore how the number of inducing points affects approximation quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help implement a sparse GP\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def sparse_gp_with_kmeans(X, y, M=200):\n",
    "    \"\"\"\n",
    "    Build pm.gp.MarginalApprox using KMeans to initialize M inducing points.\n",
    "    \n",
    "    Prompt suggestion: \"Help me set up MarginalApprox with KMeans initialization\n",
    "    for a dataset, including sampling and prediction.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test your implementation\n",
    "# Try different values of M (e.g., 10, 50, 100) and compare fit quality vs speed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Hilbert Space GP (HSGP) Theory\n",
    "\n",
    "While sparse GPs use inducing points to reduce complexity, the Hilbert Space GP (HSGP) takes a completely different approach: it approximates the GP using a **basis function expansion**. This transforms the non-parametric GP into a parametric model with a fixed number of basis functions, making it compatible with standard MCMC samplers and dramatically improving computational efficiency.\n",
    "\n",
    "The mathematical foundation of HSGP comes from spectral analysis of covariance functions. Any stationary covariance kernel can be represented through its **power spectral density**â€”essentially a Fourier transform that describes the kernel's behavior in frequency space. The HSGP approximation uses a finite set of basis functions (sinusoids) whose coefficients are drawn from a distribution determined by this spectral density.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Think of it this way: instead of defining a function through all pairwise correlations (which requires $n^2$ parameters and $n^3$ operations), HSGP defines it through $m$ basis function coefficients. These basis functions are pre-computed and don't depend on hyperparameters, so we only need to update the coefficients during sampling.\n",
    "\n",
    "The computational complexity drops from $\\mathcal{O}(n^3)$ for exact GPs to $\\mathcal{O}(nm + m)$ for HSGP, where $m$ is the number of basis functions. Even better, HSGP is fully parametricâ€”we can use `pm.set_data` for predictions without explicitly computing conditional distributions. This makes it much easier to integrate an HSGP into your existing PyMC model.\n",
    "\n",
    "Additionally, unlike many other GP approximations, HSGPs can be used anywhere within a model and with any likelihood function. This flexibility is a major advantage over methods like sparse GPs that work best with Gaussian likelihoods.\n",
    "\n",
    "### HSGP Restrictions\n",
    "\n",
    "The HSGP approximation does carry some restrictions:\n",
    "\n",
    "1. It **can only be used with stationary covariance kernels** such as the MatÃ©rn family or ExpQuad. The kernel must implement the `power_spectral_density` method.\n",
    "2. It **does not scale well with input dimension**. HSGP is a good choice for 1D processes (like time series) or 2D spatial processes, but likely not efficient beyond 3 dimensions.\n",
    "3. It **may struggle with very rapidly varying processes**. If the process changes very quickly relative to the domain extent, you may need very large $m$ to accurately represent it.\n",
    "4. **For smaller datasets, the full unapproximated GP may still be more efficient**.\n",
    "\n",
    "### Key Parameters: m and c\n",
    "\n",
    "HSGP approximations are controlled by two parameters:\n",
    "\n",
    "- **m**: The number of basis functions. Larger $m$ gives better approximation quality but increases computational cost. Think of $m$ as the \"resolution\" of your approximationâ€”more basis functions can represent more complex, rapidly-varying patterns. Increasing $m$ helps the HSGP approximate GPs with smaller lengthscales.\n",
    "\n",
    "- **c**: The boundary extension factor. HSGP basis functions are defined on a finite domain $[-L, L]$ where $L = c \\cdot S$ and $S$ is half the range of your centered data. Larger $c$ values help approximate GPs with longer lengthscales and ensure predictions away from data aren't affected by boundary conditions. However, increasing $c$ may require increasing $m$ to compensate for loss of fidelity at smaller lengthscales.\n",
    "\n",
    "The art of using HSGP effectively lies in choosing $m$ and $c$ appropriately for your data and expected lengthscales. Fortunately, PyMC provides a helper function to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing HSGP Basis Functions\n",
    "\n",
    "To build intuition, let's visualize what HSGP basis functions actually look like. These are the sinusoidal building blocks that will be combined to approximate our GP. Notice that we need to center the data firstâ€”this is an important requirement for HSGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a centered grid\n",
    "x_grid = np.linspace(-5, 5, 1000)\n",
    "\n",
    "# Create subplots to show effect of L and m\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "ylim = 0.55\n",
    "axs[0].set_ylim([-ylim, ylim])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_xlabel(\"x (centered)\")\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "# Change L as we create the basis vectors\n",
    "L_options = [5.0, 6.0, 20.0]\n",
    "m_options = [3, 3, 5]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    L = L_options[i]\n",
    "    m_val = m_options[i]\n",
    "    \n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(pt.as_tensor([L]), [m_val])\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(\n",
    "        x_grid[:, None],\n",
    "        pt.as_tensor([L]),\n",
    "        eigvals,\n",
    "        [m_val],\n",
    "    ).eval()\n",
    "    \n",
    "    for j in range(phi.shape[1]):\n",
    "        ax.plot(x_grid, phi[:, j])\n",
    "    \n",
    "    ax.set_xticks(np.arange(-5, 6, 5))\n",
    "    \n",
    "    S = 5.0\n",
    "    c = L / S\n",
    "    ax.text(-4.9, -0.45, f\"L = {L}\\nc = {c}\", fontsize=12)\n",
    "    ax.set_title(f\"{m_val} basis functions\")\n",
    "    ax.set_xlabel(\"x (centered)\")\n",
    "\n",
    "axs[0].set_ylabel(\"Basis function value\")\n",
    "plt.suptitle(\"The Effect of Changing L on HSGP Basis Vectors\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Basis Functions\n",
    "\n",
    "These plots reveal critical insights about HSGP basis functions:\n",
    "\n",
    "**Left panel (L=5, c=1.0)**: When $L$ equals the data range, all basis vectors are forced to pinch to zero at the boundaries (at $x=-5$ and $x=5$). This means the HSGP approximation becomes poor near the edges of your data. This is why we need $c > 1$.\n",
    "\n",
    "**Middle panel (L=6, c=1.2)**: With $c=1.2$, the basis functions extend beyond the data range and are no longer forced to zero at the data boundaries. This helps the approximation remain accurate across the entire domain. Values of $c$ around 1.2 are considered the minimum for reasonable approximations.\n",
    "\n",
    "**Right panel (L=20, c=4.0, m=5)**: With larger $L$ or $c$, the basis functions become lower frequency (longer wavelength). Notice how the first basis function (blue) is nearly flatâ€”it's becoming partially unidentifiable with an intercept term. This is why we sometimes need to drop the first basis function, or increase $m$ to compensate.\n",
    "\n",
    "Notice that the basis functions are sinusoids with increasing frequency. Lower-order basis functions capture long-range trends, while higher-order functions capture increasingly rapid oscillations. An HSGP approximation works by taking a weighted sum of these basis functions.\n",
    "\n",
    "The key lessons:\n",
    "- **Increasing $m$ helps approximate GPs with smaller lengthscales** (more basis functions = higher resolution)\n",
    "- **Increasing $c$ or $L$ helps approximate GPs with larger lengthscales** but may require increasing $m$ to maintain fidelity at smaller lengthscales\n",
    "- **Consider where predictions will be made**â€”they also need to be away from the boundary \"pinch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: HSGP Implementation\n",
    "\n",
    "Now let's implement an HSGP model and see it in action. We'll use the same dataset as before for direct comparison with the sparse GP.\n",
    "\n",
    "### Choosing HSGP Parameters\n",
    "\n",
    "PyMC provides a helper function `approx_hsgp_hyperparams` that suggests values for $m$ and $c$ based on:\n",
    "- The range of your input data\n",
    "- The range of lengthscales you expect (from your prior)\n",
    "- The covariance function type\n",
    "\n",
    "These recommendations are based on approximation error bounds derived in the HSGP literature. The heuristics help you choose $c$ large enough to handle the largest lengthscales you might fit, and $m$ large enough to accommodate the smallest lengthscales. Let's use this function to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate m and c\n",
    "x_range = [x.min(), x.max()]\n",
    "lengthscale_range = [0.5, 3.0]  # Based on our prior knowledge\n",
    "\n",
    "m_recommended, c_recommended = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=x_range,\n",
    "    lengthscale_range=lengthscale_range,\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"Recommended m: {m_recommended}\")\n",
    "print(f\"Recommended c: {c_recommended:.2f}\")\n",
    "\n",
    "# We'll use these values for our model\n",
    "m_hsgp = m_recommended\n",
    "c_hsgp = c_recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the HSGP Model\n",
    "\n",
    "The HSGP model specification in PyMC is remarkably similar to a standard GP. The key difference is that we use `pm.gp.HSGP` instead of `pm.gp.Latent` or `pm.gp.Marginal`, and we specify the approximation parameters $m$ and $c$. \n",
    "\n",
    "Notice that we use the `.prior` method just like with `pm.gp.Latent`. For basic usage, HSGP can be treated as a drop-in replacement for the standard latent GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_model:\n",
    "    # Priors on hyperparameters (same as sparse GP)\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP approximation\n",
    "    gp = pm.gp.HSGP(m=[m_hsgp], c=c_hsgp, cov_func=cov)\n",
    "    \n",
    "    # Prior over the latent function\n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    # Sample posterior\n",
    "    idata_hsgp = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining HSGP Results\n",
    "\n",
    "Let's check sampling diagnostics and posterior distributions for the HSGP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary_hsgp = az.summary(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")\n",
    "print(summary_hsgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plot\n",
    "az.plot_trace(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with HSGP\n",
    "\n",
    "One major advantage of HSGP is the ease of prediction. Since it's parametric, we can use the `.conditional` method just like with other GPs. Let's make predictions and visualize the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test points\n",
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "with hsgp_model:\n",
    "    f_pred_hsgp = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    # Sample posterior predictive\n",
    "    posterior_pred_hsgp = pm.sample_posterior_predictive(\n",
    "        idata_hsgp,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute posterior summary\n",
    "f_hsgp_mean = posterior_pred_hsgp.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_hsgp_std = posterior_pred_hsgp.posterior_predictive['f_pred'].std(dim=['chain', 'draw']).values\n",
    "\n",
    "# Plot HSGP results\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([x_test, x_test[::-1]]),\n",
    "    y=np.concatenate([f_hsgp_mean + 2*f_hsgp_std, (f_hsgp_mean - 2*f_hsgp_std)[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(139,0,139,0.2)',\n",
    "    line=dict(color='rgba(139,0,139,0)'),\n",
    "    name='95% Credible Interval',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_hsgp_mean,\n",
    "    mode='lines',\n",
    "    name='HSGP Posterior Mean',\n",
    "    line=dict(color='darkviolet', width=2)\n",
    "))\n",
    "\n",
    "# True function\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_true_interp,\n",
    "    mode='lines',\n",
    "    name='True Function',\n",
    "    line=dict(color='gold', width=3, dash='dash')\n",
    "))\n",
    "\n",
    "# Data subsample\n",
    "subsample_idx = rng.choice(len(df), size=200, replace=False)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'HSGP Fit (m={m_hsgp}, c={c_hsgp:.2f})',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the HSGP Fit\n",
    "\n",
    "The HSGP inferred posterior (purple) accurately matches the true underlying GP (gold dashed line). We also see that the credible intervals appropriately capture uncertainty. This demonstrates that even with an approximation using basis functions, we can achieve excellent fit quality.\n",
    "\n",
    "Notice that with recommended parameters from `approx_hsgp_hyperparams`, the approximation is essentially indistinguishable from what an exact GP would produce. The computational cost, however, is dramatically lowerâ€”$\\mathcal{O}(nm)$ instead of $\\mathcal{O}(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing HSGP to Sparse GP\n",
    "\n",
    "Let's directly compare the posterior distributions from the HSGP and sparse GP models using a forest plot, which is ideal for comparing multiple models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare posteriors using forest plot\n",
    "az.plot_forest(\n",
    "    [idata_sparse, idata_hsgp],\n",
    "    model_names=['Sparse GP', 'HSGP'],\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    combined=True,\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Comparison\n",
    "\n",
    "The posterior distributions from HSGP and sparse GP should be very similar, particularly for the lengthscale and amplitude parameters that control the function's smoothness and scale. Small differences are expected since both are approximations, but substantial disagreement would suggest that one or both approximations is inadequate.\n",
    "\n",
    "Both methods successfully inferred hyperparameters close to the true values (lengthscale=1.0, amplitude=3.0, noise=0.5), demonstrating that either approach can work well for moderate-sized datasets with smooth underlying functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: Advanced HSGP - Centered vs Non-Centered Parameterization\n",
    "\n",
    "An important consideration when using HSGP is choosing between centered and non-centered parameterizations. This is analogous to the choice you make in hierarchical models, and for similar reasons: the correlation structure in the posterior.\n",
    "\n",
    "### When to Use Each Parameterization\n",
    "\n",
    "**Centered parameterization** works better when:\n",
    "- The underlying GP is strongly informed by the data\n",
    "- You have lots of data relative to the lengthscale\n",
    "- The signal-to-noise ratio is high\n",
    "\n",
    "**Non-centered parameterization** (the default) works better when:\n",
    "- The underlying GP is weakly informed by the data  \n",
    "- You have sparse data or large lengthscales\n",
    "- The signal-to-noise ratio is low\n",
    "\n",
    "In our example with 2000 noisy observations, the centered parameterization might actually be better. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_centered:\n",
    "    # Same priors\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP with centered parameterization\n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_hsgp], \n",
    "        c=c_hsgp, \n",
    "        cov_func=cov,\n",
    "        parametrization='centered'  # Key difference!\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    # Sample\n",
    "    idata_hsgp_centered = pm.sample(\n",
    "        1000,\n",
    "        tune=1000,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare sampling efficiency\n",
    "print(\"Non-centered parameterization:\")\n",
    "print(az.summary(idata_hsgp, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])\n",
    "print(\"\\nCentered parameterization:\")\n",
    "print(az.summary(idata_hsgp_centered, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Parameterization Effects\n",
    "\n",
    "Compare the effective sample sizes (ESS) between the two parameterizations. Higher ESS means more efficient samplingâ€”you're getting more independent samples per iteration. For this dataset with strong signal, you may find the centered parameterization provides better ESS.\n",
    "\n",
    "The choice of parameterization doesn't affect what you're learning about the hyperparametersâ€”it only affects how efficiently the sampler explores the posterior. If you find sampling is slow or you see low ESS, try switching parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the HSGP Approximate Gram Matrix\n",
    "\n",
    "Another way to check HSGP fidelity is to directly compare the unapproximated Gram matrix (covariance matrix) $\\mathbf{K}$ to the one resulting from the HSGP approximation:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{K}} = \\Phi \\Lambda \\Phi^T\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the matrix of eigenvectors (basis functions), and $\\Lambda$ has the spectral densities computed at the eigenvalues along the diagonal. Let's visualize this for different values of $m$ and $c$ to see when the approximation starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for clearer visualization\n",
    "n_viz = 100\n",
    "x_viz = np.linspace(0, 10, n_viz)\n",
    "\n",
    "# True GP covariance\n",
    "chosen_ell = 1.5\n",
    "cov_func_viz = pm.gp.cov.Matern52(1, ls=chosen_ell)\n",
    "K_true = cov_func_viz(x_viz[:, None]).eval()\n",
    "\n",
    "# Helper function to calculate HSGP approximate Gram matrix\n",
    "def calculate_K_approx(x_centered, L, m_val, cov_func):\n",
    "    \"\"\"Calculate the HSGP approximate covariance matrix.\"\"\"\n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(L, m_val)\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(x_centered, L, eigvals, m_val)\n",
    "    omega = pt.sqrt(eigvals)\n",
    "    psd = cov_func.power_spectral_density(omega)\n",
    "    return (phi @ pt.diag(psd) @ phi.T).eval()\n",
    "\n",
    "# Center the data\n",
    "x_center = (x_viz.max() + x_viz.min()) / 2.0\n",
    "x_viz_centered = (x_viz - x_center)[:, None]\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharey=True)\n",
    "\n",
    "# True Gram matrix\n",
    "axs[0, 0].imshow(K_true, cmap='inferno', vmin=0, vmax=1)\n",
    "axs[0, 0].set_title(f'True Gram matrix\\nâ„“ = {chosen_ell}')\n",
    "axs[0, 0].set_ylabel('Index')\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "# Various m and c combinations\n",
    "configs = [\n",
    "    ([30], 2.5, 1),\n",
    "    ([15], 2.5, 2),\n",
    "    ([30], 1.2, 3),\n",
    "    ([15], 1.2, 4),\n",
    "    ([5], 2.5, 5),\n",
    "]\n",
    "\n",
    "for m_val, c_val, idx in configs:\n",
    "    row = 0 if idx <= 2 else 1\n",
    "    col = idx if idx <= 2 else idx - 3\n",
    "    \n",
    "    L = pm.gp.hsgp_approx.set_boundary(x_viz_centered, c_val)\n",
    "    K_approx = calculate_K_approx(x_viz_centered, L, m_val, cov_func_viz)\n",
    "    \n",
    "    axs[row, col].imshow(K_approx, cmap='inferno', vmin=0, vmax=1, interpolation='none')\n",
    "    axs[row, col].set_title(f'm = {m_val[0]}, c = {c_val}')\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('Index')\n",
    "    if row == 1:\n",
    "        axs[row, col].set_xlabel('Index')\n",
    "\n",
    "plt.suptitle('HSGP Approximation Quality: Comparing to True Gram Matrix', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gram Matrix Approximations\n",
    "\n",
    "These plots compare approximate Gram matrices to the unapproximated one (top left). The goal is visual similarityâ€”the more alike they look, the better the approximation. Important caveats:\n",
    "\n",
    "- These results are **only relevant for this specific domain and lengthscale** ($\\ell = 1.5$). Different lengthscales will show different approximation quality.\n",
    "- The approximation looks good for $m = 30$ or $m = 15$ with $c=2.5$. The rest show clear differences.\n",
    "- $c=1.2$ is generally too small, regardless of $m$, showing degradation at the boundaries.\n",
    "- Surprisingly, $m=5$, $c=1.2$ can look better than $m=5$, $c=2.5$. When we \"stretch\" the basis to fill a larger domain, we lose fidelity at smaller lengthscales if $m$ is too small.\n",
    "\n",
    "The lesson: **you need to experiment across your range of lengthscales** to find adequate $m$ and $c$ values. Often during prototyping, you can use lower fidelity (smaller $m$) for faster iteration, then dial in higher fidelity once you understand the relevant lengthscales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Heuristics for Choosing m and c\n",
    "\n",
    "In practice, you'll need to infer the lengthscale from data, so HSGP needs to approximate a GP across a range of lengthscales representative of your prior. Based on the research literature and empirical experience:\n",
    "\n",
    "1. **Start with `approx_hsgp_hyperparams`**: This function provides good default values. It chooses $c$ large enough to handle your largest expected lengthscales and $m$ large enough for your smallest lengthscales.\n",
    "\n",
    "2. **For smooth functions with moderate lengthscales**: You can often reduce $m$ to 50-100, lowering computational cost.\n",
    "\n",
    "3. **For rapidly-varying functions**: Increase $m$ to 100-200 or more to capture high-frequency components.\n",
    "\n",
    "4. **For long lengthscales**: Increase $c$ to 2.5-4.0 to ensure basis functions extend well beyond your data.\n",
    "\n",
    "5. **Check the basis vectors if sampling struggles**: The first eigenvector can become unidentifiable with the intercept when $c$ is large. Consider using the `drop_first` option.\n",
    "\n",
    "6. **Verify approximation quality**: Compare HSGP to exact GP on a data subset when possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Comparing HSGP vs Full GP\n",
    "\n",
    "Now you'll explore the trade-off between approximation quality and computation time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help compare HSGP vs standard GP\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def hsgp_vs_full_gp(X, y, m_values=(20, 50, 100), L_factor=1.5):\n",
    "    \"\"\"\n",
    "    Fit HSGP for several m, compare to full GP in time and RMSE.\n",
    "    \n",
    "    Prompt suggestion: \"Help me implement HSGP in PyMC and produce plots of\n",
    "    computation time vs error across different m values.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on a subset of data\n",
    "# Use 500-1000 points for reasonable comparison times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6: Advanced HSGP - 2D Spatial Example\n",
    "\n",
    "So far we've focused on one-dimensional examples. HSGP also works well in two dimensions, making it excellent for spatial modeling. Let's see a quick 2D example to understand how $m$ and $c$ work in multiple dimensions.\n",
    "\n",
    "### Simulating 2D Spatial Data\n",
    "\n",
    "We'll create data on a 2D grid with a spatial GP component and a fixed effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_2d_spatial(n_points=400, ell_true=1.5, eta_true=1.0, sigma_true=0.3):\n",
    "    \"\"\"Simulate 2D spatial data from a GP.\"\"\"\n",
    "    # Create spatial grid\n",
    "    n_side = int(np.sqrt(n_points))\n",
    "    x1, x2 = np.meshgrid(\n",
    "        np.linspace(0, 10, n_side),\n",
    "        np.linspace(0, 10, n_side)\n",
    "    )\n",
    "    X = np.column_stack([x1.flatten(), x2.flatten()])\n",
    "    \n",
    "    # Sample from 2D GP\n",
    "    cov_func = eta_true**2 * pm.gp.cov.Matern52(2, ls=ell_true)\n",
    "    K = cov_func(X).eval()\n",
    "    K_stable = K + 1e-8 * np.eye(len(X))\n",
    "    f_true = rng.multivariate_normal(np.zeros(len(X)), K_stable)\n",
    "    \n",
    "    # Add noise\n",
    "    y = f_true + sigma_true * rng.standard_normal(len(X))\n",
    "    \n",
    "    return X, y, f_true\n",
    "\n",
    "X_2d, y_2d, f_true_2d = simulate_2d_spatial()\n",
    "\n",
    "print(f\"Generated {len(X_2d)} observations on 2D grid\")\n",
    "print(f\"X ranges: [{X_2d[:, 0].min():.2f}, {X_2d[:, 0].max():.2f}] x [{X_2d[:, 1].min():.2f}, {X_2d[:, 1].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the true spatial field and the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create side-by-side plots\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['True Spatial Field', 'Observed Data'],\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "# Reshape for plotting\n",
    "n_side = int(np.sqrt(len(X_2d)))\n",
    "f_grid = f_true_2d.reshape(n_side, n_side)\n",
    "y_grid = y_2d.reshape(n_side, n_side)\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=f_grid, colorscale='RdBu', zmid=0, showscale=True),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=y_grid, colorscale='RdBu', zmid=0, showscale=True),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='x1')\n",
    "fig.update_yaxes(title_text='x2', row=1, col=1)\n",
    "fig.update_layout(height=400, title_text='2D Spatial GP Data')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a 2D HSGP Model\n",
    "\n",
    "For 2D HSGPs, we specify $m$ and $c$ as two-element listsâ€”one value per dimension. The total number of basis functions is $m_1 \\times m_2$, so computational cost grows multiplicatively with dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get recommendations for 2D\n",
    "m_2d, c_2d = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=[0, 10],\n",
    "    lengthscale_range=[1.0, 3.0],\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"2D HSGP recommendations: m={m_2d}, c={c_2d:.2f}\")\n",
    "print(f\"Total basis functions: {m_2d**2}\")\n",
    "\n",
    "with pm.Model() as hsgp_2d_model:\n",
    "    # Priors\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "    \n",
    "    # 2D covariance\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(2, ls=ell)\n",
    "    \n",
    "    # HSGP with 2D specification\n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_2d, m_2d],  # m for each dimension\n",
    "        c=c_2d,           # c applies to both\n",
    "        cov_func=cov\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=X_2d)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y_2d)\n",
    "    \n",
    "    # Sample\n",
    "    idata_2d = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the 2D HSGP Fit\n",
    "\n",
    "Let's extract the posterior mean of the spatial field and compare it to the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior mean\n",
    "f_post = idata_2d.posterior['f'].mean(dim=['chain', 'draw']).values\n",
    "f_post_grid = f_post.reshape(n_side, n_side)\n",
    "\n",
    "# Create comparison plot\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=['True Spatial Field', 'HSGP Posterior Mean'],\n",
    "    horizontal_spacing=0.15\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=f_grid, colorscale='RdBu', zmid=0, showscale=False),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=f_post_grid, colorscale='RdBu', zmid=0, showscale=True),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='x1')\n",
    "fig.update_yaxes(title_text='x2', row=1, col=1)\n",
    "fig.update_layout(height=400, title_text='2D HSGP Recovery of True Field')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding 2D HSGP Performance\n",
    "\n",
    "The HSGP successfully recovered the spatial structure from the noisy observations. Notice how smoothly the posterior captures the underlying pattern while averaging out the noise.\n",
    "\n",
    "For 2D problems, remember that the total number of basis functions is $m_1 \\times m_2$. With $m=[32, 32]$, we're using 1,024 basis functions. This is still far more efficient than exact inference on 400 points (which would require $400^3 \\approx 64$ million operations), but it shows why HSGP doesn't scale well beyond 3 dimensionsâ€”the basis functions multiply quickly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Exploring HSGP Parameter Choices\n",
    "\n",
    "Now experiment with different HSGP parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help explore HSGP parameter choices\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def tune_hsgp_params(X, y, m_grid=(30, 60, 120), c_grid=(1.5, 2.5, 4.0)):\n",
    "    \"\"\"\n",
    "    Grid-search m and c to evaluate speed and accuracy trade-offs.\n",
    "    \n",
    "    Prompt suggestion: \"Help me create a function that runs multiple HSGP fits\n",
    "    over m and c grid and summarizes performance with Plotly heatmaps.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on a moderate-sized dataset\n",
    "# Visualize RMSE and sampling time as heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7: Comparing All Approaches\n",
    "\n",
    "We've now explored both sparse GPs and HSGP approximations in detail. Let's bring everything together with a comprehensive comparison that highlights when to use each approach.\n",
    "\n",
    "### Computational Complexity Summary\n",
    "\n",
    "Let's visualize the computational complexity of each approach as a function of dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison plot of computational complexity\n",
    "n_values = np.logspace(2, 4, 50)  # 100 to 10,000 data points\n",
    "m_sparse = 100  # inducing points\n",
    "m_hsgp = 100    # basis functions\n",
    "\n",
    "# Relative computational cost (arbitrary units)\n",
    "cost_exact = n_values**3 / 1e6  # Scale for visibility\n",
    "cost_sparse = n_values * m_sparse**2 / 1e6\n",
    "cost_hsgp = n_values * m_hsgp / 1e6\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_exact,\n",
    "    mode='lines',\n",
    "    name='Exact GP: O(nÂ³)',\n",
    "    line=dict(color='blue', width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_sparse,\n",
    "    mode='lines',\n",
    "    name=f'Sparse GP: O(nmÂ²), m={m_sparse}',\n",
    "    line=dict(color='green', width=3)\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_hsgp,\n",
    "    mode='lines',\n",
    "    name=f'HSGP: O(nm), m={m_hsgp}',\n",
    "    line=dict(color='red', width=3)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Computational Complexity: Exact GP vs. Approximations',\n",
    "    xaxis_title='Number of data points (n)',\n",
    "    yaxis_title='Relative computational cost',\n",
    "    xaxis_type='log',\n",
    "    yaxis_type='log',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Complexity Comparison\n",
    "\n",
    "This log-log plot dramatically illustrates why approximations are essential for large datasets:\n",
    "\n",
    "- **The blue line (exact GP)** curves upward steeply, showing the crushing $\\mathcal{O}(n^3)$ growth. By $n=10,000$, exact inference is essentially infeasible.\n",
    "\n",
    "- **The green line (sparse GP)** grows much more slowly at $\\mathcal{O}(nm^2)$, making datasets of several thousand points tractable.\n",
    "\n",
    "- **The red line (HSGP)** has the gentlest slope at $\\mathcal{O}(nm)$, showing near-linear scaling that makes even very large datasets manageable.\n",
    "\n",
    "The crossover points where approximations become worthwhile depend on your patience, hardware, and accuracy requirements, but as a rough guide: consider sparse GPs beyond ~1,000 points and HSGP beyond ~5,000 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Guide: Which Method to Use\n",
    "\n",
    "Here's practical guidance for choosing between methods:\n",
    "\n",
    "**Use Standard (Exact) GP when:**\n",
    "- $n < 1,000$ points\n",
    "- You need exact inference without approximation error\n",
    "- You're using non-stationary kernels\n",
    "- Computation time isn't critical\n",
    "\n",
    "**Use Sparse GP (Inducing Points) when:**\n",
    "- $1,000 < n < 10,000$ points  \n",
    "- Data has uneven sampling density\n",
    "- You have domain knowledge about where to place inducing points\n",
    "- You're primarily using Gaussian likelihoods\n",
    "- **Typical use cases**: Spatial data with known regions of interest, time series with known change points\n",
    "\n",
    "**Use HSGP when:**\n",
    "- $n > 5,000$ points\n",
    "- Using stationary kernels (MatÃ©rn, ExpQuad)\n",
    "- Input dimension is 1, 2, or 3\n",
    "- You need to integrate the GP into a larger model\n",
    "- You need predictions at many new locations\n",
    "- **Typical use cases**: Long time series, spatial data on regular grids, any large dataset with smooth variation\n",
    "\n",
    "**Practical tip**: When prototyping, start with a low-fidelity HSGP (small $m$) for fast iteration. Once you understand the relevant lengthscales, dial in appropriate $m$ and $c$ for production."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this session, we've tackled one of the most practical challenges in GP modeling: scaling to larger datasets. We explored two complementary approaches that dramatically reduce computational complexity while maintaining the core benefits of GP modeling.\n",
    "\n",
    "### Sparse GP Approximations\n",
    "\n",
    "Sparse methods reduce complexity from $\\mathcal{O}(n^3)$ to $\\mathcal{O}(nm^2)$ by representing the GP through $m < n$ strategically placed inducing points. We saw how K-means provides a practical initialization strategy, and how the FITC approximation balances speed with uncertainty quantification. These methods work well for moderately large datasets where inducing points can be placed thoughtfully.\n",
    "\n",
    "### HSGP Approximations\n",
    "\n",
    "The Hilbert Space GP uses basis function expansions to achieve $\\mathcal{O}(nm + m)$ complexity. By representing the GP as a weighted sum of pre-computed sinusoidal basis functions, HSGP becomes fully parametric and easy to integrate into larger models. The approximation quality is controlled by $m$ (number of basis functions) and $c$ (boundary factor), which can be chosen using PyMC's `approx_hsgp_hyperparams` helper function.\n",
    "\n",
    "### Key Insights\n",
    "\n",
    "We learned that choosing between these methods requires understanding:\n",
    "- Your dataset size and structure\n",
    "- Whether your kernel is stationary\n",
    "- The dimensionality of your input space\n",
    "- The expected range of lengthscales\n",
    "- Whether you need exact inference or can tolerate approximation\n",
    "\n",
    "Both approximations involve trade-offsâ€”we exchange some exactness for massive computational gains. The art lies in making these trade-offs intelligently based on your specific application.\n",
    "\n",
    "### Looking Ahead\n",
    "\n",
    "With these scaling techniques in your toolkit, you can now apply GP models to real-world datasets that would have been computationally prohibitive with exact inference. In the next session, we'll explore multi-output GPs and comprehensive case studies that bring together all the concepts from this workshop, including using HSGP for complex real-world modeling tasks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
