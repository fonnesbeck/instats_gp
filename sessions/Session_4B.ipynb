{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_4B.ipynb)\n",
    "\n",
    "# Session 4B: Case Study – Soccer Player Skill Modeling\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Build hierarchical models** that decompose skill from context\n",
    "2. **Integrate HSGP with hierarchical structure** for scalable temporal modeling\n",
    "3. **Apply maximum entropy priors** to encode domain knowledge\n",
    "4. **Execute a complete case study** analyzing soccer player performance across multiple timescales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import preliz as pz\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import logit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:=8675309)\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "print(f\"PyMC: {pm.__version__}\")\n",
    "print(f\"NumPy: {np.__version__}\")\n",
    "print(f\"Pandas: {pd.__version__}\")\n",
    "print(f\"ArviZ: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Soccer Factor Model: From Finance to Sports\n",
    "\n",
    "### The Inspiration: Factor Models in Asset Pricing\n",
    "\n",
    "The Soccer-Factor-Model (SFM) draws inspiration from academic literature on asset pricing, specifically:\n",
    "\n",
    "**1. Fama-French Factor Models** explain stock returns using \"factors\" that capture market-wide effects:\n",
    "   - Here: We use \"Team-Factors\" to explain the role of team strength in enabling player $i$ to score goals\n",
    "\n",
    "**2. Fund Manager Skill Attribution** decomposes fund returns to isolate manager skill from market exposure:\n",
    "\n",
    "$$r_{p,t} = \\alpha_p + \\sum^N_{n=1} \\beta_n \\, f_{n,t} + \\varepsilon_{p,t}$$\n",
    "\n",
    "where:\n",
    "- $r_{p,t}$: Portfolio return in excess of the risk-free rate\n",
    "- $f_{n,t}$: Return from following factor $n$'s strategy\n",
    "- $\\beta_n$: Sensitivity to factor $n$\n",
    "- $\\alpha_p$: **Manager skill**—returns beyond what factors explain\n",
    "- $\\varepsilon_{p,t}$: Random error (mean zero)\n",
    "\n",
    "The parameter $\\alpha_p$ quantifies skill: the ability to generate returns **beyond** what you'd get by mechanically following the factor strategies.\n",
    "\n",
    "### From Asset Pricing to Soccer\n",
    "\n",
    "In our case, the outcome variable is binary—whether player $i$ scored a goal in season $s$, match $m$:\n",
    "\n",
    "$$y_{i,s,m} = \\begin{cases} 0 & \\text{did not score} \\\\\\\\ 1 & \\text{scored} \\end{cases}$$\n",
    "\n",
    "We model the probability of scoring as:\n",
    "\n",
    "$$p_{i,s,m} \\equiv P(y_{i,s,m} = 1 | \\mathbf{x}_{i,s,m-1}, \\alpha_i, \\beta) = \\sigma(\\alpha_{i} + \\mathbf{x}_{i,s,m-1} \\, \\mathbf{\\beta})$$\n",
    "\n",
    "where:\n",
    "- $p_{i,s,m}$: Probability of player $i$ scoring in season $s$, match $m$\n",
    "- $\\mathbf{x}_{i,s,m-1}$: **Factors** capturing team imbalance (information available **before** match $m$)\n",
    "- $\\beta$: Factor sensitivities (how much team context matters)\n",
    "- $\\alpha_i$: **Player skill**—our main interest! (scoring ability after controlling for team effects)\n",
    "- $\\sigma(\\cdot)$: Sigmoid function, mapping linear predictions to $[0,1]$ probabilities\n",
    "\n",
    "**Critical Detail**: Factors $\\mathbf{x}_{i,s,m-1}$ only use information available **before** match $m$ starts. This ensures we're making predictions, not fitting to outcomes we already know.\n",
    "\n",
    "### The Challenge: Skill vs. Context\n",
    "\n",
    "In finance, the Fama-French factor model asks a fundamental question: *Is a fund manager truly skilled, or just lucky with market exposure?* A portfolio might outperform simply because it's heavily weighted toward stocks that happened to do well, not because of superior stock-picking ability.\n",
    "\n",
    "We face a similar challenge in sports analytics. Is a player elite, or do they benefit from:\n",
    "- Strong teammates who create scoring opportunities?\n",
    "- Weak opponents who allow more goals?\n",
    "- Home-field advantage?\n",
    "- Being in their physical prime vs. early career or decline?\n",
    "\n",
    "### Our Modeling Strategy\n",
    "\n",
    "We extend the basic factor model to include **temporal dynamics** using Gaussian Processes:\n",
    "\n",
    "$$P(\\text{goal}_{ij} = 1) = \\text{logit}^{-1}(\\alpha_i + f_{\\text{within}}(t) + f_{\\text{long}}(s) + \\mathbf{X}_{ij}^T\\boldsymbol{\\beta})$$\n",
    "\n",
    "Where:\n",
    "- $\\alpha_i$: Player-specific skill (our main interest—the \"true talent\")\n",
    "- $f_{\\text{within}}(t)$: Within-season form variation (matchday-to-matchday fluctuations)\n",
    "- $f_{\\text{long}}(s)$: Long-term aging curve (career trajectory across seasons)\n",
    "- $\\mathbf{X}_{ij}^T\\boldsymbol{\\beta}$: Team context effects (factors we want to control for)\n",
    "\n",
    "This is a **factor model with temporal structure**: we decompose observed outcomes into skill (what we care about), temporal dynamics (form and aging), and context (confounders we account for).\n",
    "\n",
    "**Why add GPs?** Players don't have constant performance:\n",
    "- **Within-season variation**: Injuries, fatigue, hot/cold streaks vary matchday-to-matchday\n",
    "- **Across-season variation**: Young players improve, veterans decline (aging curves)\n",
    "- **Unknown functional forms**: We don't know the exact shape of these effects—GPs learn them from data\n",
    "\n",
    "By modeling these temporal patterns as GPs, we can:\n",
    "1. Separate persistent skill from temporary form\n",
    "2. Identify career peak and decline phases\n",
    "3. Account for uncertainty in all temporal patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factors: The Heart of the Model\n",
    "\n",
    "Factors help us understand which part of scoring probability $p_i$ comes from **team strength** ($\\mathbf{X}_i \\, \\mathbf{\\beta}$) versus **player skill** ($\\alpha_i$). \n",
    "\n",
    "Think of it this way: if a striker plays for a dominant team that creates 20 chances per game against weak opponents at home, their high goal count might reflect those advantages—not exceptional finishing ability. The factors let us ask: **\"After controlling for these advantages, how good is this player really?\"**\n",
    "\n",
    "The parameter $\\alpha_i$ is our main interest: it measures player $i$'s skill to score goals **after accounting for team context**.\n",
    "\n",
    "### Factor Engineering\n",
    "\n",
    "The quality of our factors depends on available data. We use publicly available data from Kaggle ([Premier League dataset](https://www.kaggle.com/datasets/marclamyhshshs/premier-league-games)), covering all matches since 1992—an impressive historical record!\n",
    "\n",
    "The dataset enables us to engineer 8 factors capturing team context:\n",
    "\n",
    "1. **`goalsscored_diff`**: Current goal difference in the match\n",
    "2. **`goalsscored_rank_team`**: Player's team ranking in goals scored\n",
    "3. **`goalsscored_rank_team_wo_player`**: Team ranking excluding this player's goals (isolates team effect)\n",
    "4. **`goalsconceded_rank_opp`**: Opponent's ranking in goals conceded (defensive weakness)\n",
    "5. **`points_diff`**: Points differential over last 5 games (recent momentum)\n",
    "6. **`goal_balance_team`**: Goals scored - goals conceded for player's team\n",
    "7. **`goal_balance_opp`**: Goals scored - goals conceded for opponent\n",
    "8. **`goal_balance_diff`**: Difference between `goal_balance_team` and `goal_balance_opp`\n",
    "\n",
    "**Critical principle**: Factors must reflect **team effort**, not individual player ability. Including player-specific factors would confound our ability to isolate $\\alpha_i$.\n",
    "\n",
    "### Context Factors: Controlling for Confounders\n",
    "\n",
    "For this demonstration, we use **3 key factors** that capture distinct dimensions:\n",
    "\n",
    "- **`home_pitch`** (binary): Home-field advantage (venue familiarity, crowd support)\n",
    "- **`points_diff`**: Recent momentum and form (hot/cold streaks)\n",
    "- **`goal_balance_diff`**: Overall team quality gap (skill mismatch between teams)\n",
    "\n",
    "These factors let us ask: *After controlling for being at home, having momentum, and facing weak opposition, which players still score at high rates?* Those are the truly skilled players.\n",
    "\n",
    "**What about other factors?** The dataset includes additional information (shots, possession, etc.) that could create more sophisticated factors. We encourage you to experiment! The factor engineering process is creative—domain knowledge about what drives team-level performance is invaluable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "\n",
    "We'll use a dataset of soccer player performance across multiple seasons. The data includes:\n",
    "\n",
    "- **Players**: Top performers from major leagues\n",
    "- **Temporal coverage**: Multiple seasons with matchday-level granularity\n",
    "- **Contextual factors**: Team strength, opponent quality, home advantage, momentum\n",
    "- **Outcome**: Binary goal-scoring events\n",
    "\n",
    "First, we define our analysis parameters and load the data. We analyze a comprehensive set of 27 elite players spanning different eras and playing styles, demonstrating the full modeling workflow across a diverse set of careers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define factors (context variables) to control for\n",
    "factors_numeric = [\"points_diff\", \"goal_balance_diff\"]\n",
    "factors = [\"home_pitch\"] + factors_numeric\n",
    "\n",
    "# Select top players to analyze (using lowercase hyphenated names as in the data)\n",
    "players_ordered = pd.Index(\n",
    "    sorted(\n",
    "        [\n",
    "            \"ricardo-quaresma\",\n",
    "            \"hugo-almeida\",\n",
    "            \"joao-felix\",\n",
    "            \"helder-postiga\",\n",
    "            \"andre-silva\",\n",
    "            \"luis-fabiano\",\n",
    "            \"ailton\",\n",
    "            \"paulo-sergio\",\n",
    "            \"mario-gomez\",\n",
    "            \"oliver-neuville\",\n",
    "            \"peter-crouch\",\n",
    "            \"stefan-kiessling\",\n",
    "            \"cristiano-ronaldo\",\n",
    "            \"nani\",\n",
    "            \"ronaldo\",\n",
    "            \"neymar\",\n",
    "            \"pato\",\n",
    "            \"ronaldinho\",\n",
    "            \"diego-costa\",\n",
    "            \"michael-owen\",\n",
    "            \"thierry-henry\",\n",
    "            \"wayne-rooney\",\n",
    "            \"lionel-messi\",\n",
    "            \"karim-benzema\",\n",
    "            \"edinson-cavani\",\n",
    "            \"gonzalo-higuain\",\n",
    "            \"chicharito\",\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Random number generator for reproducibility\n",
    "rng = RNG\n",
    "\n",
    "print(f\"Analyzing {len(players_ordered)} players\")\n",
    "print(f\"Context factors: {factors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Organizing the Data\n",
    "\n",
    "Now we load the dataset and filter to our selected players. The data is organized with one row per match appearance, tracking whether the player scored in that match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = pd.read_csv(\"../data/SFM_data_byPlayer_clean.csv\")\n",
    "complete_data = (\n",
    "    complete_data[complete_data.name_player.isin(players_ordered)]\n",
    "    .sort_values([\"name_player\", \"kick_off\"])\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(f\"Total observations: {len(complete_data)}\")\n",
    "print(f\"Players in dataset: {complete_data.name_player.nunique()}\")\n",
    "print(f\"Date range: {complete_data.kick_off.min()} to {complete_data.kick_off.max()}\")\n",
    "\n",
    "complete_data.set_index([\"name_player\", \"season_nbr\", \"kick_off\"]).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factor Standardization\n",
    "\n",
    "Before using our factors in the model, we need to standardize them. Why?\n",
    "\n",
    "1. **Numerical stability**: Regression coefficients are easier to interpret when predictors have similar scales\n",
    "2. **Prior specification**: We can use the same prior for all factor coefficients when they're standardized\n",
    "3. **Convergence**: MCMC samplers work better with standardized inputs\n",
    "\n",
    "We use `StandardScaler` to transform numeric factors to have mean 0 and standard deviation 1. The binary `home_pitch` factor doesn't need standardization—it's already on a sensible scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract numeric factors for standardization\n",
    "factors_numeric_train = complete_data[factors_numeric]\n",
    "\n",
    "# Standardize numeric factors\n",
    "scaler = StandardScaler()\n",
    "factors_numeric_sdz = pd.DataFrame(\n",
    "    scaler.fit_transform(factors_numeric_train), columns=factors_numeric\n",
    ")\n",
    "\n",
    "# Add the non-numeric factor to the standardized DataFrame\n",
    "factors_sdz = factors_numeric_sdz.copy()\n",
    "factors_sdz[\"home_pitch\"] = complete_data[\"home_pitch\"].values\n",
    "\n",
    "# Make sure the order is the same as the PyMC coords later on\n",
    "factors_sdz = factors_sdz[factors]\n",
    "\n",
    "print(\"Standardized factors summary:\")\n",
    "print(factors_sdz.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization\n",
    "\n",
    "Let's examine the distribution of our standardized factors and the goal-scoring outcome variable. This helps us understand the data we're working with and verify our preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "factors_numeric_sdz.plot(\n",
    "    kind=\"hist\", alpha=0.6, bins=20, title=\"Histogram of Standardized Factors\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how both numeric factors are now centered at 0 with similar spreads. This standardization ensures they contribute comparably to the model and helps with MCMC sampling efficiency.\n",
    "\n",
    "Now let's look at the distribution of our outcome variable—goals scored per match:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data[\"goal\"].value_counts(normalize=True).plot(\n",
    "    alpha=0.6, kind=\"bar\", rot=0, title=\"Goal Proportion in Data\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows the typical distribution we expect in soccer: most matches result in no goals for a given player (0), with a smaller proportion where they score (1). This class imbalance is natural—even elite strikers don't score in every match. Our Bernoulli likelihood will handle this appropriately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HSGP Parameters: Modeling Temporal Dynamics\n",
    "\n",
    "### Why Gaussian Processes for Temporal Effects?\n",
    "\n",
    "Player performance varies over time in complex, non-linear ways:\n",
    "- **Injuries**: A player might struggle for weeks after returning from injury\n",
    "- **Form**: Hot streaks and cold spells appear within a season\n",
    "- **Aging**: Young players improve, peak in their prime, then gradually decline\n",
    "- **Fatigue**: Performance might dip late in congested fixture schedules\n",
    "\n",
    "We don't know the exact functional form of these effects. Should form follow a sine wave? A random walk? Something else entirely? \n",
    "\n",
    "**GPs let the data tell us.** Instead of imposing a rigid functional form, GPs flexibly learn smooth patterns from the data while quantifying uncertainty.\n",
    "\n",
    "### The HSGP Approximation\n",
    "\n",
    "We'll use the **Hilbert Space GP (HSGP)** approximation, which makes GP inference computationally tractable for larger datasets. HSGP approximates the GP using a series of basis functions—think of it as expressing a smooth curve using a sum of simple periodic functions (like Fourier series).\n",
    "\n",
    "For more details, see:\n",
    "- [HSGP paper (Riutort-Mayol et al., 2020)](https://arxiv.org/abs/2004.11408)\n",
    "- [PyMC HSGP Basic Tutorial](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/HSGP-Basic.html)\n",
    "- [PyMC HSGP Advanced Tutorial](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/HSGP-Advanced.html)\n",
    "\n",
    "### Three Timescales, Three GPs\n",
    "\n",
    "We'll fit **three separate HSGPs** to capture different temporal patterns:\n",
    "\n",
    "1. **Short-term** (gameday-level, lengthscale ~2-5 matchdays): Quick fluctuations, noise\n",
    "2. **Medium-term** (gameday-level, lengthscale ~15-25 matchdays): Within-season form (hot/cold streaks)\n",
    "3. **Long-term** (season-level, lengthscale ~2-6 seasons): Aging curves (career trajectory)\n",
    "\n",
    "By combining these, we can separate persistent career trends from temporary form fluctuations.\n",
    "\n",
    "### Choosing HSGP Parameters: `m` and `c`\n",
    "\n",
    "HSGP requires specifying two parameters:\n",
    "\n",
    "- **`m`** (number of basis functions): Higher `m` → better approximation of GPs with **small lengthscales**, but higher computational cost\n",
    "- **`c`** (boundary factor): Higher `c` → better approximation of GPs with **large lengthscales**, but may need more `m` to maintain fidelity\n",
    "\n",
    "**Key considerations**:\n",
    "1. Consider the **range of your input data** (e.g., matchdays 1-38, seasons 0-20)\n",
    "2. Consider your **expected lengthscale range** (how quickly do you expect correlation to decay?)\n",
    "3. Make sure predictions won't be affected by **boundary conditions**\n",
    "4. The **first basis vector may be unidentified with the intercept** when `c` is large—we'll drop it with `drop_first=True`\n",
    "\n",
    "Fortunately, PyMC provides `pm.gp.hsgp_approx.approx_hsgp_hyperparams()` to help us choose reasonable `m` and `c` values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Data Range: Matchdays\n",
    "\n",
    "Let's first examine the range of matchdays in our data. In soccer, a full Premier League season has 38 matchdays (each team plays 19 home and 19 away games).\n",
    "\n",
    "This distribution tells us our gameday-scale GPs will receive inputs ranging from 1 to 38."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.matchday.plot(\n",
    "    kind=\"hist\", alpha=0.6, bins=20, title=\"Matchday Distribution\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is relatively uniform across matchdays 1-38, as expected. Each matchday has roughly equal representation in our dataset.\n",
    "\n",
    "### Lengthscale Priors: Encoding Domain Knowledge\n",
    "\n",
    "Before choosing `m` and `c`, we need to specify our expected **lengthscale ranges**. The lengthscale controls how quickly correlation decays—larger lengthscales mean smoother, slower-varying functions.\n",
    "\n",
    "**Short-term GP** (2-5 matchdays): Captures rapid fluctuations and near-term effects like recent injury or a bad week of training.\n",
    "\n",
    "**Medium-term GP** (15-25 matchdays): Captures sustained form changes—the difference between a player in a purple patch versus a goal drought lasting months.\n",
    "\n",
    "We use **maximum entropy priors** via PreliZ to encode these ranges while remaining minimally informative otherwise. This is Bayesian best practice: we specify what we know (reasonable lengthscale bounds from domain knowledge) without imposing unnecessary constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_short_dist, _ = pz.maxent(pz.InverseGamma(), 2, 5)\n",
    "ls_medium_dist, ax = pz.maxent(pz.InverseGamma(), 15, 25)\n",
    "ax.set(title=\"Short/Mid Lengthscales Priors\", xlabel=\"days\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These priors encode our belief that:\n",
    "- Short-term effects decay quickly (2-5 matchdays)\n",
    "- Medium-term form persists longer (15-25 matchdays, roughly half a season)\n",
    "\n",
    "The InverseGamma distribution allows for heavy right tails, acknowledging that lengthscales could be larger than our central range suggests, but we think the specified range is most plausible.\n",
    "\n",
    "### Choosing `m` and `c` for Within-Season GP\n",
    "\n",
    "Now we use PyMC's helper function to choose HSGP parameters:\n",
    "\n",
    "- **`x_range`**: The range of our input data (matchdays 0-38)\n",
    "- **`lengthscale_range`**: Our expected lengthscale bounds (combining short and medium: 5-25 matchdays)\n",
    "- **`cov_func`**: The covariance function we'll use (Matérn52 for smoothness)\n",
    "\n",
    "The function recommends `m` and `c` values that will accurately approximate GPs with these characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_within, c_within = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=[0, complete_data.matchday.max()],\n",
    "    lengthscale_range=[5, 25],  # nbr of matchdays with autocorrelation\n",
    "    cov_func=\"matern52\",  # our covariance function\n",
    ")\n",
    "\n",
    "print(\"Recommended smallest number of basis vectors (m):\", m_within)\n",
    "print(\"Recommended smallest scaling factor (c):\", np.round(c_within, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These recommended values balance approximation accuracy with computational efficiency. With `m` around 50-60 basis functions and `c` around 5-6, we can accurately represent the smooth within-season fluctuations we expect.\n",
    "\n",
    "Now let's examine the season distribution to inform our long-term GP:\n",
    "\n",
    "### Understanding the Data Range: Seasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.season_nbr.plot(\n",
    "    kind=\"hist\", alpha=0.6, bins=20, title=\"Season Distribution\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution is right-skewed: many observations from early seasons (young players), fewer from late career seasons. This is natural—only exceptional players maintain top-level performance for 15+ seasons.\n",
    "\n",
    "### Choosing `m` and `c` for Long-Term (Aging) GP\n",
    "\n",
    "For the long-term GP modeling career trajectories, we expect:\n",
    "\n",
    "- **Lengthscale range**: 2-6 seasons (performance over a few years is correlated)\n",
    "- **Input range**: 0 to max season number in the data\n",
    "\n",
    "A lengthscale of 2-6 seasons makes sense: a player's performance today tells us something about their performance 2-3 years from now, but correlation weakens over longer timescales. This captures the smooth aging curves we expect—young players improving, peak years, and eventual decline.\n",
    "\n",
    "Again, we use the helper function to get appropriate HSGP parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_long_dist, ax = pz.maxent(pz.InverseGamma(), 2, 6)\n",
    "ax.set(title=\"Long Lengthscales Prior\", xlabel=\"season\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_long, c_long = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=[0, complete_data.season_nbr.max()],\n",
    "    lengthscale_range=[2, 6],\n",
    "    cov_func=\"matern52\",\n",
    ")\n",
    "\n",
    "print(\"Recommended smallest number of basis vectors (m):\", m_long)\n",
    "print(\"Recommended smallest scaling factor (c):\", np.round(c_long, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters chosen, we're ready to build our complete hierarchical model with GP components.\n",
    "\n",
    "## Building the PyMC Model\n",
    "\n",
    "Now we're ready to build our hierarchical factor model with GP components. We'll construct it step-by-step, adding one piece at a time.\n",
    "\n",
    "First, let's set up the coordinate system and data containers that will organize our model.\n",
    "\n",
    "### Setting Up Coordinates and Indices\n",
    "\n",
    "PyMC uses a **coordinate system** to organize multi-dimensional parameters. This makes it easy to index and visualize results.\n",
    "\n",
    "We create:\n",
    "- **Categorical indices** for players and gamedays (mapping names/days to integer codes)\n",
    "- **Coordinate dictionary** defining the dimensions our model will use\n",
    "\n",
    "This organizational structure will make our model code cleaner and our posterior analysis much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create categorical indices for players\n",
    "player_idx = pd.Categorical(\n",
    "    complete_data[\"name_player\"], categories=players_ordered\n",
    ").codes\n",
    "\n",
    "# Create categorical indices for gamedays\n",
    "unique_gamedays = complete_data[\"matchday\"].sort_values().unique()\n",
    "gameday_idx = pd.Categorical(\n",
    "    complete_data[\"matchday\"],\n",
    "    categories=unique_gamedays,\n",
    ").codes\n",
    "\n",
    "# Get unique seasons\n",
    "unique_seasons = complete_data[\"season_nbr\"].unique()\n",
    "\n",
    "# Define coordinate system for the model\n",
    "coords = {\n",
    "    \"factor\": factors,\n",
    "    \"gameday\": unique_gamedays,\n",
    "    \"obs_id\": complete_data.index,\n",
    "    \"player\": players_ordered,\n",
    "    \"season\": unique_seasons,\n",
    "    \"timescale\": [\"short\", \"medium\", \"long\"],\n",
    "}\n",
    "\n",
    "print(f\"Number of observations: {len(complete_data)}\")\n",
    "print(f\"Number of players: {len(players_ordered)}\")\n",
    "print(f\"Number of unique gamedays: {len(unique_gamedays)}\")\n",
    "print(f\"Number of unique seasons: {len(unique_seasons)}\")\n",
    "print(f\"Number of factors: {len(factors)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Containers\n",
    "\n",
    "We use `pm.Data()` to create containers for our observed data. This serves two purposes:\n",
    "\n",
    "1. **In-sample**: During model fitting, these hold our training data\n",
    "2. **Out-of-sample**: We can swap in new data later for predictions without rebuilding the model\n",
    "\n",
    "All our key variables are stored here: factors, time indices (gameday, season, player), and the outcome (goals scored)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=coords) as SFM_rlp:\n",
    "    # Data containers\n",
    "    factor_data = pm.Data(\n",
    "        \"factor_data\", factors_sdz.to_numpy(), dims=(\"obs_id\", \"factor\")\n",
    "    )\n",
    "    gameday_id = pm.Data(\"gameday_id\", gameday_idx, dims=\"obs_id\")\n",
    "    player_id = pm.Data(\"player_id\", player_idx, dims=\"obs_id\")\n",
    "    season_id = pm.Data(\n",
    "        \"season_id\", complete_data[\"season_nbr\"].to_numpy(), dims=\"obs_id\"\n",
    "    )\n",
    "    goals_obs = pm.Data(\"goals_obs\", complete_data[\"goal\"].to_numpy(), dims=\"obs_id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Player Effects\n",
    "\n",
    "This is the core of our factor model: **player-specific skill parameters** $\\alpha_i$.\n",
    "\n",
    "We use a **hierarchical prior** (also called partial pooling):\n",
    "\n",
    "$$\\alpha_i \\sim \\text{Normal}(\\mu_{\\text{global}}, \\sigma)$$\n",
    "\n",
    "where:\n",
    "- $\\mu_{\\text{global}}$: Population mean (logit of overall goal rate)\n",
    "- $\\sigma$: Player diversity (how much players vary in skill)\n",
    "\n",
    "**Why hierarchical?** This structure:\n",
    "1. **Shares information** across players (helps when data is limited for some players)\n",
    "2. **Learns diversity** from the data (how much do players actually differ?)\n",
    "3. **Regularizes** individual estimates (extreme values get shrunk toward the mean)\n",
    "\n",
    "The prior on $\\sigma$ (player diversity) comes from domain knowledge: we expect moderate variation in player skill, not extreme differences. The maximum entropy prior finds the unique distribution that encodes this while remaining minimally informative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "player_diversity_dist, ax = pz.maxent(pz.Exponential(), 0.1, 2)\n",
    "ax.set(title=\"Player Diversity Prior\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    sigma = player_diversity_dist.to_pymc(name=\"player_diversity\")\n",
    "    player_effect = pm.Normal(\n",
    "        \"player_effect\",\n",
    "        mu=logit(complete_data[\"goal\"].mean()),\n",
    "        sigma=sigma,\n",
    "        dims=\"player\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Gaussian Processes\n",
    "\n",
    "This is where we build our three-timescale temporal model. Let's break down what's happening:\n",
    "\n",
    "#### 1. GP Hyperparameters\n",
    "\n",
    "**Amplitude** (marginal standard deviation): How much can the GP vary from 0?\n",
    "- We use a **penalized complexity (PC) prior**: Exponential distribution chosen so there's only 1% probability that amplitude exceeds a threshold\n",
    "- This weakly regularizes toward 0 while allowing data to push it higher if needed\n",
    "\n",
    "> **Penalized Complexity Priors** provide a practical way to set informative priors in hierarchical models. They start from a simple base model and measure a richer model's deviation using a Kullback-Leibler-based distance. The prior density falls exponentially with that distance, so the base model (distance = 0) remains most likely and overfitting is discouraged. You only specify a scale parameter by choosing an interpretable upper bound U and a small tail probability α, keeping the prior transparent, reparameterization-invariant, and parsimonious.\n",
    "\n",
    "\n",
    "**Lengthscale**: Controls smoothness (we specified these earlier via maximum entropy priors)\n",
    "\n",
    "#### 2. Covariance Functions\n",
    "\n",
    "We use **Matérn52** kernels for all GPs:\n",
    "- Smoother than Matérn32, rougher than squared exponential\n",
    "- Good default for natural phenomena\n",
    "- Differentiable (unlike Matérn12), allowing gradient-based inference\n",
    "\n",
    "We create **composite kernels**:\n",
    "- `cov_within = cov_short + cov_medium`: Combines fast and slow within-season variation\n",
    "- `cov_long`: Separate kernel for across-season aging\n",
    "\n",
    "**Kernel addition** means these components are independent and additive—within-season form and aging effects combine linearly in logit space.\n",
    "\n",
    "#### 3. HSGP Construction\n",
    "\n",
    "**For within-season GP**: We use `.prior()` method, which creates the HSGP approximation automatically and returns the GP evaluated at our gameday coordinates. We use `drop_first=True` to avoid aliasing with the intercept.\n",
    "\n",
    "**For long-term GP**: We use `.prior_linearized()` to expose the underlying structure of the HSGP approximation. This is a more explicit approach that reveals how the approximation actually works.\n",
    "\n",
    "---\n",
    "\n",
    "##### Understanding `.prior_linearized()`\n",
    "\n",
    "The `.prior_linearized()` method gives us direct access to the **building blocks** of the HSGP approximation:\n",
    "\n",
    "```python\n",
    "phi, sqrt_psd = gp_long.prior_linearized(X=X_seasons)\n",
    "```\n",
    "\n",
    "This returns two components that work together to construct the GP:\n",
    "\n",
    "**1. `phi` (Φ)**: The eigenvector basis matrix (shape: `n × m`)\n",
    "\n",
    "Think of `phi` as a collection of predetermined **basis functions** that we'll use to build smooth curves. These are the **Laplace eigenfunctions**—special functions that solve the eigenvalue problem for the Laplacian operator on your domain. \n",
    "\n",
    "Here's what makes them special:\n",
    "- They're **fixed** and **independent of kernel hyperparameters** (lengthscale, amplitude)\n",
    "- They only depend on the **domain geometry** (the range of your input data)\n",
    "- They're like **Fourier basis functions**, but tailored to your specific domain boundaries\n",
    "- Lower-frequency eigenfunctions capture smooth, long-range patterns\n",
    "- Higher-frequency eigenfunctions capture fine-scale details\n",
    "\n",
    "**2. `sqrt_psd`**: Square root of power spectral density (shape: `m`)\n",
    "\n",
    "The `sqrt_psd` vector tells us **how much weight** to give each basis function. This is where the kernel properties (lengthscale, amplitude) come into play:\n",
    "- It encodes the **smoothness** of your GP (controlled by lengthscale)\n",
    "- It encodes the **magnitude** of variation (controlled by amplitude)\n",
    "- Different kernels have different spectral densities—this is what makes a Matérn52 kernel behave differently from an ExpQuad kernel\n",
    "\n",
    "##### The Mathematical Construction\n",
    "\n",
    "The HSGP approximation expresses the GP as a **weighted sum of basis functions**:\n",
    "\n",
    "$$f(x) \\approx \\sum_{j=1}^{m} \\beta_j \\cdot \\sqrt{S(\\lambda_j)} \\cdot \\phi_j(x)$$\n",
    "\n",
    "Or in matrix notation:\n",
    "\n",
    "$$f = \\Phi (\\beta \\odot \\sqrt{\\text{PSD}})$$\n",
    "\n",
    "where:\n",
    "- $\\Phi$ (`phi`): The basis function matrix—our building blocks\n",
    "- $\\beta$ (`basis_coeffs`): Random coefficients we'll sample—determines the specific function realization\n",
    "- $\\sqrt{\\text{PSD}}$ (`sqrt_psd`): Spectral weights—encodes kernel properties\n",
    "- $\\odot$: Element-wise multiplication\n",
    "\n",
    "##### Non-Centered Parameterization\n",
    "\n",
    "With explicit access to these components, we can implement a **non-centered parameterization**:\n",
    "\n",
    "```python\n",
    "# Step 1: Sample standardized coefficients\n",
    "beta = pm.Normal(\"basis_coeffs_long\", size=m)  # mean=0, sigma=1\n",
    "\n",
    "# Step 2: Construct the GP\n",
    "f_long = phi @ (beta * sqrt_psd)\n",
    "```\n",
    "\n",
    "Let's break down what happens here:\n",
    "\n",
    "1. We sample $m$ **standardized** coefficients from a standard normal distribution\n",
    "2. We scale each coefficient by the corresponding spectral weight: `beta * sqrt_psd`\n",
    "3. We project onto the basis functions: `phi @ (...)`\n",
    "\n",
    "This is mathematically equivalent to directly sampling:\n",
    "```python\n",
    "beta = pm.Normal(\"beta\", sigma=sqrt_psd, size=m)\n",
    "f_long = phi @ beta\n",
    "```\n",
    "\n",
    "but the non-centered version has a crucial advantage for MCMC sampling. \n",
    "\n",
    "In hierarchical models, when parameters depend on hyperparameters, the posterior geometry can develop a \"funnel\" shape—easy to explore at some scales but very difficult at others. The non-centered parameterization **decorrelates** the coefficients from the hyperparameters, making gradient-based samplers (like NUTS) much more efficient\n",
    "\n",
    "---\n",
    "\n",
    "Using `.prior_linearized()` enables several features:\n",
    "\n",
    "1. **Non-centered parameterization**: Improved MCMC efficiency (as we're using here)\n",
    "2. **Flexible distributions**: Want Student-t basis coefficients instead of Normal? Just change the distribution!\n",
    "3. **Custom constraints**: Apply masks or transformations to basis functions\n",
    "4. **Integration**: Easier to combine with other model components (fixed effects, hierarchical structures)\n",
    "5. **Prediction workflows**: Use `pm.Data` and `pm.set_data` for efficient out-of-sample predictions\n",
    "\n",
    "In our model, we use `.prior_linearized()` for the long-term aging curve to demonstrate this construction and to leverage the non-centered parameterization for better sampling efficiency in our hierarchical model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    X_gamedays = pm.Data(\"X_gamedays\", unique_gamedays, dims=\"gameday\")[:, None]\n",
    "    X_seasons = pm.Data(\"X_seasons\", unique_seasons, dims=\"season\")[:, None]\n",
    "\n",
    "    ## PC prior on amplitude\n",
    "    # 1% chance that amplitude > sigmoid(1.1)=0.75%\n",
    "    alpha_scale, upper_scale = 0.01, 1.1\n",
    "    amplitude = pm.Exponential(\n",
    "        \"amplitude\", lam=-np.log(alpha_scale) / upper_scale, dims=\"timescale\"\n",
    "    )\n",
    "    ls = pm.InverseGamma(\n",
    "        \"ls\",\n",
    "        alpha=np.array([ls_short_dist.alpha, ls_medium_dist.alpha, ls_long_dist.alpha]),\n",
    "        beta=np.array([ls_short_dist.beta, ls_medium_dist.beta, ls_long_dist.beta]),\n",
    "        dims=\"timescale\",\n",
    "    )\n",
    "\n",
    "    # cov matrices\n",
    "    cov_short = amplitude[0] ** 2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[0])\n",
    "    cov_medium = amplitude[1] ** 2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[1])\n",
    "    cov_within = cov_short + cov_medium\n",
    "    cov_long = amplitude[2] ** 2 * pm.gp.cov.Matern52(input_dim=1, ls=ls[2])\n",
    "\n",
    "    ## define GPs\n",
    "    gp_within = pm.gp.HSGP(\n",
    "        m=[m_within],\n",
    "        c=c_within,\n",
    "        cov_func=cov_within,\n",
    "        drop_first=True,\n",
    "    )\n",
    "    f_within = gp_within.prior(\n",
    "        \"f_within\",\n",
    "        X=X_gamedays,\n",
    "        hsgp_coeffs_dims=\"basis_coeffs_within\",\n",
    "        dims=\"gameday\",\n",
    "    )\n",
    "\n",
    "    ## using .prior_linearized, just for demo\n",
    "    gp_long = pm.gp.HSGP(m=[m_long], c=c_long, cov_func=cov_long, drop_first=True)\n",
    "    basis_vectors_long, sqrt_psd_long = gp_long.prior_linearized(X=X_seasons)\n",
    "    basis_coeffs_long = pm.Normal(\n",
    "        \"basis_coeffs_long\", shape=gp_long.n_basis_vectors - 1\n",
    "    )\n",
    "    # non-centered parameterization\n",
    "    f_long = pm.Deterministic(\n",
    "        \"f_long\",\n",
    "        basis_vectors_long @ (basis_coeffs_long * sqrt_psd_long),\n",
    "        dims=\"season\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining Skill and Temporal Effects\n",
    "\n",
    "Now we assemble the complete linear predictor for player skill:\n",
    "\n",
    "$$\\alpha = \\alpha_i[\\text{player}] + f_{\\text{within}}[\\text{gameday}] + f_{\\text{long}}[\\text{season}]$$\n",
    "\n",
    "This combines three components:\n",
    "1. **Player-specific baseline** ($\\alpha_i$): Persistent skill level\n",
    "2. **Within-season variation** ($f_{\\text{within}}$): Form fluctuations within a season\n",
    "3. **Long-term trajectory** ($f_{\\text{long}}$): Career aging curve\n",
    "\n",
    "Notice we use **indexing** to broadcast these components to the observation level:\n",
    "- `player_effect[player_id]`: Maps player-level parameters to each match observation\n",
    "- `f_within[gameday_id]`: Maps gameday-level GP values to each observation\n",
    "- `f_long[season_id]`: Maps season-level GP values to each observation\n",
    "\n",
    "This $\\alpha$ represents player skill **after** accounting for temporal dynamics but **before** accounting for team context (factors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    alpha = pm.Deterministic(\n",
    "        \"alpha\",\n",
    "        player_effect[player_id] + f_within[gameday_id] + f_long[season_id],\n",
    "        dims=\"obs_id\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Team Context: Factor Coefficients\n",
    "\n",
    "The factor coefficients $\\boldsymbol{\\beta}$ quantify how much team context affects scoring probability.\n",
    "\n",
    "We use a **weakly informative prior**: `Normal(0, 0.25)` on the logit scale. This says:\n",
    "- We expect factors to have modest effects (centered at 0)\n",
    "- But we allow data to push them higher if needed\n",
    "- On the probability scale, a coefficient of ±0.25 changes odds by roughly ±28%\n",
    "\n",
    "Since we standardized our factors, all coefficients get the same prior—they're on comparable scales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    slope = pm.Normal(\"slope\", sigma=0.25, dims=\"factor\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood: Bernoulli Regression\n",
    "\n",
    "Finally, we connect our model to the data through the **likelihood**:\n",
    "\n",
    "$$p = \\text{sigmoid}(\\alpha + \\mathbf{X} \\boldsymbol{\\beta})$$\n",
    "$$y \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "This is **logistic regression**:\n",
    "1. Combine player skill ($\\alpha$) and team context ($\\mathbf{X} \\boldsymbol{\\beta}$) linearly\n",
    "2. Transform to probability via sigmoid function\n",
    "3. Model binary outcomes (goal/no goal) with Bernoulli distribution\n",
    "\n",
    "The **sigmoid** (logistic) function maps the real line to $[0,1]$:\n",
    "$$\\text{sigmoid}(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "This ensures our predicted probabilities are valid (between 0 and 1) regardless of how large or small the linear predictor becomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    # regression\n",
    "    p = pm.Deterministic(\n",
    "        \"p\", pm.math.sigmoid(alpha + pm.math.dot(factor_data, slope)), dims=\"obs_id\"\n",
    "    )\n",
    "\n",
    "    # likelihood\n",
    "    pm.Bernoulli(\n",
    "        \"goals_scored\",\n",
    "        p=p,\n",
    "        observed=goals_obs,\n",
    "        dims=\"obs_id\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graphical model shows the complete dependency structure. Notice:\n",
    "\n",
    "- **Hierarchical structure**: Player effects (squares) depend on shared diversity parameter\n",
    "- **GP components**: Temporal effects with hyperparameters (amplitude, lengthscale)\n",
    "- **Observed data**: The shaded `goals_scored` node\n",
    "- **Deterministic nodes**: `alpha` and `p` are computed from other parameters (no additional randomness)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.model_to_graphviz(SFM_rlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior Predictive Checks\n",
    "\n",
    "Before fitting the model, we should **always** check our priors. Prior predictive sampling generates data from the model **before** seeing the actual observations. This helps us verify:\n",
    "\n",
    "1. **Prior makes sense**: Do simulated outcomes cover reasonable ranges?\n",
    "2. **No coding errors**: Does the model run without errors?\n",
    "3. **Computational feasibility**: Will sampling be tractably fast?\n",
    "\n",
    "If prior predictions are wildly unrealistic (e.g., predicting 100% scoring rates or negative probabilities), we need to revise our priors before wasting time on posterior sampling.\n",
    "\n",
    "Let's examine three aspects of our priors: scoring probabilities, predicted goals, and GP temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    idata = pm.sample_prior_predictive(random_seed=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Scoring Probabilities\n",
    "\n",
    "This shows the prior distribution over **scoring rates** for a single observation. \n",
    "\n",
    "What we want to see:\n",
    "- Distribution centered somewhere reasonable (not at 0% or 100%)\n",
    "- Sufficient spread to allow data to move it substantially\n",
    "- No impossible values (outside [0,1])\n",
    "\n",
    "The prior should be **weakly informative**: constraining extreme nonsense while allowing the data to dominate inference. If this distribution looks unreasonable, we'd revise our priors before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_posterior(idata.prior.p.sel(obs_id=0))\n",
    "ax.set(\n",
    "    xlabel=\"scoring rate\",\n",
    "    title=\"Prior scoring rate\",\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Predictive Goals\n",
    "\n",
    "This shows simulated goal outcomes (0 or 1) from our prior. Since we're modeling binary events with relatively low base rates, we expect:\n",
    "- Most predictions to be 0 (no goal)\n",
    "- Some predictions to be 1 (goal scored)\n",
    "- Proportion matching roughly what we know about soccer (goals are infrequent events)\n",
    "\n",
    "If the prior predicted goals in 90% of matches, we'd know something was wrong—that's not realistic for soccer. This check confirms our model structure is sensible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_dist(\n",
    "    idata.prior_predictive.goals_scored,\n",
    "    hist_kwargs={\"alpha\": 0.7},\n",
    ")\n",
    "ax.set(\n",
    "    xlabel=\"Goals scored\", title=\"Prior predictive goals scored\", ylabel=\"Proportion\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior GP Functions\n",
    "\n",
    "Let's prepare the data to visualize our temporal GPs. We need to align the within-season and long-term GPs on a common timeline for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_within_prior = idata.prior[\"f_within\"]\n",
    "f_long_prior = idata.prior[\"f_long\"]\n",
    "\n",
    "index = pd.MultiIndex.from_product(\n",
    "    [unique_seasons, unique_gamedays],\n",
    "    names=[\"season_nbr\", \"gameday\"],\n",
    ")\n",
    "unique_combinations = pd.DataFrame(index=index).reset_index()\n",
    "\n",
    "f_long_prior_aligned = f_long_prior.sel(\n",
    "    season=unique_combinations[\"season_nbr\"].to_numpy()\n",
    ").rename({\"season\": \"timestamp\"})\n",
    "f_long_prior_aligned[\"timestamp\"] = unique_combinations.index\n",
    "\n",
    "f_within_prior_aligned = f_within_prior.sel(\n",
    "    gameday=unique_combinations[\"gameday\"].to_numpy()\n",
    ").rename({\"gameday\": \"timestamp\"})\n",
    "f_within_prior_aligned[\"timestamp\"] = unique_combinations.index\n",
    "\n",
    "f_total_prior = f_long_prior_aligned + f_within_prior_aligned\n",
    "\n",
    "some_draws = rng.choice(f_total_prior.draw, size=20, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These plots visualize what temporal patterns our GP priors allow **before seeing data**:\n",
    "\n",
    "**Top row**: Individual GP components\n",
    "- **Left (Within-season)**: Short and medium timescale variation across matchdays\n",
    "- **Right (Long-term)**: Aging curves across seasons\n",
    "\n",
    "**Bottom (Total)**: Combined effect of all temporal components\n",
    "\n",
    "What to look for:\n",
    "- **Smoothness**: Functions should be continuous, not jumpy (our Matérn52 kernel enforces this)\n",
    "- **Amplitude**: Variation should be reasonable (not swinging wildly between ±100 goals)\n",
    "- **Uncertainty**: Wide credible intervals (we're uncertain before seeing data)\n",
    "- **Mean near 0**: GPs centered at 0 (the player effects capture the baseline)\n",
    "\n",
    "The random draws (dark lines) show example realizations from the prior. The mean (yellow) and HDI (blue) summarize the distribution. Notice how the long-term GP is smoother (larger lengthscale) than the within-season GP (smaller lengthscale)—this matches our prior beliefs about how performance varies at different timescales.\n",
    "\n",
    "After seeing data, these distributions will sharpen considerably, with uncertainty narrowing around the patterns the data supports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AB\n",
    "    CC\n",
    "    \"\"\",\n",
    "    figsize=(12, 7.5),\n",
    "    layout=\"constrained\",\n",
    "    sharey=True,\n",
    ")\n",
    "\n",
    "axes[\"A\"].plot(\n",
    "    f_within_prior.gameday,\n",
    "    az.extract(f_within_prior)[\"f_within\"].isel(sample=0),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    "    label=\"random draws\",\n",
    ")\n",
    "axes[\"A\"].plot(\n",
    "    f_within_prior.gameday,\n",
    "    az.extract(f_within_prior)[\"f_within\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_within_prior.gameday,\n",
    "    y=f_within_prior,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9, \"label\": r\"$83\\%$ HDI\"},\n",
    "    ax=axes[\"A\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"A\"].plot(\n",
    "    f_within_prior.gameday,\n",
    "    f_within_prior.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    "    label=\"Mean\",\n",
    ")\n",
    "axes[\"A\"].set(\n",
    "    xlabel=\"Gameday\", ylabel=\"Nbr Goals\", title=\"Within season variation\\nShort + Medium GP\"\n",
    ")\n",
    "axes[\"A\"].legend(fontsize=10, frameon=True, ncols=3)\n",
    "\n",
    "axes[\"B\"].plot(\n",
    "    f_long_prior.season,\n",
    "    az.extract(f_long_prior)[\"f_long\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_long_prior.season,\n",
    "    y=f_long_prior,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9},\n",
    "    ax=axes[\"B\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"B\"].plot(\n",
    "    f_long_prior.season,\n",
    "    f_long_prior.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    ")\n",
    "axes[\"B\"].set(\n",
    "    xlabel=\"Season\", ylabel=\"Nbr Goals\", title=\"Across seasons variation\\nAging curve\"\n",
    ")\n",
    "\n",
    "axes[\"C\"].plot(\n",
    "    f_total_prior.timestamp,\n",
    "    az.extract(f_total_prior)[\"x\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_total_prior.timestamp,\n",
    "    y=f_total_prior,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9},\n",
    "    ax=axes[\"C\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"C\"].plot(\n",
    "    f_total_prior.timestamp,\n",
    "    f_total_prior.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    ")\n",
    "axes[\"C\"].set(xlabel=\"Timestamp\", ylabel=\"Nbr Goals\", title=\"Total GP\")\n",
    "plt.suptitle(\"Prior GPs\", fontsize=18);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Inference\n",
    "\n",
    "Now we fit the model using MCMC (Markov Chain Monte Carlo) sampling. We use:\n",
    "\n",
    "- **NumPyro's NUTS sampler**: Fast, efficient implementation of the No-U-Turn Sampler\n",
    "- **`target_accept=0.95`**: High acceptance rate for better exploration (at cost of slightly slower sampling)\n",
    "- **Posterior predictive sampling**: Generate predictions for model checking\n",
    "\n",
    "The sampling process:\n",
    "1. **Warmup/tuning**: Algorithm adapts step size and mass matrix\n",
    "2. **Sampling**: Draw from the posterior distribution\n",
    "3. **Posterior predictive**: Generate predictions from the fitted model\n",
    "\n",
    "If we see divergences or other warnings, we'd need to investigate (reparameterize, adjust priors, etc.). A few divergences (2 here) is often acceptable, but many would indicate problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with SFM_rlp:\n",
    "    idata.extend(pm.sample(nuts_sampler=\"nutpie\", random_seed=rng, target_accept=0.95))\n",
    "    idata.extend(\n",
    "        pm.sample_posterior_predictive(\n",
    "            idata, random_seed=rng, compile_kwargs={\"mode\": \"NUMBA\"}\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convergence Diagnostics: Effective Sample Size\n",
    "\n",
    "**Effective Sample Size (ESS)** measures how many independent samples our MCMC chains are equivalent to, accounting for autocorrelation.\n",
    "\n",
    "What we want:\n",
    "- **ESS > 400** (bare minimum for stable estimates)\n",
    "- **ESS > 1000** (good for most purposes)\n",
    "- **Higher is better** (more independent information)\n",
    "\n",
    "This table shows ESS quantiles across all parameters:\n",
    "- **bulk ESS**: For central posterior estimates (mean, median)\n",
    "- **tail ESS**: For extreme quantiles (important for intervals)\n",
    "\n",
    "If ESS is very low (< 100), we'd need to run longer chains or reparameterize the model. The color coding (green = good, red = problematic) makes it easy to spot issues at a glance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess = az.ess(idata.posterior)\n",
    "ess.quantile([0.01, 0.5, 0.99]).to_dataframe().astype(int).style.background_gradient(\n",
    "    axis=None, cmap=\"RdYlGn\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Energy Plot: MCMC Efficiency Check\n",
    "\n",
    "The **energy plot** compares the marginal energy distribution with the energy transition distribution. This diagnostic reveals:\n",
    "\n",
    "**What we want to see**:\n",
    "- **Overlapping distributions** (like we have here)\n",
    "- **Similar shapes** between marginal and transition\n",
    "\n",
    "**Red flags**:\n",
    "- **Separated distributions**: Indicates sampler is struggling to explore efficiently\n",
    "- Often caused by difficult geometry (highly correlated parameters, funnel shapes, etc.)\n",
    "\n",
    "Our energy plot looks good—the distributions overlap well, suggesting NUTS is efficiently exploring the posterior. Combined with our low divergence count, this indicates the sampling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_energy(idata);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting Posterior Parameters\n",
    "\n",
    "Now we examine what the model learned from the data. We'll look at:\n",
    "1. **Factor effects**: How much do team context variables matter?\n",
    "2. **Player skills**: Who are the truly elite scorers after controlling for context?\n",
    "3. **Temporal patterns**: How does performance vary within and across seasons?\n",
    "\n",
    "All comparisons show **prior vs. posterior**—how much did the data update our beliefs?\n",
    "\n",
    "### Factor Coefficients (Team Context Effects)\n",
    "\n",
    "These forest plots show **how much team context matters** for scoring probability.\n",
    "\n",
    "**Interpreting the coefficients**:\n",
    "- **Positive values**: Factor increases scoring probability\n",
    "- **Negative values**: Factor decreases scoring probability  \n",
    "- **Magnitude**: Larger absolute values = stronger effects\n",
    "- **Uncertainty**: Wider intervals = less certain about the effect\n",
    "\n",
    "**What we learn**:\n",
    "- Compare prior (wide, centered at 0) to posterior (narrower, shifted)\n",
    "- If posterior is clearly away from 0, that factor matters\n",
    "- If posterior overlaps 0 heavily, that factor might not be important\n",
    "\n",
    "The right panel zooms in on the posterior alone for easier interpretation. Notice how the data has dramatically updated our beliefs—the posterior intervals are much narrower than the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    A\n",
    "    B\n",
    "    \"\"\",\n",
    "    figsize=(10, 8),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "az.plot_forest(\n",
    "    [idata.prior, idata.posterior],\n",
    "    model_names=[\"Prior\", \"Posterior\"],\n",
    "    var_names=[\"slope\"],\n",
    "    combined=True,\n",
    "    ax=axes[\"A\"],\n",
    ")\n",
    "axes[\"A\"].axvline(c=\"grey\", ls=\"--\")\n",
    "axes[\"A\"].set(title=\"Slopes\")\n",
    "\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=[\"slope\"],\n",
    "    combined=True,\n",
    "    ax=axes[\"B\"],\n",
    ")\n",
    "axes[\"B\"].axvline(c=\"grey\", ls=\"--\")\n",
    "axes[\"B\"].set(title=\"Posterior only\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Player Effects: Who Has True Skill?\n",
    "\n",
    "This is the core output of our factor model: **player-specific skill** ($\\alpha_i$) after controlling for team context and temporal dynamics.\n",
    "\n",
    "**Interpreting player effects**:\n",
    "- **Higher values**: Better scorers (all else equal)\n",
    "- **Separation**: If credible intervals don't overlap, we're confident one player is better\n",
    "- **Shrinkage**: The hierarchical prior pulls extreme estimates toward the population mean (dashed line)\n",
    "- **Uncertainty**: Wider intervals for players with less data\n",
    "\n",
    "**What makes this compelling**:\n",
    "- We've controlled for home advantage, team quality, opponent strength, etc.\n",
    "- We've separated persistent skill from temporary form and aging\n",
    "- These are **skill estimates**, not just raw goal counts\n",
    "\n",
    "Compare the prior (all players look similar) to the posterior (clear differentiation). The posterior reveals a hierarchy of talent that the prior didn't assume—this came from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AB\n",
    "    \"\"\",\n",
    "    figsize=(12, 8),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "players_mean = idata.posterior.player_effect.mean()\n",
    "az.plot_forest(\n",
    "    [idata.prior, idata.posterior],\n",
    "    model_names=[\"Prior\", \"Posterior\"],\n",
    "    var_names=\"player_effect\",\n",
    "    combined=True,\n",
    "    ax=axes[\"A\"],\n",
    ")\n",
    "axes[\"A\"].axvline(x=players_mean, c=\"grey\", ls=\"--\")\n",
    "axes[\"A\"].set(title=\"Player effects\")\n",
    "\n",
    "az.plot_forest(\n",
    "    idata,\n",
    "    var_names=\"player_effect\",\n",
    "    combined=True,\n",
    "    ax=axes[\"B\"],\n",
    ")\n",
    "axes[\"B\"].axvline(x=players_mean, c=\"grey\", ls=\"--\")\n",
    "axes[\"B\"].set(title=\"Posterior only\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Predictive Checks\n",
    "\n",
    "After fitting, we need to verify the model **describes the data well**. Posterior predictive checks compare:\n",
    "- **Observed data**: What actually happened\n",
    "- **Posterior predictions**: What the fitted model predicts\n",
    "\n",
    "**What we want**:\n",
    "- Predicted distribution overlaps observed data\n",
    "- Model captures key features of the data\n",
    "- No systematic discrepancies\n",
    "\n",
    "If predictions systematically miss the data (e.g., always overpredicting), we'd need to revise the model structure.\n",
    "\n",
    "### Prior vs. Posterior Updating: Scoring Rates\n",
    "\n",
    "This plot shows how our beliefs about scoring rates changed after seeing the data:\n",
    "\n",
    "- **Prior** (blue): Before seeing data, wide range of plausible scoring rates\n",
    "- **Posterior** (orange): After seeing data, a narrower distribution\n",
    "\n",
    "**What this tells us**:\n",
    "1. **Learning occurred**: The posterior is more concentrated than the prior\n",
    "2. **Data dominated**: The posterior location is driven by actual scoring rates, not our prior\n",
    "3. **Uncertainty reduced**: We're more confident about typical scoring rates after seeing data\n",
    "\n",
    "This is Bayesian updating in action—starting with weak beliefs, then letting data refine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_dist(idata.prior.p, label=\"Prior\")\n",
    "az.plot_dist(idata.posterior.p, ax=ax, color=\"C1\", label=\"Posterior\")\n",
    "ax.set(title=\"Scoring rate updating\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = az.plot_ppc(idata)\n",
    "ax.set(xlabel=\"Goals scored\", title=\"Posterior Predictive Checks\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Dynamics: Performance Evolution\n",
    "\n",
    "Now we examine what the GPs learned about temporal patterns. These are the **most interesting outputs** for understanding player careers:\n",
    "\n",
    "- **Within-season**: How does form fluctuate during a season?\n",
    "- **Across-season**: What do aging curves look like?\n",
    "- **Combined**: The total temporal effect on scoring\n",
    "\n",
    "Remember: These patterns are **after controlling for** player-specific baselines and team context. They capture pure temporal variation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_long_post = idata.posterior[\"f_long\"]\n",
    "f_within_post = idata.posterior[\"f_within\"]\n",
    "\n",
    "f_long_post_aligned = f_long_post.sel(\n",
    "    season=unique_combinations[\"season_nbr\"].to_numpy()\n",
    ").rename({\"season\": \"timestamp\"})\n",
    "f_long_post_aligned[\"timestamp\"] = unique_combinations.index\n",
    "\n",
    "f_within_post_aligned = f_within_post.sel(\n",
    "    gameday=unique_combinations[\"gameday\"].to_numpy()\n",
    ").rename({\"gameday\": \"timestamp\"})\n",
    "f_within_post_aligned[\"timestamp\"] = unique_combinations.index\n",
    "\n",
    "f_total_post = f_long_post_aligned + f_within_post_aligned\n",
    "\n",
    "some_draws = rng.choice(4000, size=20, replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior GP Functions: What Did We Learn?\n",
    "\n",
    "Compare these posterior GPs to the prior GPs we saw earlier:\n",
    "\n",
    "**Within-season variation** (top left):\n",
    "- **Much narrower uncertainty**: Data strongly constrains the pattern\n",
    "- **Learned structure**: Notice any systematic trends? (e.g., performance dip in late season)\n",
    "- **Form fluctuations**: Deviation from 0 shows matchday-to-matchday changes\n",
    "\n",
    "**Aging curves** (top right):\n",
    "- **Career trajectory**: How performance evolves across seasons\n",
    "- **Peak identification**: Where is the maximum? When do players peak?\n",
    "- **Decline phase**: Do we see late-career decline?\n",
    "\n",
    "**Total temporal effect** (bottom):\n",
    "- **Combined dynamics**: All temporal variation in one view\n",
    "- **Magnitude**: How big are these effects compared to player baselines?\n",
    "\n",
    "**Key insight**: The posterior uncertainty (HDI) is **much narrower** than the prior. The data has told us a great deal about temporal patterns. The smooth curves reflect our Matérn52 kernel—we get continuous, plausible performance trajectories rather than jumpy, unrealistic patterns.\n",
    "\n",
    "This is the power of GPs: we didn't specify \"performance follows a parabola\" or \"form varies sinusoidally\"—we let the data discover the functional form while respecting smoothness constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axes = plt.subplot_mosaic(\n",
    "    \"\"\"\n",
    "    AB\n",
    "    CC\n",
    "    \"\"\",\n",
    "    figsize=(12, 7.5),\n",
    "    layout=\"constrained\",\n",
    ")\n",
    "\n",
    "axes[\"A\"].plot(\n",
    "    f_within_post.gameday,\n",
    "    az.extract(f_within_post)[\"f_within\"].isel(sample=0),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    "    label=\"random draws\",\n",
    ")\n",
    "axes[\"A\"].plot(\n",
    "    f_within_post.gameday,\n",
    "    az.extract(f_within_post)[\"f_within\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_within_post.gameday,\n",
    "    y=f_within_post,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9, \"label\": r\"$83\\%$ HDI\"},\n",
    "    ax=axes[\"A\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"A\"].plot(\n",
    "    f_within_post.gameday,\n",
    "    f_within_post.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    "    label=\"Mean\",\n",
    ")\n",
    "axes[\"A\"].set(\n",
    "    xlabel=\"Gameday\", ylabel=\"Nbr Goals\", title=\"Within season variation\\nShort + Medium GP\"\n",
    ")\n",
    "axes[\"A\"].legend(fontsize=10, frameon=True, ncols=3)\n",
    "\n",
    "axes[\"B\"].plot(\n",
    "    f_long_post.season,\n",
    "    az.extract(f_long_post)[\"f_long\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_long_post.season,\n",
    "    y=f_long_post,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9},\n",
    "    ax=axes[\"B\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"B\"].plot(\n",
    "    f_long_post.season,\n",
    "    f_long_post.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    ")\n",
    "axes[\"B\"].set(\n",
    "    xlabel=\"Season\", ylabel=\"Nbr Goals\", title=\"Across seasons variation\\nAging curve\"\n",
    ")\n",
    "\n",
    "axes[\"C\"].plot(\n",
    "    f_total_post.timestamp,\n",
    "    az.extract(f_total_post)[\"x\"].isel(sample=some_draws),\n",
    "    color=\"#70133A\",\n",
    "    alpha=0.3,\n",
    "    lw=1.5,\n",
    ")\n",
    "az.plot_hdi(\n",
    "    x=f_total_post.timestamp,\n",
    "    y=f_total_post,\n",
    "    hdi_prob=0.83,\n",
    "    color=\"#AAC4E6\",\n",
    "    fill_kwargs={\"alpha\": 0.9},\n",
    "    ax=axes[\"C\"],\n",
    "    smooth=False,\n",
    ")\n",
    "axes[\"C\"].plot(\n",
    "    f_total_post.timestamp,\n",
    "    f_total_post.mean((\"chain\", \"draw\")),\n",
    "    color=\"#FBE64D\",\n",
    "    lw=2.5,\n",
    ")\n",
    "axes[\"C\"].set(xlabel=\"Timestamp\", ylabel=\"Nbr Goals\", title=\"Total GP\")\n",
    "plt.suptitle(\"Posterior GPs\", fontsize=18);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
