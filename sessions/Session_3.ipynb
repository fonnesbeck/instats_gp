{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_3.ipynb)\n",
    "\n",
    "# Session 3: Scaling with HSGP and Sparse Methods\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Understand the computational bottlenecks of standard Gaussian processes\n",
    "- Apply inducing point methods and sparse approximations to scale GPs to larger datasets\n",
    "- Implement Hilbert Space GP (HSGP) approximations in PyMC\n",
    "- Choose appropriate approximation parameters using helper functions and heuristics\n",
    "- Navigate the trade-offs between approximation fidelity and computational efficiency\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sessions, we explored the foundations of Gaussian processes and built models with various kernels and likelihoods. However, you may have noticed that as datasets grow larger, GP computations become increasingly expensive. The standard GP formulation requires inverting an $n \\times n$ covariance matrix, where $n$ is the number of data points. This operation has $\\mathcal{O}(n^3)$ computational complexity and $\\mathcal{O}(n^2)$ memory requirementsâ€”quickly becoming prohibitive for datasets with thousands of observations.\n",
    "\n",
    "In this session, we'll explore two powerful approaches to overcome these computational barriers: sparse GP approximations using inducing points, and the Hilbert Space GP (HSGP) method. These techniques allow us to apply GP models to much larger datasets while maintaining the flexibility and uncertainty quantification that make GPs so valuable.\n",
    "\n",
    "Think of these methods as strategic compromises: we trade away some exactness in our GP representation to gain massive improvements in speed and scalability. The key question we'll answer throughout this session is: *how do we make this trade-off intelligently?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:= 8675309)\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Understanding the Computational Challenge\n",
    "\n",
    "Before diving into solutions, let's develop intuition for *why* standard GPs become computationally expensive. The bottleneck lies in computing and inverting the covariance matrix.\n",
    "\n",
    "For a GP with $n$ observations, we need to:\n",
    "\n",
    "1. **Compute** the $n \\times n$ covariance matrix $K$ by evaluating the kernel function at all pairs of data points\n",
    "2. **Invert** this matrix (or equivalently, solve a linear system) to compute the marginal likelihood\n",
    "3. **Repeat** these operations at every step during MCMC sampling as hyperparameters change\n",
    "\n",
    "The matrix inversion step dominates the computational cost, scaling as $\\mathcal{O}(n^3)$. This cubic scaling means that doubling your dataset size increases computation time by roughly 8Ã—. For a dataset with 10,000 points, a full GP could take hours or days to fit, making interactive model development essentially impossible.\n",
    "\n",
    "### The Cost of Cubic Scaling\n",
    "\n",
    "To make this concrete, consider what happens as we increase dataset size:\n",
    "\n",
    "- **n=50**: Covariance matrix has 2,500 elements, ~125,000 operations to invert\n",
    "- **n=200**: Covariance matrix has 40,000 elements, ~8 million operations to invert (64Ã— more than n=50)\n",
    "- **n=1,000**: Covariance matrix has 1,000,000 elements, ~1 billion operations to invert (8,000Ã— more than n=50)\n",
    "- **n=10,000**: Covariance matrix has 100,000,000 elements, ~1 trillion operations to invert (8,000,000Ã— more than n=50)\n",
    "\n",
    "And remember: these matrix inversions happen at **every MCMC iteration**. For 4,000 samples across 4 chains, that's 16,000 inversions!\n",
    "\n",
    "This is where approximation methods become essential. Rather than abandoning GPs for large datasets, we can use clever mathematical tricks to reduce computational complexity while retaining most of the modeling flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data for Sparse GP Demonstration\n",
    "\n",
    "We'll create a dataset with 2000 observationsâ€”large enough to make standard GP inference slow, but small enough to allow us to compare against the exact solution. Our data will be drawn from a GP with a MatÃ©rn 5/2 kernel and moderate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for data generation\n",
    "n = 2000\n",
    "ell_true = 1.0\n",
    "eta_true = 3.0\n",
    "sigma_true = 0.5\n",
    "\n",
    "# Generate input locations\n",
    "x = 10 * np.sort(RNG.random(n))\n",
    "\n",
    "# Define true covariance function\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ell_true)\n",
    "\n",
    "# Sample the latent GP function\n",
    "K = cov_func(x[:, None]).eval()\n",
    "K_stable = K + 1e-8 * np.eye(n)  # Add jitter for numerical stability\n",
    "f_true = RNG.multivariate_normal(np.zeros(n), K_stable)\n",
    "\n",
    "# Add observation noise\n",
    "y = f_true + sigma_true * RNG.standard_normal(n)\n",
    "\n",
    "# Create a Polars DataFrame\n",
    "df = pl.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y,\n",
    "    'f_true': f_true\n",
    "})\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=df['x'],\n",
    "    y=df['y'],\n",
    "    mode='markers',\n",
    "    name='Observed data',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=df['x'],\n",
    "    y=df['f_true'],\n",
    "    mode='lines',\n",
    "    name='True latent function',\n",
    "    line=dict(color='dodgerblue', width=2)\n",
    ")).update_layout(\n",
    "    title='Simulated GP Data',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Sparse GPs: The Inducing Point Idea\n",
    "\n",
    "Standard GPs require computing and inverting an $n \\times n$ covariance matrix $K_{nn}$, where $n$ is the number of observations. As we saw earlier, this $\\mathcal{O}(n^3)$ operation becomes prohibitively expensive for large datasets. Sparse GPs solve this by introducing a clever approximation based on **inducing points**.\n",
    "\n",
    "#### The Core Approximation Strategy\n",
    "\n",
    "Instead of modeling correlations between all $n$ observations directly, sparse GPs introduce a smaller set of $m$ **inducing points** (also called pseudo-inputs) at strategic locations $\\mathbf{X}_u$. These inducing points act as an **information bottleneck**: all correlations between observations flow through this compressed representation.\n",
    "\n",
    "**Think of it like this**: Imagine understanding temperature patterns across a country. Rather than measuring correlations between all pairs of cities (expensive), you could:\n",
    "1. Select $m$ strategically-placed weather stations (inducing points)\n",
    "2. Model how each city correlates with these stations\n",
    "3. Infer city-to-city relationships indirectly through the stations\n",
    "\n",
    "This changes the problem from requiring $\\mathcal{O}(n^3)$ operations to just $\\mathcal{O}(nm^2)$ operations, where $m \\ll n$.\n",
    "\n",
    "#### The Mathematical Details\n",
    "\n",
    "For a standard GP, the posterior mean and covariance at test locations $\\mathbf{x}_*$ are:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\boldsymbol{\\mu}_* &= K_{*n} \\underbrace{K_{nn}^{-1}}_{\\mathcal{O}(n^3)} \\mathbf{y} \\\\\n",
    "\\boldsymbol{\\Sigma}_* &= K_{**} - K_{*n} \\underbrace{K_{nn}^{-1}}_{\\mathcal{O}(n^3)} K_{n*}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The bottleneck is inverting the large $n \\times n$ matrix $K_{nn}$. This happens at every MCMC iteration as hyperparameters change.\n",
    "\n",
    "**Sparse GPs avoid this** by factorizing the covariance structure through inducing points. The FITC (Fully Independent Training Conditional) approximation assumes observations are conditionally independent given the inducing point values $\\mathbf{u}$:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{f} \\mid \\mathbf{u}) \\approx \\prod_{i=1}^n p(f_i \\mid \\mathbf{u})\n",
    "$$\n",
    "\n",
    "This leads to the approximate posterior:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\tilde{\\boldsymbol{\\mu}}_* &= K_{*m} \\underbrace{K_{mm}^{-1}}_{\\mathcal{O}(m^3)} K_{mn} \\Lambda^{-1} \\mathbf{y} \\\\\n",
    "\\tilde{\\boldsymbol{\\Sigma}}_* &= K_{**} - K_{*m} \\left(K_{mm}^{-1} - K_{mm}^{-1} \\Sigma_m K_{mm}^{-1}\\right) K_{m*}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where $\\Lambda = \\text{diag}(K_{nn} - K_{nm} K_{mm}^{-1} K_{mn}) + \\sigma^2 I$ is diagonal (cheap to invert).\n",
    "\n",
    "**The key insight**: We only invert the small $m \\times m$ matrix $K_{mm}$, not the huge $n \\times n$ matrix. We compute:\n",
    "- $K_{mm}$: $m \\times m$ covariance between inducing points (invert this: $\\mathcal{O}(m^3)$)\n",
    "- $K_{nm}$: $n \\times m$ covariance between observations and inducing points (no inversion needed)\n",
    "- $\\Lambda$: diagonal, so trivial to invert\n",
    "\n",
    "For our example with $n=2000$ and $m=20$:\n",
    "- Standard GP: $\\mathcal{O}(2000^3) \\approx 8$ billion operations\n",
    "- Sparse GP: $\\mathcal{O}(2000 \\times 20^2 + 20^3) \\approx 800$ thousand operations\n",
    "- **Speedup: ~10,000Ã—**\n",
    "\n",
    "#### What Inducing Points Actually Do\n",
    "\n",
    "Inducing points don't replace your dataâ€”they **summarize** it by playing two complementary roles:\n",
    "\n",
    "1. **As anchor points**: They define strategic locations where the GP explicitly represents function values. The function everywhere else is determined by kernel-based interpolation from these anchors.\n",
    "\n",
    "2. **As a compression mechanism**: Instead of tracking $\\frac{n(n-1)}{2}$ pairwise correlations between observations, the GP only needs:\n",
    "   - $\\frac{m(m-1)}{2}$ correlations between inducing points ($m \\times m$ relationships)\n",
    "   - $n \\times m$ correlations from each observation to inducing points\n",
    "\n",
    "The FITC approximation assumes that once we know the function values at the inducing points, observations become conditionally independent. This is a strong assumption but works well in practice when:\n",
    "- The function is smooth (appropriate kernel)\n",
    "- Inducing points are well-distributed\n",
    "- $m$ is large enough to capture the function's complexity\n",
    "\n",
    "#### Choosing Inducing Point Locations\n",
    "\n",
    "The approximation quality depends critically on where we place the inducing points. A practical strategy is **K-means clustering**:\n",
    "- Places inducing points at cluster centers in the input space\n",
    "- Naturally concentrates them where data is dense (important regions)\n",
    "- Still ensures coverage across the entire domain\n",
    "- Fast and deterministic (given a random seed)\n",
    "\n",
    "For our 2000-observation dataset, we'll use $m=20$ inducing pointsâ€”a 100Ã— compression that still captures the essential structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-means to select inducing points\n",
    "m = 20  # Number of inducing points\n",
    "\n",
    "kmeans = KMeans(n_clusters=m, random_state=RANDOM_SEED, n_init=10)\n",
    "kmeans.fit(x[:, None])\n",
    "Xu = np.sort(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "print(f\"Selected {m} inducing points using K-means\")\n",
    "print(f\"Inducing points span: [{Xu.min():.2f}, {Xu.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where K-means placed our inducing points relative to the data density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize inducing points overlaid on the data scatter plot\n",
    "subsample_idx = RNG.choice(len(df), size=500, replace=False)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Observed data',\n",
    "    marker=dict(size=3, color='#0066cc', opacity=0.3)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=Xu,\n",
    "    y=[0] * m,  # Place at y=0\n",
    "    mode='markers',\n",
    "    name=f'Inducing points (n={m})',\n",
    "    marker=dict(\n",
    "        size=10, \n",
    "        color='#ff6b6b',\n",
    "        line=dict(width=1.5, color='#c92a2a')\n",
    "    )\n",
    ")).update_layout(\n",
    "    title='Data Distribution with K-Means Selected Inducing Points',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    height=450,\n",
    "    showlegend=True,\n",
    "    legend=dict(x=0.02, y=0.98, bgcolor='rgba(255,255,255,0.8)'),\n",
    "    plot_bgcolor='rgba(0,0,0,0)',\n",
    "    hovermode='closest'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Inducing Point Placement\n",
    "\n",
    "The K-means algorithm has distributed the 20 inducing points fairly evenly across the input domain. Since our data is uniformly distributed, this even spacing makes senseâ€”the inducing points provide coverage across the entire range where we'll need to make predictions.\n",
    "\n",
    "Notice that **inducing points don't need to be at actual data locations**. They're auxiliary variables introduced solely to compress the GP's representation. The sparse GP will:\n",
    "1. Learn latent function values at these 20 inducing locations\n",
    "2. Use the kernel to propagate information from inducing points to observations\n",
    "3. Interpolate smoothly from inducing points to any prediction location\n",
    "\n",
    "#### Understanding the Trade-off\n",
    "\n",
    "The approximation quality depends on the interplay between $m$ (number of inducing points) and the function's complexity:\n",
    "\n",
    "- **Too few inducing points ($m$ too small)**: The GP becomes overly smooth, unable to capture rapid variations or fine-scale structure. Information is lost in the compression.\n",
    "\n",
    "- **Too many inducing points ($m$ too large)**: Computational savings diminish as we approach the cost of a full GP. The approximation becomes nearly exact but defeats the purpose.\n",
    "\n",
    "- **Well-chosen $m$**: Captures the essential structure while maintaining computational efficiency. For smooth functions, surprisingly few inducing points often suffice.\n",
    "\n",
    "For our smooth underlying function with $m=20$ inducing points distributed across the domain, we're betting that kernel-based interpolation through these 20 anchors can effectively reconstruct the function between observations. The visualization in the next sections will show whether this bet pays off!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Covariance Approximation\n",
    "\n",
    "The visualization below reveals why sparse GPs work: the low-rank factorization captures nearly all the correlation structure of the full covariance matrix using just $m=20$ inducing points. \n",
    "\n",
    "The approximation error remains small (typically 5-10% relative error) because smooth GPs have covariance matrices that are inherently low-rankâ€”nearby points are highly correlated, while distant points contribute little information. This mathematical property allows us to compress a 2000Ã—2000 matrix (requiring 2 million parameters) down to just 20 strategic locations (requiring 20 parameters) without meaningful loss of fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute full covariance matrix (subset) using the true covariance function\n",
    "K_full = cov_func(x[:, None]).eval()\n",
    "\n",
    "# Compute sparse approximation covariance\n",
    "Xu_viz = Xu[:, None]\n",
    "x_viz_2d = x[:, None]\n",
    "K_mm = cov_func(Xu_viz).eval()\n",
    "K_nm = cov_func(x_viz_2d, Xu_viz).eval()\n",
    "K_sparse_approx = K_nm @ np.linalg.solve(K_mm, K_nm.T)\n",
    "\n",
    "# Create side-by-side heatmaps\n",
    "fig = make_subplots(\n",
    "    rows=1, cols=3,\n",
    "    subplot_titles=(\n",
    "        'Full Covariance',\n",
    "        'Sparse Approximation',\n",
    "        'Approximation Error'\n",
    "    ),\n",
    "    horizontal_spacing=0.1\n",
    ")\n",
    "\n",
    "# Full covariance\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=K_full, colorscale='Viridis', showscale=False,\n",
    "               hovertemplate='i=%{x}, j=%{y}<br>K=%{z:.3f}<extra></extra>'),\n",
    "    row=1, col=1\n",
    ")\n",
    "\n",
    "# Sparse approximation\n",
    "fig.add_trace(\n",
    "    go.Heatmap(z=K_sparse_approx, colorscale='Viridis', showscale=False,\n",
    "               hovertemplate='i=%{x}, j=%{y}<br>KÌƒ=%{z:.3f}<extra></extra>'),\n",
    "    row=1, col=2\n",
    ")\n",
    "\n",
    "# Error\n",
    "error = K_full - K_sparse_approx\n",
    "fig.add_trace(\n",
    "    go.Heatmap(\n",
    "        z=error,\n",
    "        colorscale='RdBu_r',\n",
    "        zmid=0,\n",
    "        showscale=True,\n",
    "        colorbar=dict(title='Error', x=1.0),\n",
    "        hovertemplate='i=%{x}, j=%{y}<br>Error=%{z:.3f}<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=3\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text='Data point index', row=1, col=1)\n",
    "fig.update_xaxes(title_text='Data point index', row=1, col=2)\n",
    "fig.update_xaxes(title_text='Data point index', row=1, col=3)\n",
    "fig.update_yaxes(title_text='Data point index', row=1, col=1)\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Maximum absolute error: {np.abs(error).max():.4f}\")\n",
    "print(f\"Mean absolute error: {np.abs(error).mean():.4f}\")\n",
    "print(f\"Relative error (Frobenius norm): {np.linalg.norm(error) / np.linalg.norm(K_full):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sparse GP Model with FITC\n",
    "\n",
    "Now we'll build our sparse GP model using PyMC's `MarginalApprox` class (the modern replacement for `MarginalSparse`) with the FITC approximation. Notice how the model specification is nearly identical to a standard GPâ€”we just provide the inducing point locations `Xu` and specify the approximation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    # Priors on hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # Sparse GP with FITC approximation\n",
    "    gp = pm.gp.MarginalApprox(cov_func=cov, approx='FITC')\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Marginal likelihood\n",
    "    y_obs = gp.marginal_likelihood(\n",
    "        'y_obs',\n",
    "        X=x[:, None],\n",
    "        Xu=Xu[:, None],\n",
    "        y=y,\n",
    "        sigma=sigma\n",
    "    )\n",
    "    \n",
    "    # Sample posterior\n",
    "    idata_sparse = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Posterior\n",
    "\n",
    "Let's check the posterior distributions of our hyperparameters and verify that sampling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.summary(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plot\n",
    "az.plot_trace(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Sparse GP\n",
    "\n",
    "One of the benefits of the sparse GP approximation is that prediction is also fast. Let's make predictions at a dense grid of test points and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test points\n",
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "# Add conditional distribution to model and sample\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    # Sample posterior predictive\n",
    "    posterior_pred = pm.sample_posterior_predictive(\n",
    "        idata_sparse,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in plot_gp_dist for cleaner visualization\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "subsample_idx = RNG.choice(len(df), size=200, replace=False)\n",
    "\n",
    "# Reshape samples for plot_gp_dist: needs shape (n_samples, n_points)\n",
    "# posterior_predictive has shape (chain, draw, n_points)\n",
    "f_pred_samples = posterior_pred.posterior_predictive['f_pred'].values\n",
    "# Stack chain and draw dimensions: (chain, draw, n_points) -> (chain*draw, n_points)\n",
    "n_chains, n_draws, n_points = f_pred_samples.shape\n",
    "f_pred_samples = f_pred_samples.reshape(n_chains * n_draws, n_points)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot GP distribution with credible intervals\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax,\n",
    "    f_pred_samples,\n",
    "    x_test,\n",
    "    palette='Reds',\n",
    "    plot_samples=False\n",
    ")\n",
    "\n",
    "# Add true function and data\n",
    "ax.plot(x_test, f_true_interp, 'b--', linewidth=2, label='True Function')\n",
    "ax.plot(df['x'][subsample_idx], df['y'][subsample_idx], 'o',\n",
    "        color='gray', alpha=0.5, markersize=3, label='Data (subsample)')\n",
    "\n",
    "# Add inducing points\n",
    "ax.plot(Xu, np.ones(len(Xu)) * ax.get_ylim()[0], 'cx',\n",
    "        markersize=10, markeredgewidth=2, label='Inducing Points')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.set_title('Sparse GP Predictions with FITC Approximation', fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE: Sparse GP with Cherry Blossoms Data\n",
    "\n",
    "Now it's your turn to experiment with sparse GPs using real historical data. The cherry blossoms dataset contains over 1,000 years of recorded bloom dates from Kyoto, Japanâ€”one of the longest phenological records in existence.\n",
    "\n",
    "**Your task**: Apply the sparse GP techniques you've learned to model how cherry blossom bloom timing has changed over the centuries.\n",
    "\n",
    "**Dataset**: The cherry blossoms data (`data/cherry_blossoms.csv`) contains:\n",
    "- `year`: Year (801-2015 CE)\n",
    "- `doy`: Day of year when cherry blossoms bloomed (with some missing values)\n",
    "\n",
    "This dataset is sparse in time (many years have missing observations) and exhibits long-term trends that make it perfect for practicing sparse GP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cherry blossoms data\n",
    "cherry_df = pl.read_csv(\n",
    "    DATA_DIR + 'cherry_blossoms.csv', \n",
    "    separator=';',\n",
    "    null_values=['NA']  # Treat 'NA' strings as null\n",
    ")\n",
    "\n",
    "# Remove rows with missing bloom dates\n",
    "cherry_df = cherry_df.filter(pl.col('doy').is_not_null())\n",
    "\n",
    "# Extract year and day-of-year\n",
    "years = cherry_df['year'].to_numpy().astype(float)\n",
    "doy = cherry_df['doy'].to_numpy().astype(float)\n",
    "\n",
    "print(f\"Cherry Blossoms Dataset: {len(cherry_df)} observations\")\n",
    "print(f\"Year range: {int(years.min())} - {int(years.max())}\")\n",
    "print(f\"Day-of-year range: {int(doy.min())} - {int(doy.max())}\")\n",
    "print(f\"Mean bloom date: Day {doy.mean():.1f} (approximately {doy.mean():.0f} days after Jan 1)\")\n",
    "\n",
    "# Visualize the data\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=years,\n",
    "    y=doy,\n",
    "    mode='markers',\n",
    "    name='Observed bloom dates',\n",
    "    marker=dict(size=5, color='hotpink', opacity=0.7, line=dict(width=0.5, color='darkviolet'))\n",
    ")).update_layout(\n",
    "    title='Cherry Blossom Bloom Dates in Kyoto (827-2015)',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Day of Year',\n",
    "    height=400,\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE INSTRUCTIONS:\n",
    "#\n",
    "# STEP 1: Choose inducing points using K-means\n",
    "# - Try different numbers of inducing points: M = 20, 50, 100\n",
    "# - Use the K-means clustering approach shown earlier\n",
    "# - Visualize where the inducing points are placed\n",
    "#\n",
    "# Prompt suggestion: \"Help me use K-means to select M inducing points from the\n",
    "# cherry blossoms years data and create a visualization showing their placement.\"\n",
    "\n",
    "# STEP 2: Build a sparse GP model with MarginalApprox\n",
    "# - Use a MatÃ©rn 5/2 or ExpQuad kernel (cherry blossoms show smooth long-term trends)\n",
    "# - Consider appropriate priors for lengthscale (think in terms of decades or centuries)\n",
    "# - Use the FITC approximation\n",
    "#\n",
    "# Prompt suggestion: \"Help me build a pm.gp.MarginalApprox model for the cherry\n",
    "# blossoms data with priors suitable for multi-century trends.\"\n",
    "\n",
    "# STEP 3: Make predictions and visualize\n",
    "# - Predict bloom dates across the full time range\n",
    "# - Compare predictions with different numbers of inducing points (M=20 vs M=100)\n",
    "# - Visualize uncertainty (credible intervals)\n",
    "#\n",
    "# Prompt suggestion: \"Help me create predictions from the sparse GP and make a\n",
    "# plotly visualization showing the posterior mean, credible intervals, and data points.\"\n",
    "\n",
    "# STEP 4: Explore the trade-offs\n",
    "# - How does increasing M affect:\n",
    "#   * Approximation quality (smoothness, fit to data)\n",
    "#   * Computation time (sampling speed)\n",
    "#   * Uncertainty estimates\n",
    "# - Can you identify interesting historical patterns (e.g., warming trends)?\n",
    "\n",
    "# YOUR LLM-ASSISTED CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Hilbert Space GP (HSGP) Theory\n",
    "\n",
    "While sparse GPs use inducing points to reduce complexity, the Hilbert Space GP (HSGP) takes a completely different approach: it approximates the GP using a **basis function expansion**. This transforms the non-parametric GP into a parametric model with a fixed number of basis functions, making it compatible with standard MCMC samplers and dramatically improving computational efficiency.\n",
    "\n",
    "The mathematical foundation of HSGP comes from spectral analysis of covariance functions. Any stationary covariance kernel can be represented through its **power spectral density**â€”essentially a Fourier transform that describes the kernel's behavior in frequency space. The HSGP approximation uses a finite set of basis functions (sinusoids) whose coefficients are drawn from a distribution determined by this spectral density.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Think of it this way: instead of defining a function through all pairwise correlations (which requires $n^2$ parameters and $n^3$ operations), HSGP defines it through $m$ basis function coefficients. These basis functions are pre-computed and don't depend on hyperparameters, so we only need to update the coefficients during sampling.\n",
    "\n",
    "The computational complexity drops from $\\mathcal{O}(n^3)$ for exact GPs to $\\mathcal{O}(nm + m)$ for HSGP, where $m$ is the number of basis functions. Even better, HSGP is fully parametricâ€”we can use `pm.set_data` for predictions without explicitly computing conditional distributions. This makes it much easier to integrate an HSGP into your existing PyMC model.\n",
    "\n",
    "Additionally, unlike many other GP approximations, HSGPs can be used anywhere within a model and with any likelihood function. This flexibility is a major advantage over methods like sparse GPs that work best with Gaussian likelihoods.\n",
    "\n",
    "### HSGP Restrictions\n",
    "\n",
    "The HSGP approximation does carry some restrictions:\n",
    "\n",
    "1. It **can only be used with stationary covariance kernels** such as the MatÃ©rn family or ExpQuad. The kernel must implement the `power_spectral_density` method.\n",
    "2. It **does not scale well with input dimension**. HSGP is a good choice for 1D processes (like time series) or 2D spatial processes, but likely not efficient beyond 3 dimensions.\n",
    "3. It **may struggle with very rapidly varying processes**. If the process changes very quickly relative to the domain extent, you may need very large $m$ to accurately represent it.\n",
    "4. **For smaller datasets, the full unapproximated GP may still be more efficient**.\n",
    "\n",
    "### Key Parameters: m and c\n",
    "\n",
    "HSGP approximations are controlled by two parameters:\n",
    "\n",
    "- **m**: The number of basis functions. Larger $m$ gives better approximation quality but increases computational cost. Think of $m$ as the \"resolution\" of your approximationâ€”more basis functions can represent more complex, rapidly-varying patterns. Increasing $m$ helps the HSGP approximate GPs with smaller lengthscales.\n",
    "\n",
    "- **c**: The boundary extension factor. HSGP basis functions are defined on a finite domain $[-L, L]$ where $L = c \\cdot S$ and $S$ is half the range of your centered data. Larger $c$ values help approximate GPs with longer lengthscales and ensure predictions away from data aren't affected by boundary conditions. However, increasing $c$ may require increasing $m$ to compensate for loss of fidelity at smaller lengthscales.\n",
    "\n",
    "The art of using HSGP effectively lies in choosing $m$ and $c$ appropriately for your data and expected lengthscales. Fortunately, PyMC provides a helper function to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing HSGP Basis Functions\n",
    "\n",
    "To build intuition, let's visualize what HSGP basis functions actually look like. These are the sinusoidal building blocks that will be combined to approximate our GP. Notice that we need to center the data firstâ€”this is an important requirement for HSGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "ylim = 0.55\n",
    "axs[0].set_ylim([-ylim, ylim])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_xlabel(\"x (centered)\")\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "L_options = [5.0, 6.0, 20.0]\n",
    "m_options = [3, 3, 5]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    L = L_options[i]\n",
    "    m_val = m_options[i]\n",
    "    \n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(pt.as_tensor([L]), [m_val])\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(\n",
    "        x_grid[:, None],\n",
    "        pt.as_tensor([L]),\n",
    "        eigvals,\n",
    "        [m_val],\n",
    "    ).eval()\n",
    "    \n",
    "    for j in range(phi.shape[1]):\n",
    "        ax.plot(x_grid, phi[:, j])\n",
    "    \n",
    "    ax.set_xticks(np.arange(-5, 6, 5))\n",
    "    \n",
    "    S = 5.0\n",
    "    c = L / S\n",
    "    ax.text(-4.9, -0.45, f\"L = {L}\\nc = {c}\", fontsize=12)\n",
    "    ax.set_title(f\"{m_val} basis functions\")\n",
    "    ax.set_xlabel(\"x (centered)\")\n",
    "\n",
    "axs[0].set_ylabel(\"Basis function value\")\n",
    "plt.suptitle(\"The Effect of Changing L on HSGP Basis Vectors\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Basis Functions\n",
    "\n",
    "These plots reveal critical insights about HSGP basis functions:\n",
    "\n",
    "**Left panel (L=5, c=1.0)**: When $L$ equals the data range, all basis vectors are forced to pinch to zero at the boundaries (at $x=-5$ and $x=5$). This means the HSGP approximation becomes poor near the edges of your data. This is why we need $c > 1$.\n",
    "\n",
    "**Middle panel (L=6, c=1.2)**: With $c=1.2$, the basis functions extend beyond the data range and are no longer forced to zero at the data boundaries. This helps the approximation remain accurate across the entire domain. Values of $c$ around 1.2 are considered the minimum for reasonable approximations.\n",
    "\n",
    "**Right panel (L=20, c=4.0, m=5)**: With larger $L$ or $c$, the basis functions become lower frequency (longer wavelength). Notice how the first basis function (blue) is nearly flatâ€”it's becoming partially unidentifiable with an intercept term. This is why we sometimes need to drop the first basis function, or increase $m$ to compensate.\n",
    "\n",
    "Notice that the basis functions are sinusoids with increasing frequency. Lower-order basis functions capture long-range trends, while higher-order functions capture increasingly rapid oscillations. An HSGP approximation works by taking a weighted sum of these basis functions.\n",
    "\n",
    "The key lessons:\n",
    "- **Increasing $m$ helps approximate GPs with smaller lengthscales** (more basis functions = higher resolution)\n",
    "- **Increasing $c$ or $L$ helps approximate GPs with larger lengthscales** but may require increasing $m$ to maintain fidelity at smaller lengthscales\n",
    "- **Consider where predictions will be made**â€”they also need to be away from the boundary \"pinch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: HSGP Implementation\n",
    "\n",
    "Now let's implement an HSGP model and see it in action. We'll use the same dataset as before for direct comparison with the sparse GP.\n",
    "\n",
    "### Choosing HSGP Parameters\n",
    "\n",
    "PyMC provides a helper function `approx_hsgp_hyperparams` that suggests values for $m$ and $c$ based on:\n",
    "- The range of your input data\n",
    "- The range of lengthscales you expect (from your prior)\n",
    "- The covariance function type\n",
    "\n",
    "These recommendations are based on approximation error bounds derived in the HSGP literature. The heuristics help you choose $c$ large enough to handle the largest lengthscales you might fit, and $m$ large enough to accommodate the smallest lengthscales. Let's use this function to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate m and c\n",
    "x_range = [x.min(), x.max()]\n",
    "lengthscale_range = [0.5, 3.0]  # Based on our prior knowledge\n",
    "\n",
    "m_recommended, c_recommended = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=x_range,\n",
    "    lengthscale_range=lengthscale_range,\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"Recommended m: {m_recommended}\")\n",
    "print(f\"Recommended c: {c_recommended:.2f}\")\n",
    "\n",
    "m_hsgp = m_recommended\n",
    "c_hsgp = c_recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the HSGP Model\n",
    "\n",
    "The HSGP model specification in PyMC is remarkably similar to a standard GP. The key difference is that we use `pm.gp.HSGP` instead of `pm.gp.Latent` or `pm.gp.Marginal`, and we specify the approximation parameters $m$ and $c$. \n",
    "\n",
    "Notice that we use the `.prior` method just like with `pm.gp.Latent`. For basic usage, HSGP can be treated as a drop-in replacement for the standard latent GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_model:\n",
    "\n",
    "    # Priors on hyperparameters \n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP approximation\n",
    "    gp = pm.gp.HSGP(m=[m_hsgp], c=c_hsgp, cov_func=cov)\n",
    "    \n",
    "    # Prior over the latent function\n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    idata_hsgp = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining HSGP Results\n",
    "\n",
    "Let's check sampling diagnostics and posterior distributions for the HSGP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hsgp = az.summary(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")\n",
    "print(summary_hsgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with HSGP\n",
    "\n",
    "One major advantage of HSGP is the ease of prediction. Since it's parametric, we can use the `.conditional` method just like with other GPs. Let's make predictions and visualize the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "with hsgp_model:\n",
    "    f_pred_hsgp = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    posterior_pred_hsgp = pm.sample_posterior_predictive(\n",
    "        idata_hsgp,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use built-in plot_gp_dist for cleaner visualization\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "subsample_idx = RNG.choice(len(df), size=200, replace=False)\n",
    "\n",
    "# Reshape samples for plot_gp_dist: needs shape (n_samples, n_points)\n",
    "# posterior_predictive has shape (chain, draw, n_points)\n",
    "f_pred_samples = posterior_pred_hsgp.posterior_predictive['f_pred'].values\n",
    "# Stack chain and draw dimensions: (chain, draw, n_points) -> (chain*draw, n_points)\n",
    "n_chains, n_draws, n_points = f_pred_samples.shape\n",
    "f_pred_samples = f_pred_samples.reshape(n_chains * n_draws, n_points)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot GP distribution with credible intervals\n",
    "pm.gp.util.plot_gp_dist(\n",
    "    ax,\n",
    "    f_pred_samples,\n",
    "    x_test,\n",
    "    palette='Purples',\n",
    "    plot_samples=False\n",
    ")\n",
    "\n",
    "# Add true function and data\n",
    "ax.plot(x_test, f_true_interp, color='gold', linestyle='--',\n",
    "        linewidth=3, label='True Function')\n",
    "ax.plot(df['x'][subsample_idx], df['y'][subsample_idx], 'o',\n",
    "        color='gray', alpha=0.5, markersize=3, label='Data (subsample)')\n",
    "\n",
    "ax.set_xlabel('x', fontsize=12)\n",
    "ax.set_ylabel('f(x)', fontsize=12)\n",
    "ax.set_title(f'HSGP Fit (m={m_hsgp}, c={c_hsgp:.2f})', fontsize=14)\n",
    "ax.legend()\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the HSGP Fit\n",
    "\n",
    "The HSGP inferred posterior (purple) accurately matches the true underlying GP (gold dashed line). We also see that the credible intervals appropriately capture uncertainty. This demonstrates that even with an approximation using basis functions, we can achieve excellent fit quality.\n",
    "\n",
    "Notice that with recommended parameters from `approx_hsgp_hyperparams`, the approximation is essentially indistinguishable from what an exact GP would produce. The computational cost, however, is dramatically lowerâ€”$\\mathcal{O}(nm)$ instead of $\\mathcal{O}(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing HSGP to Sparse GP\n",
    "\n",
    "Let's directly compare the posterior distributions from the HSGP and sparse GP models using a forest plot, which is ideal for comparing multiple models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(\n",
    "    [idata_sparse, idata_hsgp],\n",
    "    model_names=['Sparse GP', 'HSGP'],\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    combined=True,\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Comparison\n",
    "\n",
    "The posterior distributions from HSGP and sparse GP should be very similar, particularly for the lengthscale and amplitude parameters that control the function's smoothness and scale. Small differences are expected since both are approximations, but substantial disagreement would suggest that one or both approximations is inadequate.\n",
    "\n",
    "Both methods successfully inferred hyperparameters close to the true values (lengthscale=1.0, amplitude=3.0, noise=0.5), demonstrating that either approach can work well for moderate-sized datasets with smooth underlying functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: Advanced HSGP - Centered vs Non-Centered Parameterization\n",
    "\n",
    "An important consideration when using HSGP is choosing between centered and non-centered parameterizations. This is analogous to the choice you make in hierarchical models, and for similar reasons: the correlation structure in the posterior.\n",
    "\n",
    "### When to Use Each Parameterization\n",
    "\n",
    "**Centered parameterization** works better when:\n",
    "- The underlying GP is strongly informed by the data\n",
    "- You have lots of data relative to the lengthscale\n",
    "- The signal-to-noise ratio is high\n",
    "\n",
    "**Non-centered parameterization** (the default) works better when:\n",
    "- The underlying GP is weakly informed by the data  \n",
    "- You have sparse data or large lengthscales\n",
    "- The signal-to-noise ratio is low\n",
    "\n",
    "In our example with 2000 noisy observations, the centered parameterization might actually be better. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_centered:\n",
    "\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP with centered parameterization\n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_hsgp], \n",
    "        c=c_hsgp, \n",
    "        cov_func=cov,\n",
    "        parametrization='centered'  # Key difference!\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    idata_hsgp_centered = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-centered parameterization:\")\n",
    "print(az.summary(idata_hsgp, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])\n",
    "print(\"\\nCentered parameterization:\")\n",
    "print(az.summary(idata_hsgp_centered, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Parameterization Effects\n",
    "\n",
    "Compare the effective sample sizes (ESS) between the two parameterizations. Higher ESS means more efficient samplingâ€”you're getting more independent samples per iteration. For this dataset with strong signal, you may find the centered parameterization provides better ESS.\n",
    "\n",
    "The choice of parameterization doesn't affect what you're learning about the hyperparametersâ€”it only affects how efficiently the sampler explores the posterior. If you find sampling is slow or you see low ESS, try switching parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the HSGP Approximate Gram Matrix\n",
    "\n",
    "Another way to check HSGP fidelity is to directly compare the unapproximated Gram matrix (covariance matrix) $\\mathbf{K}$ to the one resulting from the HSGP approximation:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{K}} = \\Phi \\Lambda \\Phi^T\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the matrix of eigenvectors (basis functions), and $\\Lambda$ has the spectral densities computed at the eigenvalues along the diagonal. Let's visualize this for different values of $m$ and $c$ to see when the approximation starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for clearer visualization\n",
    "n_viz = 100\n",
    "x_viz = np.linspace(0, 10, n_viz)\n",
    "\n",
    "# True GP covariance\n",
    "chosen_ell = 1.5\n",
    "cov_func_viz = pm.gp.cov.Matern52(1, ls=chosen_ell)\n",
    "K_true = cov_func_viz(x_viz[:, None]).eval()\n",
    "\n",
    "# Helper function to calculate HSGP approximate Gram matrix\n",
    "def calculate_K_approx(x_centered, L, m_val, cov_func):\n",
    "    \"\"\"Calculate the HSGP approximate covariance matrix.\"\"\"\n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(L, m_val)\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(x_centered, L, eigvals, m_val)\n",
    "    omega = pt.sqrt(eigvals)\n",
    "    psd = cov_func.power_spectral_density(omega)\n",
    "    return (phi @ pt.diag(psd) @ phi.T).eval()\n",
    "\n",
    "# Center the data\n",
    "x_center = (x_viz.max() + x_viz.min()) / 2.0\n",
    "x_viz_centered = (x_viz - x_center)[:, None]\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharey=True)\n",
    "\n",
    "# True Gram matrix\n",
    "axs[0, 0].imshow(K_true, cmap='inferno', vmin=0, vmax=1)\n",
    "axs[0, 0].set_title(f'True Gram matrix\\nâ„“ = {chosen_ell}')\n",
    "axs[0, 0].set_ylabel('Index')\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "# Various m and c combinations\n",
    "configs = [\n",
    "    ([30], 2.5, 1),\n",
    "    ([15], 2.5, 2),\n",
    "    ([30], 1.2, 3),\n",
    "    ([15], 1.2, 4),\n",
    "    ([5], 2.5, 5),\n",
    "]\n",
    "\n",
    "for m_val, c_val, idx in configs:\n",
    "    row = 0 if idx <= 2 else 1\n",
    "    col = idx if idx <= 2 else idx - 3\n",
    "    \n",
    "    L = pm.gp.hsgp_approx.set_boundary(x_viz_centered, c_val)\n",
    "    K_approx = calculate_K_approx(x_viz_centered, L, m_val, cov_func_viz)\n",
    "    \n",
    "    axs[row, col].imshow(K_approx, cmap='inferno', vmin=0, vmax=1, interpolation='none')\n",
    "    axs[row, col].set_title(f'm = {m_val[0]}, c = {c_val}')\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('Index')\n",
    "    if row == 1:\n",
    "        axs[row, col].set_xlabel('Index')\n",
    "\n",
    "plt.suptitle('HSGP Approximation Quality: Comparing to True Gram Matrix', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gram Matrix Approximations\n",
    "\n",
    "These plots compare approximate Gram matrices to the unapproximated one (top left). The goal is visual similarityâ€”the more alike they look, the better the approximation. Important caveats:\n",
    "\n",
    "- These results are **only relevant for this specific domain and lengthscale** ($\\ell = 1.5$). Different lengthscales will show different approximation quality.\n",
    "- The approximation looks good for $m = 30$ or $m = 15$ with $c=2.5$. The rest show clear differences.\n",
    "- $c=1.2$ is generally too small, regardless of $m$, showing degradation at the boundaries.\n",
    "- Surprisingly, $m=5$, $c=1.2$ can look better than $m=5$, $c=2.5$. When we \"stretch\" the basis to fill a larger domain, we lose fidelity at smaller lengthscales if $m$ is too small.\n",
    "\n",
    "The lesson: **you need to experiment across your range of lengthscales** to find adequate $m$ and $c$ values. Often during prototyping, you can use lower fidelity (smaller $m$) for faster iteration, then dial in higher fidelity once you understand the relevant lengthscales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Heuristics for Choosing m and c\n",
    "\n",
    "In practice, you'll need to infer the lengthscale from data, so HSGP needs to approximate a GP across a range of lengthscales representative of your prior. Based on the research literature (Riutort-Mayol et al., 2020) and the PyMC implementation:\n",
    "\n",
    "1. **Start with `approx_hsgp_hyperparams`**: This function provides data-driven recommendations based on your input range and expected lengthscales. It uses approximation error bounds from the HSGP literature to choose $c$ large enough to handle your largest expected lengthscales and $m$ large enough for your smallest lengthscales.\n",
    "\n",
    "2. **Understand the trade-offs**:\n",
    "   - **Increasing $m$ helps approximate GPs with smaller lengthscales** at the cost of increased computation\n",
    "   - **Increasing $c$ helps approximate GPs with larger lengthscales** but may require increasing $m$ to compensate for loss of fidelity at smaller scales\n",
    "\n",
    "3. **Experiment with your specific problem**: You will need to verify approximation quality across your range of lengthscales. The recommendations from `approx_hsgp_hyperparams` provide a starting point, but you may be able to reduce $m$ for computational savings if your function is smooth, or need to increase it if the process varies rapidly.\n",
    "\n",
    "4. **Check the basis vectors if sampling struggles**: The first eigenvector can become unidentifiable with an intercept when $c$ is large. Consider using the `drop_first` option.\n",
    "\n",
    "5. **Verify approximation quality**: Compare the HSGP approximate Gram matrix to the true Gram matrix (as shown in the visualization above) on a data subset to confirm adequate approximation for your lengthscale range.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help compare HSGP vs standard GP\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def hsgp_vs_full_gp(X, y, m_values=(20, 50, 100), L_factor=1.5):\n",
    "    \"\"\"\n",
    "    Fit HSGP for several m, compare to full GP in time and RMSE.\n",
    "    \n",
    "    Prompt suggestion: \"Help me implement HSGP in PyMC and produce plots of\n",
    "    computation time vs error across different m values.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on a subset of data\n",
    "# Use 500-1000 points for reasonable comparison times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to choose HSGP parameters in one dimension, let's see how these principles extend to spatial modeling with 2D data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6: Advanced HSGP - 2D Spatial Example\n",
    "\n",
    "So far we've focused on one-dimensional examples. HSGP also works well in two dimensions, making it excellent for spatial modeling. Let's apply HSGP to a real-world spatial dataset to see how $m$ and $c$ work in multiple dimensions.\n",
    "\n",
    "### The Walker Lake Dataset\n",
    "\n",
    "We'll use the famous **Walker Lake dataset (Isaaks & Srivastava 1989)**, a classic dataset in spatial statistics. This involves spatial sampling of mineral concentrations across Walker Lake in Nevada. The data consist of spatial coordinates (Xloc, Yloc) in meters and measurements of variable V (concentration in parts per million). The samples are taken regularly over a coarse grid across the entire area, with additional irregular sampling in regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Walker Lake data using Polars\n",
    "# The file has mixed whitespace (tabs and multiple spaces)\n",
    "# Polars doesn't support regex separators, so we read lines and parse with Polars\n",
    "\n",
    "# Read file and clean up whitespace\n",
    "with open(DATA_DIR + 'walker.txt', 'r') as f:\n",
    "    lines = [line.split() for line in f.readlines()[8:] if line.strip()]\n",
    "\n",
    "# Create Polars DataFrame from parsed data\n",
    "walker_data = pl.DataFrame({\n",
    "    'ID': [int(row[0]) for row in lines if len(row) >= 6],\n",
    "    'Xloc': [float(row[1]) for row in lines if len(row) >= 6],\n",
    "    'Yloc': [float(row[2]) for row in lines if len(row) >= 6],\n",
    "    'V': [float(row[3]) for row in lines if len(row) >= 6],\n",
    "    'U': [float(row[4]) for row in lines if len(row) >= 6],\n",
    "    'T': [int(row[5]) for row in lines if len(row) >= 6]\n",
    "}).with_columns(\n",
    "    # Replace missing values (1E31) with None\n",
    "    pl.when(pl.col('V') > 1e30).then(None).otherwise(pl.col('V')).alias('V'),\n",
    "    pl.when(pl.col('U') > 1e30).then(None).otherwise(pl.col('U')).alias('U')\n",
    ").filter(\n",
    "    # Use only observations with valid V measurements\n",
    "    pl.col('V').is_not_null()\n",
    ")\n",
    "\n",
    "# Extract spatial coordinates and V variable\n",
    "X_2d = walker_data[['Xloc', 'Yloc']].to_numpy()\n",
    "y_2d = walker_data['V'].to_numpy()\n",
    "\n",
    "print(f\"Loaded {len(X_2d)} observations from Walker Lake dataset\")\n",
    "print(f\"X ranges: [{X_2d[:, 0].min():.0f}, {X_2d[:, 0].max():.0f}] x [{X_2d[:, 1].min():.0f}, {X_2d[:, 1].max():.0f}] meters\")\n",
    "print(f\"V (concentration) range: [{y_2d.min():.1f}, {y_2d.max():.1f}] ppm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the spatial distribution of the observed concentration data. The samples are irregularly distributed across the Walker Lake area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_2d[:, 0],\n",
    "        y=X_2d[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=y_2d,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title='V (ppm)',\n",
    "                thickness=20,\n",
    "                len=0.7\n",
    "            ),\n",
    "            line=dict(width=0.5, color='white')  \n",
    "        ),\n",
    "        text=[f'V: {v:.1f} ppm' for v in y_2d],\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>%{text}<extra></extra>'\n",
    "    )\n",
    ").update_layout(\n",
    "    title='Walker Lake Mineral Concentration Data',\n",
    "    xaxis=dict(\n",
    "        title='X Location (meters)',\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        mirror=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Y Location (meters)',\n",
    "        scaleanchor='x',\n",
    "        scaleratio=1,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        mirror=False\n",
    "    ),\n",
    "    plot_bgcolor='rgba(240,240,240,0.3)',\n",
    "    height=600,\n",
    "    width=650\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a 2D HSGP Model\n",
    "\n",
    "For 2D HSGPs, we specify $m$ and $c$ as two-element listsâ€”one value per dimension. The total number of basis functions is $m_1 \\times m_2$, so computational cost grows multiplicatively with dimension.\n",
    "\n",
    "The Walker Lake data spans 300 meters in each direction with irregular sampling. We'll use the helper function to determine appropriate HSGP parameters, though we may need to adjust based on the spatial scale of variation in mineral concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate m and c for Walker Lake spatial scale\n",
    "x_range = [X_2d.min(), X_2d.max()]  # Same range for both dimensions\n",
    "lengthscale_range = [10.0, 100.0]  # Expected spatial correlation in meters\n",
    "\n",
    "m_2d, c_2d = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=x_range,\n",
    "    lengthscale_range=lengthscale_range,\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"2D HSGP recommendations: m={m_2d}, c={c_2d:.2f}\")\n",
    "print(f\"Total basis functions: {m_2d**2}\")\n",
    "\n",
    "# Standardize the concentration data for better sampling\n",
    "y_mean = y_2d.mean()\n",
    "y_std = y_2d.std()\n",
    "y_2d_std = (y_2d - y_mean) / y_std\n",
    "\n",
    "with pm.Model() as hsgp_2d_model:\n",
    "\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=0.05)  # Prior centered around 40 meters\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(2, ls=ell)\n",
    "    \n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_2d, m_2d],   # m for each dimension\n",
    "        c=c_2d,           # c applies to both\n",
    "        cov_func=cov\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=X_2d)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y_2d_std)\n",
    "    \n",
    "    idata_2d = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the 2D HSGP Fit\n",
    "\n",
    "Let's create a predicted surface over a regular grid to visualize the spatial pattern learned by the HSGP. We'll interpolate the GP to a fine grid covering the Walker Lake area, then compare the predictions to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction grid\n",
    "n_grid = 40\n",
    "x1_grid = np.linspace(X_2d[:, 0].min(), X_2d[:, 0].max(), n_grid)\n",
    "x2_grid = np.linspace(X_2d[:, 1].min(), X_2d[:, 1].max(), n_grid)\n",
    "X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "X_pred = np.column_stack([X1_mesh.ravel(), X2_mesh.ravel()])\n",
    "\n",
    "# Get posterior predictions\n",
    "with hsgp_2d_model:\n",
    "    f_pred = gp.conditional('f_pred', X_pred)\n",
    "    posterior_pred_2d = pm.sample_posterior_predictive(\n",
    "        idata_2d,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior mean and convert back to original scale\n",
    "f_post_mean = posterior_pred_2d.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_post_mean_original = f_post_mean * y_std + y_mean\n",
    "f_post_grid = f_post_mean_original.reshape(n_grid, n_grid)\n",
    "\n",
    "# Determine shared color range\n",
    "vmin = min(y_2d.min(), f_post_mean_original.min())\n",
    "vmax = max(y_2d.max(), f_post_mean_original.max())\n",
    "\n",
    "# Create side-by-side comparison with shared styling\n",
    "make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('HSGP Posterior Mean Surface', 'Observed Data Locations'),\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.08\n",
    ").add_trace(\n",
    "    go.Heatmap(\n",
    "        z=f_post_grid,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        colorscale='Viridis',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>V: %{z:.1f} ppm<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_2d[:, 0],\n",
    "        y=X_2d[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=y_2d,\n",
    "            colorscale='Viridis',\n",
    "            cmin=vmin,\n",
    "            cmax=vmax,\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=dict(text='V (ppm)', side='right'),\n",
    "                x=1.0,\n",
    "                thickness=15,\n",
    "                len=0.65,\n",
    "                xpad=10\n",
    "            ),\n",
    "            line=dict(width=0.5, color='white')\n",
    "        ),\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>V: %{marker.color:.1f} ppm<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ").update_xaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    row=1, col=1\n",
    ").update_xaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    row=1, col=2\n",
    ").update_yaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1,\n",
    "    row=1, col=1\n",
    ").update_yaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    scaleanchor='x2',\n",
    "    scaleratio=1,\n",
    "    row=1, col=2\n",
    ").update_layout(\n",
    "    title=dict(\n",
    "        text='2D HSGP Fit to Walker Lake Data',\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    height=520,\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    showlegend=False,\n",
    "    margin=dict(l=80, r=120, t=100, b=80),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text='X Location (meters)',\n",
    "            xref='paper', yref='paper',\n",
    "            x=0.5, y=-0.12,\n",
    "            xanchor='center', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=14)\n",
    "        ),\n",
    "        dict(\n",
    "            text='Y Location (meters)',\n",
    "            xref='paper', yref='paper',\n",
    "            x=-0.08, y=0.5,\n",
    "            xanchor='center', yanchor='middle',\n",
    "            textangle=-90,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding 2D HSGP Performance on Real Spatial Data\n",
    "\n",
    "The HSGP successfully learned the spatial structure of mineral concentrations across Walker Lake. Notice several important features:\n",
    "\n",
    "1. **Smooth spatial interpolation**: The predicted surface (left panel) provides smooth estimates even in areas between observations, capturing the underlying spatial pattern while avoiding overfitting to individual noisy measurements.\n",
    "\n",
    "2. **Irregular sampling handled naturally**: Unlike grid-based methods, the HSGP works seamlessly with the irregular sampling pattern (right panel), where some areas have dense measurements and others are sparse.\n",
    "\n",
    "3. **Computational efficiency**: Despite having several hundred observations and predicting on a fine grid (1,600 locations), the HSGP completed fitting and prediction efficiently using basis function expansion.\n",
    "\n",
    "For 2D problems like this spatial dataset, remember that the total number of basis functions is $m_1 \\times m_2$. This is still far more efficient than exact inference, but it shows why HSGP doesn't scale well beyond 3 dimensionsâ€”the basis functions multiply quickly! For higher-dimensional problems, sparse GP methods or other approximations may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE: Exploring HSGP Parameter Choices\n",
    "\n",
    "Now experiment with different HSGP parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your LLM to help explore HSGP parameter choices\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def tune_hsgp_params(X, y, m_grid=(30, 60, 120), c_grid=(1.5, 2.5, 4.0)):\n",
    "    \"\"\"\n",
    "    Grid-search m and c to evaluate speed and accuracy trade-offs.\n",
    "    \n",
    "    Prompt suggestion: \"Help me create a function that runs multiple HSGP fits\n",
    "    over m and c grid and summarizes performance with Plotly heatmaps.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on Walker Lake dataset\n",
    "# Visualize RMSE and sampling time as heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7: Comparing All Approaches\n",
    "\n",
    "We've now explored both sparse GPs and HSGP approximations in detail. Let's bring everything together with a comprehensive comparison that highlights when to use each approach.\n",
    "\n",
    "### Computational Complexity Summary\n",
    "\n",
    "Let's visualize the computational complexity of each approach as a function of dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.logspace(2, 4, 50)  # 100 to 10,000 data points\n",
    "m_sparse = 100  # inducing points\n",
    "m_hsgp = 100    # basis functions\n",
    "\n",
    "# Relative computational cost (arbitrary units)\n",
    "cost_exact = n_values**3 / 1e6  # Scale for visibility\n",
    "cost_sparse = n_values * m_sparse**2 / 1e6\n",
    "cost_hsgp = n_values * m_hsgp / 1e6\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_exact,\n",
    "    mode='lines',\n",
    "    name='Exact GP: O(nÂ³)',\n",
    "    line=dict(color='blue', width=3)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_sparse,\n",
    "    mode='lines',\n",
    "    name=f'Sparse GP: O(nmÂ²), m={m_sparse}',\n",
    "    line=dict(color='green', width=3)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_hsgp,\n",
    "    mode='lines',\n",
    "    name=f'HSGP: O(nm), m={m_hsgp}',\n",
    "    line=dict(color='red', width=3)\n",
    ")).update_layout(\n",
    "    title='Computational Complexity: Exact GP vs. Approximations',\n",
    "    xaxis_title='Number of data points (n)',\n",
    "    yaxis_title='Relative computational cost',\n",
    "    xaxis_type='log',\n",
    "    yaxis_type='log',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Guide: Which Method to Use\n",
    "\n",
    "Here's practical guidance for choosing between methods, based on evidence from PyMC reference materials:\n",
    "\n",
    "**Use Standard (Exact) GP when:**\n",
    "- Dataset is relatively small (typically $n < 1,000$, though this depends on hardware)\n",
    "- You need exact inference without approximation error\n",
    "- You're using **non-stationary kernels** (e.g., `Linear`, `Polynomial`)\n",
    "- Computation time isn't critical\n",
    "\n",
    "**Use Sparse GP (Inducing Points) when:**\n",
    "- Moderate-to-large datasets where exact GP is too slow\n",
    "- Data has **uneven sampling density** (inducing points can be placed strategically)\n",
    "- **Lengthscale is larger than separation between inducing points** (critical condition for good approximation)\n",
    "- You can use domain knowledge or K-means to select good inducing point locations\n",
    "- You're willing to tune the number and placement of inducing points ($m$)\n",
    "\n",
    "**Use HSGP when:**\n",
    "- Large datasets (computational advantage grows with $n$)\n",
    "- Using **stationary kernels only** (MatÃ©rn, ExpQuadâ€”kernel must implement `power_spectral_density` method)\n",
    "- **Input dimension is low** (1-3 dimensions; doesn't scale well beyond 3D)\n",
    "- Process doesn't vary **extremely rapidly** relative to domain extent\n",
    "- You need to integrate the GP into a larger hierarchical model\n",
    "- You need predictions at many new locations (linear scaling advantage)\n",
    "\n",
    "**Practical tips:**\n",
    "\n",
    "1. **Prototyping**: Start with a low-fidelity HSGP (small $m$) for fast iteration. Once you understand the relevant lengthscales, use `pm.gp.hsgp_approx_hsgp_hyperparams()` or manually dial in appropriate $m$ and $c$ values.\n",
    "\n",
    "2. **Approximation quality**: Be aware that low-fidelity approximations may sometimes give more parsimonious fits than high-fidelity versions.\n",
    "\n",
    "3. **Inducing points**: Can be selected via K-means (`pm.gp.util.kmeans_inducing_points()`), as a subset of data, or optimized as model parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-ard-header",
   "metadata": {},
   "source": [
    "## Section 3.8: Automatic Relevance Determination (ARD)\n",
    "\n",
    "So far, we've focused on computational scaling for large datasets using sparse GPs and HSGP. But there's another challenge when working with real-world data: **handling many input features where not all are equally important**.\n",
    "\n",
    "When you have dozens of potential predictors, manually selecting which features to include becomes tedious and risks overfitting. **Automatic Relevance Determination (ARD)** solves this by assigning a separate lengthscale to each input dimension, allowing the GP to automatically learn which features matter.\n",
    "\n",
    "Think of ARD as built-in feature selection: relevant dimensions get small lengthscales (the model pays close attention to changes in these features), while irrelevant dimensions get large lengthscales (the model becomes insensitive to them, effectively removing them from predictions).\n",
    "\n",
    "Let's see ARD in action on a real dataset where we genuinely don't know which features are most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Boston Housing Dataset\n",
    "\n",
    "Instead of synthetic data, let's use a real-world dataset: the Boston housing dataset. This classic dataset contains information about housing in Boston suburbs from the 1970s, with 13 features describing each neighborhood and the median home value.\n",
    "\n",
    "The features include:\n",
    "- **CRIM**: Per capita crime rate by town\n",
    "- **ZN**: Proportion of residential land zoned for large lots\n",
    "- **INDUS**: Proportion of non-retail business acres\n",
    "- **CHAS**: Charles River dummy variable (1 if tract bounds river)\n",
    "- **NOX**: Nitric oxides concentration (pollution)\n",
    "- **RM**: Average number of rooms per dwelling\n",
    "- **AGE**: Proportion of owner-occupied units built before 1940\n",
    "- **DIS**: Weighted distances to employment centers\n",
    "- **RAD**: Index of accessibility to radial highways\n",
    "- **TAX**: Property tax rate\n",
    "- **PTRATIO**: Pupil-teacher ratio\n",
    "- **LSTAT**: Percentage of lower status population\n",
    "- **MEDV**: Median value of owner-occupied homes (target)\n",
    "\n",
    "With ARD, we can fit a GP using **all these features** and let the model automatically discover which ones are most relevant for predicting house prices. Features that matter will get small lengthscales (the model pays close attention), while irrelevant features will get large lengthscales (the model effectively ignores them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing data\n",
    "boston_df = pl.read_csv(DATA_DIR + 'HousingData.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "boston_df = boston_df.drop_nulls()\n",
    "\n",
    "# Extract all features except MEDV (target) and B (excluded for ethical reasons)\n",
    "feature_cols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
    "X_boston = boston_df.select(feature_cols).to_numpy()\n",
    "y_boston = boston_df['MEDV'].to_numpy()\n",
    "\n",
    "# Standardize features for better GP performance\n",
    "X_mean = X_boston.mean(axis=0)\n",
    "X_std = X_boston.std(axis=0)\n",
    "X_boston_std = (X_boston - X_mean) / X_std\n",
    "\n",
    "# Standardize target\n",
    "y_mean = y_boston.mean()\n",
    "y_std = y_boston.std()\n",
    "y_boston_std = (y_boston - y_mean) / y_std\n",
    "\n",
    "print(f\"Boston Housing Dataset: {X_boston_std.shape[0]} observations, {X_boston_std.shape[1]} features\")\n",
    "print(f\"\\nFeature names: {feature_cols}\")\n",
    "print(f\"\\nTarget (MEDV): mean=${y_boston.mean():.1f}k, std=${y_boston.std():.1f}k, range=[${y_boston.min():.1f}k, ${y_boston.max():.1f}k]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the ARD Model\n",
    "\n",
    "Now we'll fit a GP with ARD by giving each of the 12 features its own lengthscale parameter. Notice how we specify `dims=\"features\"` for the lengthscale - this creates a vector of 12 lengthscales, one per feature.\n",
    "\n",
    "The model will simultaneously:\n",
    "1. Learn the overall amplitude and noise level\n",
    "2. Learn a separate lengthscale for each feature\n",
    "3. Use these lengthscales to make predictions\n",
    "\n",
    "**The key insight**: small lengthscales indicate relevance (function changes rapidly with that feature), while large lengthscales indicate irrelevance (covariance nearly constant across that feature's range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"features\": feature_cols}) as ard_model:\n",
    "\n",
    "    # Separate lengthscale for each feature (ARD)\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1, dims=\"features\")\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    \n",
    "    # ExpQuad kernel with ARD\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(input_dim=12, ls=ls)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_boston_std, y=y_boston_std, sigma=sigma)\n",
    "    \n",
    "    # Sample posterior\n",
    "    trace_ard = pm.sample(\n",
    "        500, \n",
    "        tune=500, \n",
    "        nuts_sampler=\"nutpie\", \n",
    "        random_seed=RANDOM_SEED, \n",
    "        chains=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learned Lengthscales by Feature\n",
    "\n",
    "Let's examine which features the model learned are most important. We'll create a bar plot showing the posterior mean lengthscale for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_ard, var_names=['ls'], figsize=(6, 12), combined=True, rope=[0,4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the ARD Results\n",
    "\n",
    "The forest plot above shows the posterior distributions of lengthscales for each feature. Because we **standardized all input features** to have mean 0 and standard deviation 1, these lengthscales are directly comparable across features - this is crucial for ARD interpretation.\n",
    "\n",
    "#### Understanding Lengthscales with Standardized Features\n",
    "\n",
    "A lengthscale represents **the distance you need to move along a feature's axis for function values to become uncorrelated**. With standardized features (mean=0, std=1):\n",
    "\n",
    "- **Highly relevant (< 1)**: Moving just one standard deviation causes substantial decorrelation. The model sees the function changing rapidly with this feature.\n",
    "\n",
    "- **Moderately relevant (1-3)**: The function changes moderately. These features contribute to predictions but with less sensitivity.\n",
    "\n",
    "- **Weakly relevant (> 10)**: The covariance becomes nearly constant across this feature's range. Moving even 10 standard deviations barely affects predictions.\n",
    "\n",
    "- **Effectively irrelevant (> 100)**: The model has learned to ignore this feature entirely. The covariance is nearly independent of this input.\n",
    "\n",
    "#### What to Look for in the Forest Plot\n",
    "\n",
    "Examine the lengthscale distributions shown above:\n",
    "\n",
    "1. **Left side of the plot (small lengthscales)**: Features with tight distributions clustered near 0-2 are the most important predictors. These typically include features like **LSTAT** (% lower status population), **RM** (avg rooms), and **PTRATIO** (pupil-teacher ratio) - all well-known drivers of housing prices.\n",
    "\n",
    "2. **Middle range (lengthscales 3-10)**: Features that contribute moderately to predictions. They matter, but changes in these features have less dramatic effects on house prices.\n",
    "\n",
    "3. **Right side of the plot (large lengthscales > 10)**: Features the model has learned to downweight or ignore. This could indicate true irrelevance or that their information is captured by other correlated features. For example, **CHAS** (Charles River proximity) affects relatively few houses in the dataset.\n",
    "\n",
    "#### The Power of ARD\n",
    "\n",
    "This automatic feature discovery happens **during model fitting** without any manual intervention. We didn't need to:\n",
    "- Manually select features beforehand\n",
    "- Run separate feature importance analyses  \n",
    "- Fit multiple models and compare\n",
    "\n",
    "The learned lengthscales serve a dual purpose: they're both **correlation parameters** (controlling how smooth the GP is along each dimension) and **importance weights** (telling us which features actually matter for predictions). This makes ARD an elegant solution to the feature selection problem in high-dimensional GP regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32c846",
   "metadata": {},
   "source": [
    "### Connecting ARD to Scaling Methods\n",
    "\n",
    "ARD is particularly valuable when combined with the scaling methods we explored earlier. For high-dimensional problems with many features:\n",
    "\n",
    "- **ARD identifies which features matter**, potentially reducing the effective dimensionality\n",
    "- **HSGP** can then efficiently handle the relevant dimensions (though remember HSGP works best for 1-3 dimensions)\n",
    "- **Sparse GPs** can scale to larger datasets while still using ARD kernels\n",
    "\n",
    "The combination of ARD for feature selection and approximation methods for computational scaling allows you to tackle real-world problems with both many observations and many features.\n",
    "\n",
    "This completes our tour of GP scaling methods. You now have the tools to apply GPs to datasets that would be intractable with exact inference, while automatically discovering which features drive your predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
