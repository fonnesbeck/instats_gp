{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/fonnesbeck/instats_gp/blob/main/sessions/Session_3.ipynb)\n",
    "\n",
    "# Session 3: Scaling with HSGP and Sparse Methods\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "- Understand the computational bottlenecks of standard Gaussian processes\n",
    "- Apply inducing point methods and sparse approximations to scale GPs to larger datasets\n",
    "- Implement Hilbert Space GP (HSGP) approximations in PyMC\n",
    "- Choose appropriate approximation parameters using helper functions and heuristics\n",
    "- Navigate the trade-offs between approximation fidelity and computational efficiency\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous sessions, we explored the foundations of Gaussian processes and built models with various kernels and likelihoods. However, you may have noticed that as datasets grow larger, GP computations become increasingly expensive. The standard GP formulation requires inverting an $n \\times n$ covariance matrix, where $n$ is the number of data points. This operation has $\\mathcal{O}(n^3)$ computational complexity and $\\mathcal{O}(n^2)$ memory requirementsâ€”quickly becoming prohibitive for datasets with thousands of observations.\n",
    "\n",
    "In this session, we'll explore two powerful approaches to overcome these computational barriers: sparse GP approximations using inducing points, and the Hilbert Space GP (HSGP) method. These techniques allow us to apply GP models to much larger datasets while maintaining the flexibility and uncertainty quantification that make GPs so valuable.\n",
    "\n",
    "Think of these methods as strategic compromises: we trade away some exactness in our GP representation to gain massive improvements in speed and scalability. The key question we'll answer throughout this session is: *how do we make this trade-off intelligently?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "RNG = np.random.default_rng(RANDOM_SEED:= 8675309)\n",
    "\n",
    "DATA_DIR = \"../data/\"\n",
    "\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"ArviZ version: {az.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.1: Understanding the Computational Challenge\n",
    "\n",
    "Before diving into solutions, let's develop intuition for *why* standard GPs become computationally expensive. The bottleneck lies in computing and inverting the covariance matrix.\n",
    "\n",
    "For a GP with $n$ observations, we need to:\n",
    "\n",
    "1. **Compute** the $n \\times n$ covariance matrix $K$ by evaluating the kernel function at all pairs of data points\n",
    "2. **Invert** this matrix (or equivalently, solve a linear system) to compute the marginal likelihood\n",
    "3. **Repeat** these operations at every step during MCMC sampling as hyperparameters change\n",
    "\n",
    "The matrix inversion step dominates the computational cost, scaling as $\\mathcal{O}(n^3)$. This cubic scaling means that doubling your dataset size increases computation time by roughly 8Ã—. For a dataset with 10,000 points, a full GP could take hours or days to fit, making interactive model development essentially impossible.\n",
    "\n",
    "### The Cost of Cubic Scaling\n",
    "\n",
    "To make this concrete, consider what happens as we increase dataset size:\n",
    "\n",
    "- **n=50**: Covariance matrix has 2,500 elements, ~125,000 operations to invert\n",
    "- **n=200**: Covariance matrix has 40,000 elements, ~8 million operations to invert (64Ã— more than n=50)\n",
    "- **n=1,000**: Covariance matrix has 1,000,000 elements, ~1 billion operations to invert (8,000Ã— more than n=50)\n",
    "- **n=10,000**: Covariance matrix has 100,000,000 elements, ~1 trillion operations to invert (8,000,000Ã— more than n=50)\n",
    "\n",
    "And remember: these matrix inversions happen at **every MCMC iteration**. For 4,000 samples across 4 chains, that's 16,000 inversions!\n",
    "\n",
    "This is where approximation methods become essential. Rather than abandoning GPs for large datasets, we can use clever mathematical tricks to reduce computational complexity while retaining most of the modeling flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Data for Sparse GP Demonstration\n",
    "\n",
    "We'll create a dataset with 2000 observationsâ€”large enough to make standard GP inference slow, but small enough to allow us to compare against the exact solution. Our data will be drawn from a GP with a MatÃ©rn 5/2 kernel and moderate noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set parameters for data generation\n",
    "n = 2000\n",
    "ell_true = 1.0\n",
    "eta_true = 3.0\n",
    "sigma_true = 0.5\n",
    "\n",
    "# Generate input locations\n",
    "x = 10 * np.sort(RNG.random(n))\n",
    "\n",
    "# Define true covariance function\n",
    "cov_func = eta_true**2 * pm.gp.cov.Matern52(1, ell_true)\n",
    "\n",
    "# Sample the latent GP function\n",
    "K = cov_func(x[:, None]).eval()\n",
    "K_stable = K + 1e-8 * np.eye(n)  # Add jitter for numerical stability\n",
    "f_true = RNG.multivariate_normal(np.zeros(n), K_stable)\n",
    "\n",
    "# Add observation noise\n",
    "y = f_true + sigma_true * RNG.standard_normal(n)\n",
    "\n",
    "# Create a Polars DataFrame\n",
    "df = pl.DataFrame({\n",
    "    'x': x,\n",
    "    'y': y,\n",
    "    'f_true': f_true\n",
    "})\n",
    "\n",
    "print(f\"Generated {len(df)} observations\")\n",
    "print(f\"x range: [{df['x'].min():.2f}, {df['x'].max():.2f}]\")\n",
    "print(f\"y range: [{df['y'].min():.2f}, {df['y'].max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Data\n",
    "\n",
    "Let's plot our simulated data. With 2000 points, we'll use transparency to show the density while still seeing the underlying smooth function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_idx = RNG.choice(len(df), size=500, replace=False)\n",
    "subsample_idx = np.sort(subsample_idx)\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Observed data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=df['x'],\n",
    "    y=df['f_true'],\n",
    "    mode='lines',\n",
    "    name='True latent function',\n",
    "    line=dict(color='dodgerblue', width=2)\n",
    ")).update_layout(\n",
    "    title='Simulated GP Data (2000 points)',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='y',\n",
    "    height=400,\n",
    "    showlegend=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing Inducing Points with K-Means\n",
    "\n",
    "A practical strategy for selecting inducing points is to use K-means clustering. This places inducing points at cluster centers in the input space, naturally concentrating them where we have more data while still covering the entire domain.\n",
    "\n",
    "We'll use $m=20$ inducing pointsâ€”a 100Ã— reduction in effective data size. This gives us $\\mathcal{O}(2000 \\times 20^2) = \\mathcal{O}(800\\text{K})$ operations instead of $\\mathcal{O}(2000^3) = \\mathcal{O}(8\\text{B})$ operationsâ€”roughly a 10,000Ã— speedup!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use K-means to select inducing points\n",
    "m = 20  # Number of inducing points\n",
    "\n",
    "kmeans = KMeans(n_clusters=m, random_state=RANDOM_SEED, n_init=10)\n",
    "kmeans.fit(x[:, None])\n",
    "Xu = np.sort(kmeans.cluster_centers_.flatten())\n",
    "\n",
    "print(f\"Selected {m} inducing points using K-means\")\n",
    "print(f\"Inducing points span: [{Xu.min():.2f}, {Xu.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize where K-means placed our inducing points relative to the data density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "\n",
    "# Data histogram\n",
    "fig.add_trace(go.Histogram(\n",
    "    x=df['x'],\n",
    "    nbinsx=50,\n",
    "    name='Data density',\n",
    "    marker=dict(color='lightblue', opacity=0.6),\n",
    "    yaxis='y2'\n",
    "))\n",
    "\n",
    "# Inducing points\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=Xu,\n",
    "    y=np.zeros(m),\n",
    "    mode='markers',\n",
    "    name='Inducing points',\n",
    "    marker=dict(size=10, color='cyan', symbol='x', line=dict(width=2))\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Inducing Point Locations from K-Means',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='',\n",
    "    yaxis2=dict(title='Count', overlaying='y', side='right'),\n",
    "    height=300,\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Inducing Point Placement\n",
    "\n",
    "Notice how K-means distributes the inducing points fairly evenly across the input domain. Since our data is uniformly distributed, the inducing points spread out to cover the range. This is exactly what we want: the inducing points act as strategic summary locations that can represent the entire function.\n",
    "\n",
    "The key insight is that these $m=20$ points don't need to be at data locationsâ€”they're auxiliary variables that help us approximate the GP efficiently. Think of them as anchor points that define a lower-dimensional representation of the function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Sparse GP Model with FITC\n",
    "\n",
    "Now we'll build our sparse GP model using PyMC's `MarginalApprox` class (the modern replacement for `MarginalSparse`) with the FITC approximation. Notice how the model specification is nearly identical to a standard GPâ€”we just provide the inducing point locations `Xu` and specify the approximation type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as sparse_model:\n",
    "    # Priors on hyperparameters\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # Sparse GP with FITC approximation\n",
    "    gp = pm.gp.MarginalApprox(cov_func=cov, approx='FITC')\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Marginal likelihood\n",
    "    y_obs = gp.marginal_likelihood(\n",
    "        'y_obs',\n",
    "        X=x[:, None],\n",
    "        Xu=Xu[:, None],\n",
    "        y=y,\n",
    "        sigma=sigma\n",
    "    )\n",
    "    \n",
    "    # Sample posterior\n",
    "    idata_sparse = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining the Posterior\n",
    "\n",
    "Let's check the posterior distributions of our hyperparameters and verify that sampling was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "summary = az.summary(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trace plot\n",
    "az.plot_trace(\n",
    "    idata_sparse,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Results\n",
    "\n",
    "Look at the trace plots and summary statistics. We should see good mixing (the traces look like \"hairy caterpillars\"), high effective sample sizes (ESS), and $\\hat{R}$ values close to 1.0. These diagnostics tell us that the sampler successfully explored the posterior, despite using only 20 inducing points to represent 2000 data points.\n",
    "\n",
    "The posterior means should be close to our true values (lengthscale=1.0, amplitude=3.0, noise=0.5), though with some uncertainty since we're working with finite data and an approximation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with the Sparse GP\n",
    "\n",
    "One of the benefits of the sparse GP approximation is that prediction is also fast. Let's make predictions at a dense grid of test points and visualize the posterior predictive distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test points\n",
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "# Add conditional distribution to model and sample\n",
    "with sparse_model:\n",
    "    f_pred = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    # Sample posterior predictive\n",
    "    posterior_pred = pm.sample_posterior_predictive(\n",
    "        idata_sparse,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_pred_mean = posterior_pred.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_pred_std = posterior_pred.posterior_predictive['f_pred'].std(dim=['chain', 'draw']).values\n",
    "\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "subsample_idx = RNG.choice(len(df), size=200, replace=False)\n",
    "\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=np.concatenate([x_test, x_test[::-1]]),\n",
    "    y=np.concatenate([f_pred_mean + 2*f_pred_std, (f_pred_mean - 2*f_pred_std)[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(255,0,0,0.2)',\n",
    "    line=dict(color='rgba(255,0,0,0)'),\n",
    "    name='95% Credible Interval',\n",
    "    showlegend=True\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_pred_mean,\n",
    "    mode='lines',\n",
    "    name='Posterior Mean',\n",
    "    line=dict(color='red', width=2)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_true_interp,\n",
    "    mode='lines',\n",
    "    name='True Function',\n",
    "    line=dict(color='dodgerblue', width=2, dash='dash')\n",
    ")).add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=Xu,\n",
    "    y=np.ones(len(Xu)) * (f_pred_mean.min() - 1),\n",
    "    mode='markers',\n",
    "    name='Inducing Points',\n",
    "    marker=dict(size=10, color='cyan', symbol='x', line=dict(width=2))\n",
    ")).update_layout(\n",
    "    title='Sparse GP Predictions with FITC Approximation',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    hovermode='x unified'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Sparse GP Fit\n",
    "\n",
    "This plot reveals several important features of the sparse GP approximation:\n",
    "\n",
    "1. **The posterior mean (red line) closely tracks the true function (blue dashed line)**, demonstrating that just 20 inducing points can effectively represent the smooth underlying pattern in 2000 observations.\n",
    "\n",
    "2. **The credible intervals appropriately capture uncertainty**, widening slightly in regions with fewer nearby inducing points and remaining tight where inducing points are dense.\n",
    "\n",
    "3. **The inducing points (cyan X markers at bottom) are strategically distributed** across the domain, acting as anchor points for the approximation.\n",
    "\n",
    "The key takeaway: we've achieved dramatic computational savings while maintaining excellent approximation quality. For smooth functions and well-placed inducing points, the sparse GP delivers results nearly indistinguishable from the exact GP. The downside of sparse approximations is that they reduce the expressiveness of the GPâ€”reducing the dimension of the covariance matrix effectively reduces the number of eigenvectors that can be used to fit the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE: Sparse GP with Cherry Blossoms Data\n",
    "\n",
    "Now it's your turn to experiment with sparse GPs using real historical data. The cherry blossoms dataset contains over 1,000 years of recorded bloom dates from Kyoto, Japanâ€”one of the longest phenological records in existence.\n",
    "\n",
    "**Your task**: Apply the sparse GP techniques you've learned to model how cherry blossom bloom timing has changed over the centuries.\n",
    "\n",
    "**Dataset**: The cherry blossoms data (`data/cherry_blossoms.csv`) contains:\n",
    "- `year`: Year (801-2015 CE)\n",
    "- `doy`: Day of year when cherry blossoms bloomed (with some missing values)\n",
    "\n",
    "This dataset is sparse in time (many years have missing observations) and exhibits long-term trends that make it perfect for practicing sparse GP methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cherry blossoms data\n",
    "cherry_df = pl.read_csv(\n",
    "    DATA_DIR + 'cherry_blossoms.csv', \n",
    "    separator=';',\n",
    "    null_values=['NA']  # Treat 'NA' strings as null\n",
    ")\n",
    "\n",
    "# Remove rows with missing bloom dates\n",
    "cherry_df = cherry_df.filter(pl.col('doy').is_not_null())\n",
    "\n",
    "# Extract year and day-of-year\n",
    "years = cherry_df['year'].to_numpy().astype(float)\n",
    "doy = cherry_df['doy'].to_numpy().astype(float)\n",
    "\n",
    "print(f\"Cherry Blossoms Dataset: {len(cherry_df)} observations\")\n",
    "print(f\"Year range: {int(years.min())} - {int(years.max())}\")\n",
    "print(f\"Day-of-year range: {int(doy.min())} - {int(doy.max())}\")\n",
    "print(f\"Mean bloom date: Day {doy.mean():.1f} (approximately {doy.mean():.0f} days after Jan 1)\")\n",
    "\n",
    "# Visualize the data\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=years,\n",
    "    y=doy,\n",
    "    mode='markers',\n",
    "    name='Observed bloom dates',\n",
    "    marker=dict(size=5, color='hotpink', opacity=0.7, line=dict(width=0.5, color='darkviolet'))\n",
    ")).update_layout(\n",
    "    title='Cherry Blossom Bloom Dates in Kyoto (827-2015)',\n",
    "    xaxis_title='Year',\n",
    "    yaxis_title='Day of Year',\n",
    "    height=400,\n",
    "    showlegend=False,\n",
    "    plot_bgcolor='white',\n",
    "    xaxis=dict(showgrid=False, zeroline=False),\n",
    "    yaxis=dict(showgrid=False, zeroline=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXERCISE INSTRUCTIONS:\n",
    "#\n",
    "# STEP 1: Choose inducing points using K-means\n",
    "# - Try different numbers of inducing points: M = 20, 50, 100\n",
    "# - Use the K-means clustering approach shown earlier\n",
    "# - Visualize where the inducing points are placed\n",
    "#\n",
    "# Prompt suggestion: \"Help me use K-means to select M inducing points from the\n",
    "# cherry blossoms years data and create a visualization showing their placement.\"\n",
    "\n",
    "# STEP 2: Build a sparse GP model with MarginalApprox\n",
    "# - Use a MatÃ©rn 5/2 or ExpQuad kernel (cherry blossoms show smooth long-term trends)\n",
    "# - Consider appropriate priors for lengthscale (think in terms of decades or centuries)\n",
    "# - Use the FITC approximation\n",
    "#\n",
    "# Prompt suggestion: \"Help me build a pm.gp.MarginalApprox model for the cherry\n",
    "# blossoms data with priors suitable for multi-century trends.\"\n",
    "\n",
    "# STEP 3: Make predictions and visualize\n",
    "# - Predict bloom dates across the full time range\n",
    "# - Compare predictions with different numbers of inducing points (M=20 vs M=100)\n",
    "# - Visualize uncertainty (credible intervals)\n",
    "#\n",
    "# Prompt suggestion: \"Help me create predictions from the sparse GP and make a\n",
    "# plotly visualization showing the posterior mean, credible intervals, and data points.\"\n",
    "\n",
    "# STEP 4: Explore the trade-offs\n",
    "# - How does increasing M affect:\n",
    "#   * Approximation quality (smoothness, fit to data)\n",
    "#   * Computation time (sampling speed)\n",
    "#   * Uncertainty estimates\n",
    "# - Can you identify interesting historical patterns (e.g., warming trends)?\n",
    "\n",
    "# YOUR LLM-ASSISTED CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.3: Hilbert Space GP (HSGP) Theory\n",
    "\n",
    "While sparse GPs use inducing points to reduce complexity, the Hilbert Space GP (HSGP) takes a completely different approach: it approximates the GP using a **basis function expansion**. This transforms the non-parametric GP into a parametric model with a fixed number of basis functions, making it compatible with standard MCMC samplers and dramatically improving computational efficiency.\n",
    "\n",
    "The mathematical foundation of HSGP comes from spectral analysis of covariance functions. Any stationary covariance kernel can be represented through its **power spectral density**â€”essentially a Fourier transform that describes the kernel's behavior in frequency space. The HSGP approximation uses a finite set of basis functions (sinusoids) whose coefficients are drawn from a distribution determined by this spectral density.\n",
    "\n",
    "### Why This Matters\n",
    "\n",
    "Think of it this way: instead of defining a function through all pairwise correlations (which requires $n^2$ parameters and $n^3$ operations), HSGP defines it through $m$ basis function coefficients. These basis functions are pre-computed and don't depend on hyperparameters, so we only need to update the coefficients during sampling.\n",
    "\n",
    "The computational complexity drops from $\\mathcal{O}(n^3)$ for exact GPs to $\\mathcal{O}(nm + m)$ for HSGP, where $m$ is the number of basis functions. Even better, HSGP is fully parametricâ€”we can use `pm.set_data` for predictions without explicitly computing conditional distributions. This makes it much easier to integrate an HSGP into your existing PyMC model.\n",
    "\n",
    "Additionally, unlike many other GP approximations, HSGPs can be used anywhere within a model and with any likelihood function. This flexibility is a major advantage over methods like sparse GPs that work best with Gaussian likelihoods.\n",
    "\n",
    "### HSGP Restrictions\n",
    "\n",
    "The HSGP approximation does carry some restrictions:\n",
    "\n",
    "1. It **can only be used with stationary covariance kernels** such as the MatÃ©rn family or ExpQuad. The kernel must implement the `power_spectral_density` method.\n",
    "2. It **does not scale well with input dimension**. HSGP is a good choice for 1D processes (like time series) or 2D spatial processes, but likely not efficient beyond 3 dimensions.\n",
    "3. It **may struggle with very rapidly varying processes**. If the process changes very quickly relative to the domain extent, you may need very large $m$ to accurately represent it.\n",
    "4. **For smaller datasets, the full unapproximated GP may still be more efficient**.\n",
    "\n",
    "### Key Parameters: m and c\n",
    "\n",
    "HSGP approximations are controlled by two parameters:\n",
    "\n",
    "- **m**: The number of basis functions. Larger $m$ gives better approximation quality but increases computational cost. Think of $m$ as the \"resolution\" of your approximationâ€”more basis functions can represent more complex, rapidly-varying patterns. Increasing $m$ helps the HSGP approximate GPs with smaller lengthscales.\n",
    "\n",
    "- **c**: The boundary extension factor. HSGP basis functions are defined on a finite domain $[-L, L]$ where $L = c \\cdot S$ and $S$ is half the range of your centered data. Larger $c$ values help approximate GPs with longer lengthscales and ensure predictions away from data aren't affected by boundary conditions. However, increasing $c$ may require increasing $m$ to compensate for loss of fidelity at smaller lengthscales.\n",
    "\n",
    "The art of using HSGP effectively lies in choosing $m$ and $c$ appropriately for your data and expected lengthscales. Fortunately, PyMC provides a helper function to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing HSGP Basis Functions\n",
    "\n",
    "To build intuition, let's visualize what HSGP basis functions actually look like. These are the sinusoidal building blocks that will be combined to approximate our GP. Notice that we need to center the data firstâ€”this is an important requirement for HSGP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_grid = np.linspace(-5, 5, 1000)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(14, 4), sharey=True)\n",
    "\n",
    "ylim = 0.55\n",
    "axs[0].set_ylim([-ylim, ylim])\n",
    "axs[1].set_yticks([])\n",
    "axs[1].set_xlabel(\"x (centered)\")\n",
    "axs[2].set_yticks([])\n",
    "\n",
    "L_options = [5.0, 6.0, 20.0]\n",
    "m_options = [3, 3, 5]\n",
    "\n",
    "for i, ax in enumerate(axs):\n",
    "    L = L_options[i]\n",
    "    m_val = m_options[i]\n",
    "    \n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(pt.as_tensor([L]), [m_val])\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(\n",
    "        x_grid[:, None],\n",
    "        pt.as_tensor([L]),\n",
    "        eigvals,\n",
    "        [m_val],\n",
    "    ).eval()\n",
    "    \n",
    "    for j in range(phi.shape[1]):\n",
    "        ax.plot(x_grid, phi[:, j])\n",
    "    \n",
    "    ax.set_xticks(np.arange(-5, 6, 5))\n",
    "    \n",
    "    S = 5.0\n",
    "    c = L / S\n",
    "    ax.text(-4.9, -0.45, f\"L = {L}\\nc = {c}\", fontsize=12)\n",
    "    ax.set_title(f\"{m_val} basis functions\")\n",
    "    ax.set_xlabel(\"x (centered)\")\n",
    "\n",
    "axs[0].set_ylabel(\"Basis function value\")\n",
    "plt.suptitle(\"The Effect of Changing L on HSGP Basis Vectors\", fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Basis Functions\n",
    "\n",
    "These plots reveal critical insights about HSGP basis functions:\n",
    "\n",
    "**Left panel (L=5, c=1.0)**: When $L$ equals the data range, all basis vectors are forced to pinch to zero at the boundaries (at $x=-5$ and $x=5$). This means the HSGP approximation becomes poor near the edges of your data. This is why we need $c > 1$.\n",
    "\n",
    "**Middle panel (L=6, c=1.2)**: With $c=1.2$, the basis functions extend beyond the data range and are no longer forced to zero at the data boundaries. This helps the approximation remain accurate across the entire domain. Values of $c$ around 1.2 are considered the minimum for reasonable approximations.\n",
    "\n",
    "**Right panel (L=20, c=4.0, m=5)**: With larger $L$ or $c$, the basis functions become lower frequency (longer wavelength). Notice how the first basis function (blue) is nearly flatâ€”it's becoming partially unidentifiable with an intercept term. This is why we sometimes need to drop the first basis function, or increase $m$ to compensate.\n",
    "\n",
    "Notice that the basis functions are sinusoids with increasing frequency. Lower-order basis functions capture long-range trends, while higher-order functions capture increasingly rapid oscillations. An HSGP approximation works by taking a weighted sum of these basis functions.\n",
    "\n",
    "The key lessons:\n",
    "- **Increasing $m$ helps approximate GPs with smaller lengthscales** (more basis functions = higher resolution)\n",
    "- **Increasing $c$ or $L$ helps approximate GPs with larger lengthscales** but may require increasing $m$ to maintain fidelity at smaller lengthscales\n",
    "- **Consider where predictions will be made**â€”they also need to be away from the boundary \"pinch\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.4: HSGP Implementation\n",
    "\n",
    "Now let's implement an HSGP model and see it in action. We'll use the same dataset as before for direct comparison with the sparse GP.\n",
    "\n",
    "### Choosing HSGP Parameters\n",
    "\n",
    "PyMC provides a helper function `approx_hsgp_hyperparams` that suggests values for $m$ and $c$ based on:\n",
    "- The range of your input data\n",
    "- The range of lengthscales you expect (from your prior)\n",
    "- The covariance function type\n",
    "\n",
    "These recommendations are based on approximation error bounds derived in the HSGP literature. The heuristics help you choose $c$ large enough to handle the largest lengthscales you might fit, and $m$ large enough to accommodate the smallest lengthscales. Let's use this function to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate m and c\n",
    "x_range = [x.min(), x.max()]\n",
    "lengthscale_range = [0.5, 3.0]  # Based on our prior knowledge\n",
    "\n",
    "m_recommended, c_recommended = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=x_range,\n",
    "    lengthscale_range=lengthscale_range,\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"Recommended m: {m_recommended}\")\n",
    "print(f\"Recommended c: {c_recommended:.2f}\")\n",
    "\n",
    "m_hsgp = m_recommended\n",
    "c_hsgp = c_recommended"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the HSGP Model\n",
    "\n",
    "The HSGP model specification in PyMC is remarkably similar to a standard GP. The key difference is that we use `pm.gp.HSGP` instead of `pm.gp.Latent` or `pm.gp.Marginal`, and we specify the approximation parameters $m$ and $c$. \n",
    "\n",
    "Notice that we use the `.prior` method just like with `pm.gp.Latent`. For basic usage, HSGP can be treated as a drop-in replacement for the standard latent GP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_model:\n",
    "\n",
    "    # Priors on hyperparameters \n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    # Define covariance function\n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP approximation\n",
    "    gp = pm.gp.HSGP(m=[m_hsgp], c=c_hsgp, cov_func=cov)\n",
    "    \n",
    "    # Prior over the latent function\n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    idata_hsgp = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examining HSGP Results\n",
    "\n",
    "Let's check sampling diagnostics and posterior distributions for the HSGP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_hsgp = az.summary(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    round_to=2\n",
    ")\n",
    "print(summary_hsgp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_trace(\n",
    "    idata_hsgp,\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    figsize=(10, 6)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making Predictions with HSGP\n",
    "\n",
    "One major advantage of HSGP is the ease of prediction. Since it's parametric, we can use the `.conditional` method just like with other GPs. Let's make predictions and visualize the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = np.linspace(-0.5, 10.5, 300)\n",
    "\n",
    "with hsgp_model:\n",
    "    f_pred_hsgp = gp.conditional('f_pred', x_test[:, None])\n",
    "    \n",
    "    posterior_pred_hsgp = pm.sample_posterior_predictive(\n",
    "        idata_hsgp,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_hsgp_mean = posterior_pred_hsgp.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_hsgp_std = posterior_pred_hsgp.posterior_predictive['f_pred'].std(dim=['chain', 'draw']).values\n",
    "\n",
    "# Plot HSGP results\n",
    "fig = go.Figure()\n",
    "\n",
    "# Credible interval\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=np.concatenate([x_test, x_test[::-1]]),\n",
    "    y=np.concatenate([f_hsgp_mean + 2*f_hsgp_std, (f_hsgp_mean - 2*f_hsgp_std)[::-1]]),\n",
    "    fill='toself',\n",
    "    fillcolor='rgba(139,0,139,0.2)',\n",
    "    line=dict(color='rgba(139,0,139,0)'),\n",
    "    name='95% Credible Interval',\n",
    "    showlegend=True\n",
    "))\n",
    "\n",
    "# Posterior mean\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_hsgp_mean,\n",
    "    mode='lines',\n",
    "    name='HSGP Posterior Mean',\n",
    "    line=dict(color='darkviolet', width=2)\n",
    "))\n",
    "\n",
    "# True function\n",
    "f_true_interp = np.interp(x_test, x, f_true)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=x_test,\n",
    "    y=f_true_interp,\n",
    "    mode='lines',\n",
    "    name='True Function',\n",
    "    line=dict(color='gold', width=3, dash='dash')\n",
    "))\n",
    "\n",
    "# Data subsample\n",
    "subsample_idx = RNG.choice(len(df), size=200, replace=False)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['x'][subsample_idx],\n",
    "    y=df['y'][subsample_idx],\n",
    "    mode='markers',\n",
    "    name='Data (subsample)',\n",
    "    marker=dict(size=3, color='gray', opacity=0.5)\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title=f'HSGP Fit (m={m_hsgp}, c={c_hsgp:.2f})',\n",
    "    xaxis_title='x',\n",
    "    yaxis_title='f(x)',\n",
    "    height=500,\n",
    "    showlegend=False,\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the HSGP Fit\n",
    "\n",
    "The HSGP inferred posterior (purple) accurately matches the true underlying GP (gold dashed line). We also see that the credible intervals appropriately capture uncertainty. This demonstrates that even with an approximation using basis functions, we can achieve excellent fit quality.\n",
    "\n",
    "Notice that with recommended parameters from `approx_hsgp_hyperparams`, the approximation is essentially indistinguishable from what an exact GP would produce. The computational cost, however, is dramatically lowerâ€”$\\mathcal{O}(nm)$ instead of $\\mathcal{O}(n^3)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing HSGP to Sparse GP\n",
    "\n",
    "Let's directly compare the posterior distributions from the HSGP and sparse GP models using a forest plot, which is ideal for comparing multiple models side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(\n",
    "    [idata_sparse, idata_hsgp],\n",
    "    model_names=['Sparse GP', 'HSGP'],\n",
    "    var_names=['ell', 'eta', 'sigma'],\n",
    "    combined=True,\n",
    "    figsize=(10, 5)\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Comparison\n",
    "\n",
    "The posterior distributions from HSGP and sparse GP should be very similar, particularly for the lengthscale and amplitude parameters that control the function's smoothness and scale. Small differences are expected since both are approximations, but substantial disagreement would suggest that one or both approximations is inadequate.\n",
    "\n",
    "Both methods successfully inferred hyperparameters close to the true values (lengthscale=1.0, amplitude=3.0, noise=0.5), demonstrating that either approach can work well for moderate-sized datasets with smooth underlying functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.5: Advanced HSGP - Centered vs Non-Centered Parameterization\n",
    "\n",
    "An important consideration when using HSGP is choosing between centered and non-centered parameterizations. This is analogous to the choice you make in hierarchical models, and for similar reasons: the correlation structure in the posterior.\n",
    "\n",
    "### When to Use Each Parameterization\n",
    "\n",
    "**Centered parameterization** works better when:\n",
    "- The underlying GP is strongly informed by the data\n",
    "- You have lots of data relative to the lengthscale\n",
    "- The signal-to-noise ratio is high\n",
    "\n",
    "**Non-centered parameterization** (the default) works better when:\n",
    "- The underlying GP is weakly informed by the data  \n",
    "- You have sparse data or large lengthscales\n",
    "- The signal-to-noise ratio is low\n",
    "\n",
    "In our example with 2000 noisy observations, the centered parameterization might actually be better. Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as hsgp_centered:\n",
    "\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=1)\n",
    "    eta = pm.HalfNormal('eta', sigma=5)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(1, ls=ell)\n",
    "    \n",
    "    # HSGP with centered parameterization\n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_hsgp], \n",
    "        c=c_hsgp, \n",
    "        cov_func=cov,\n",
    "        parametrization='centered'  # Key difference!\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=x[:, None])\n",
    "    sigma = pm.HalfNormal('sigma', sigma=2)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y)\n",
    "    \n",
    "    idata_hsgp_centered = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Non-centered parameterization:\")\n",
    "print(az.summary(idata_hsgp, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])\n",
    "print(\"\\nCentered parameterization:\")\n",
    "print(az.summary(idata_hsgp_centered, var_names=['ell', 'eta', 'sigma'])[['ess_bulk', 'ess_tail', 'r_hat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Parameterization Effects\n",
    "\n",
    "Compare the effective sample sizes (ESS) between the two parameterizations. Higher ESS means more efficient samplingâ€”you're getting more independent samples per iteration. For this dataset with strong signal, you may find the centered parameterization provides better ESS.\n",
    "\n",
    "The choice of parameterization doesn't affect what you're learning about the hyperparametersâ€”it only affects how efficiently the sampler explores the posterior. If you find sampling is slow or you see low ESS, try switching parameterizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the HSGP Approximate Gram Matrix\n",
    "\n",
    "Another way to check HSGP fidelity is to directly compare the unapproximated Gram matrix (covariance matrix) $\\mathbf{K}$ to the one resulting from the HSGP approximation:\n",
    "\n",
    "$$\n",
    "\\tilde{\\mathbf{K}} = \\Phi \\Lambda \\Phi^T\n",
    "$$\n",
    "\n",
    "where $\\Phi$ is the matrix of eigenvectors (basis functions), and $\\Lambda$ has the spectral densities computed at the eigenvalues along the diagonal. Let's visualize this for different values of $m$ and $c$ to see when the approximation starts to degrade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a subset for clearer visualization\n",
    "n_viz = 100\n",
    "x_viz = np.linspace(0, 10, n_viz)\n",
    "\n",
    "# True GP covariance\n",
    "chosen_ell = 1.5\n",
    "cov_func_viz = pm.gp.cov.Matern52(1, ls=chosen_ell)\n",
    "K_true = cov_func_viz(x_viz[:, None]).eval()\n",
    "\n",
    "# Helper function to calculate HSGP approximate Gram matrix\n",
    "def calculate_K_approx(x_centered, L, m_val, cov_func):\n",
    "    \"\"\"Calculate the HSGP approximate covariance matrix.\"\"\"\n",
    "    eigvals = pm.gp.hsgp_approx.calc_eigenvalues(L, m_val)\n",
    "    phi = pm.gp.hsgp_approx.calc_eigenvectors(x_centered, L, eigvals, m_val)\n",
    "    omega = pt.sqrt(eigvals)\n",
    "    psd = cov_func.power_spectral_density(omega)\n",
    "    return (phi @ pt.diag(psd) @ phi.T).eval()\n",
    "\n",
    "# Center the data\n",
    "x_center = (x_viz.max() + x_viz.min()) / 2.0\n",
    "x_viz_centered = (x_viz - x_center)[:, None]\n",
    "\n",
    "# Create comparison plot\n",
    "fig, axs = plt.subplots(2, 3, figsize=(12, 8), sharey=True)\n",
    "\n",
    "# True Gram matrix\n",
    "axs[0, 0].imshow(K_true, cmap='inferno', vmin=0, vmax=1)\n",
    "axs[0, 0].set_title(f'True Gram matrix\\nâ„“ = {chosen_ell}')\n",
    "axs[0, 0].set_ylabel('Index')\n",
    "axs[1, 0].axis('off')\n",
    "\n",
    "# Various m and c combinations\n",
    "configs = [\n",
    "    ([30], 2.5, 1),\n",
    "    ([15], 2.5, 2),\n",
    "    ([30], 1.2, 3),\n",
    "    ([15], 1.2, 4),\n",
    "    ([5], 2.5, 5),\n",
    "]\n",
    "\n",
    "for m_val, c_val, idx in configs:\n",
    "    row = 0 if idx <= 2 else 1\n",
    "    col = idx if idx <= 2 else idx - 3\n",
    "    \n",
    "    L = pm.gp.hsgp_approx.set_boundary(x_viz_centered, c_val)\n",
    "    K_approx = calculate_K_approx(x_viz_centered, L, m_val, cov_func_viz)\n",
    "    \n",
    "    axs[row, col].imshow(K_approx, cmap='inferno', vmin=0, vmax=1, interpolation='none')\n",
    "    axs[row, col].set_title(f'm = {m_val[0]}, c = {c_val}')\n",
    "    \n",
    "    if col == 0:\n",
    "        axs[row, col].set_ylabel('Index')\n",
    "    if row == 1:\n",
    "        axs[row, col].set_xlabel('Index')\n",
    "\n",
    "plt.suptitle('HSGP Approximation Quality: Comparing to True Gram Matrix', fontsize=14)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding Gram Matrix Approximations\n",
    "\n",
    "These plots compare approximate Gram matrices to the unapproximated one (top left). The goal is visual similarityâ€”the more alike they look, the better the approximation. Important caveats:\n",
    "\n",
    "- These results are **only relevant for this specific domain and lengthscale** ($\\ell = 1.5$). Different lengthscales will show different approximation quality.\n",
    "- The approximation looks good for $m = 30$ or $m = 15$ with $c=2.5$. The rest show clear differences.\n",
    "- $c=1.2$ is generally too small, regardless of $m$, showing degradation at the boundaries.\n",
    "- Surprisingly, $m=5$, $c=1.2$ can look better than $m=5$, $c=2.5$. When we \"stretch\" the basis to fill a larger domain, we lose fidelity at smaller lengthscales if $m$ is too small.\n",
    "\n",
    "The lesson: **you need to experiment across your range of lengthscales** to find adequate $m$ and $c$ values. Often during prototyping, you can use lower fidelity (smaller $m$) for faster iteration, then dial in higher fidelity once you understand the relevant lengthscales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practical Heuristics for Choosing m and c\n",
    "\n",
    "In practice, you'll need to infer the lengthscale from data, so HSGP needs to approximate a GP across a range of lengthscales representative of your prior. Based on the research literature and empirical experience:\n",
    "\n",
    "1. **Start with `approx_hsgp_hyperparams`**: This function provides good default values. It chooses $c$ large enough to handle your largest expected lengthscales and $m$ large enough for your smallest lengthscales.\n",
    "\n",
    "2. **For smooth functions with moderate lengthscales**: You can often reduce $m$ to 50-100, lowering computational cost.\n",
    "\n",
    "3. **For rapidly-varying functions**: Increase $m$ to 100-200 or more to capture high-frequency components.\n",
    "\n",
    "4. **For long lengthscales**: Increase $c$ to 2.5-4.0 to ensure basis functions extend well beyond your data.\n",
    "\n",
    "5. **Check the basis vectors if sampling struggles**: The first eigenvector can become unidentifiable with the intercept when $c$ is large. Consider using the `drop_first` option.\n",
    "\n",
    "6. **Verify approximation quality**: Compare HSGP to exact GP on a data subset when possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ¤– EXERCISE: Use your LLM to help compare HSGP vs standard GP\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def hsgp_vs_full_gp(X, y, m_values=(20, 50, 100), L_factor=1.5):\n",
    "    \"\"\"\n",
    "    Fit HSGP for several m, compare to full GP in time and RMSE.\n",
    "    \n",
    "    Prompt suggestion: \"Help me implement HSGP in PyMC and produce plots of\n",
    "    computation time vs error across different m values.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on a subset of data\n",
    "# Use 500-1000 points for reasonable comparison times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we understand how to choose HSGP parameters in one dimension, let's see how these principles extend to spatial modeling with 2D data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.6: Advanced HSGP - 2D Spatial Example\n",
    "\n",
    "So far we've focused on one-dimensional examples. HSGP also works well in two dimensions, making it excellent for spatial modeling. Let's apply HSGP to a real-world spatial dataset to see how $m$ and $c$ work in multiple dimensions.\n",
    "\n",
    "### The Walker Lake Dataset\n",
    "\n",
    "We'll use the famous **Walker Lake dataset (Isaaks & Srivastava 1989)**, a classic dataset in spatial statistics. This involves spatial sampling of mineral concentrations across Walker Lake in Nevada. The data consist of spatial coordinates (Xloc, Yloc) in meters and measurements of variable V (concentration in parts per million). The samples are taken regularly over a coarse grid across the entire area, with additional irregular sampling in regions of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Walker Lake data using Polars\n",
    "# The file has mixed whitespace (tabs and multiple spaces)\n",
    "# Polars doesn't support regex separators, so we read lines and parse with Polars\n",
    "\n",
    "# Read file and clean up whitespace\n",
    "with open(DATA_DIR + 'walker.txt', 'r') as f:\n",
    "    lines = [line.split() for line in f.readlines()[8:] if line.strip()]\n",
    "\n",
    "# Create Polars DataFrame from parsed data\n",
    "walker_data = pl.DataFrame({\n",
    "    'ID': [int(row[0]) for row in lines if len(row) >= 6],\n",
    "    'Xloc': [float(row[1]) for row in lines if len(row) >= 6],\n",
    "    'Yloc': [float(row[2]) for row in lines if len(row) >= 6],\n",
    "    'V': [float(row[3]) for row in lines if len(row) >= 6],\n",
    "    'U': [float(row[4]) for row in lines if len(row) >= 6],\n",
    "    'T': [int(row[5]) for row in lines if len(row) >= 6]\n",
    "}).with_columns(\n",
    "    # Replace missing values (1E31) with None\n",
    "    pl.when(pl.col('V') > 1e30).then(None).otherwise(pl.col('V')).alias('V'),\n",
    "    pl.when(pl.col('U') > 1e30).then(None).otherwise(pl.col('U')).alias('U')\n",
    ").filter(\n",
    "    # Use only observations with valid V measurements\n",
    "    pl.col('V').is_not_null()\n",
    ")\n",
    "\n",
    "# Extract spatial coordinates and V variable\n",
    "X_2d = walker_data[['Xloc', 'Yloc']].to_numpy()\n",
    "y_2d = walker_data['V'].to_numpy()\n",
    "\n",
    "print(f\"Loaded {len(X_2d)} observations from Walker Lake dataset\")\n",
    "print(f\"X ranges: [{X_2d[:, 0].min():.0f}, {X_2d[:, 0].max():.0f}] x [{X_2d[:, 1].min():.0f}, {X_2d[:, 1].max():.0f}] meters\")\n",
    "print(f\"V (concentration) range: [{y_2d.min():.1f}, {y_2d.max():.1f}] ppm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the spatial distribution of the observed concentration data. The samples are irregularly distributed across the Walker Lake area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "go.Figure().add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_2d[:, 0],\n",
    "        y=X_2d[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=12,\n",
    "            color=y_2d,\n",
    "            colorscale='Viridis',\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title='V (ppm)',\n",
    "                thickness=20,\n",
    "                len=0.7\n",
    "            ),\n",
    "            line=dict(width=0.5, color='white')  \n",
    "        ),\n",
    "        text=[f'V: {v:.1f} ppm' for v in y_2d],\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>%{text}<extra></extra>'\n",
    "    )\n",
    ").update_layout(\n",
    "    title='Walker Lake Mineral Concentration Data',\n",
    "    xaxis=dict(\n",
    "        title='X Location (meters)',\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        mirror=False\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Y Location (meters)',\n",
    "        scaleanchor='x',\n",
    "        scaleratio=1,\n",
    "        showgrid=False,\n",
    "        zeroline=False,\n",
    "        showline=False,\n",
    "        mirror=False\n",
    "    ),\n",
    "    plot_bgcolor='rgba(240,240,240,0.3)',\n",
    "    height=600,\n",
    "    width=650\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a 2D HSGP Model\n",
    "\n",
    "For 2D HSGPs, we specify $m$ and $c$ as two-element listsâ€”one value per dimension. The total number of basis functions is $m_1 \\times m_2$, so computational cost grows multiplicatively with dimension.\n",
    "\n",
    "The Walker Lake data spans 300 meters in each direction with irregular sampling. We'll use the helper function to determine appropriate HSGP parameters, though we may need to adjust based on the spatial scale of variation in mineral concentrations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine appropriate m and c for Walker Lake spatial scale\n",
    "x_range = [X_2d.min(), X_2d.max()]  # Same range for both dimensions\n",
    "lengthscale_range = [10.0, 100.0]  # Expected spatial correlation in meters\n",
    "\n",
    "m_2d, c_2d = pm.gp.hsgp_approx.approx_hsgp_hyperparams(\n",
    "    x_range=x_range,\n",
    "    lengthscale_range=lengthscale_range,\n",
    "    cov_func='matern52'\n",
    ")\n",
    "\n",
    "print(f\"2D HSGP recommendations: m={m_2d}, c={c_2d:.2f}\")\n",
    "print(f\"Total basis functions: {m_2d**2}\")\n",
    "\n",
    "# Standardize the concentration data for better sampling\n",
    "y_mean = y_2d.mean()\n",
    "y_std = y_2d.std()\n",
    "y_2d_std = (y_2d - y_mean) / y_std\n",
    "\n",
    "with pm.Model() as hsgp_2d_model:\n",
    "\n",
    "    ell = pm.Gamma('ell', alpha=2, beta=0.05)  # Prior centered around 40 meters\n",
    "    eta = pm.HalfNormal('eta', sigma=2)\n",
    "    \n",
    "    cov = eta**2 * pm.gp.cov.Matern52(2, ls=ell)\n",
    "    \n",
    "    gp = pm.gp.HSGP(\n",
    "        m=[m_2d, m_2d],   # m for each dimension\n",
    "        c=c_2d,           # c applies to both\n",
    "        cov_func=cov\n",
    "    )\n",
    "    \n",
    "    f = gp.prior('f', X=X_2d)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    y_obs = pm.Normal('y_obs', mu=f, sigma=sigma, observed=y_2d_std)\n",
    "    \n",
    "    idata_2d = pm.sample(\n",
    "        500,\n",
    "        tune=500,\n",
    "        chains=2,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        nuts_sampler=\"nutpie\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the 2D HSGP Fit\n",
    "\n",
    "Let's create a predicted surface over a regular grid to visualize the spatial pattern learned by the HSGP. We'll interpolate the GP to a fine grid covering the Walker Lake area, then compare the predictions to the observed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prediction grid\n",
    "n_grid = 40\n",
    "x1_grid = np.linspace(X_2d[:, 0].min(), X_2d[:, 0].max(), n_grid)\n",
    "x2_grid = np.linspace(X_2d[:, 1].min(), X_2d[:, 1].max(), n_grid)\n",
    "X1_mesh, X2_mesh = np.meshgrid(x1_grid, x2_grid)\n",
    "X_pred = np.column_stack([X1_mesh.ravel(), X2_mesh.ravel()])\n",
    "\n",
    "# Get posterior predictions\n",
    "with hsgp_2d_model:\n",
    "    f_pred = gp.conditional('f_pred', X_pred)\n",
    "    posterior_pred_2d = pm.sample_posterior_predictive(\n",
    "        idata_2d,\n",
    "        var_names=['f_pred'],\n",
    "        random_seed=RANDOM_SEED\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract posterior mean and convert back to original scale\n",
    "f_post_mean = posterior_pred_2d.posterior_predictive['f_pred'].mean(dim=['chain', 'draw']).values\n",
    "f_post_mean_original = f_post_mean * y_std + y_mean\n",
    "f_post_grid = f_post_mean_original.reshape(n_grid, n_grid)\n",
    "\n",
    "# Determine shared color range\n",
    "vmin = min(y_2d.min(), f_post_mean_original.min())\n",
    "vmax = max(y_2d.max(), f_post_mean_original.max())\n",
    "\n",
    "# Create side-by-side comparison with shared styling\n",
    "make_subplots(\n",
    "    rows=1, cols=2,\n",
    "    subplot_titles=('HSGP Posterior Mean Surface', 'Observed Data Locations'),\n",
    "    horizontal_spacing=0.12,\n",
    "    vertical_spacing=0.08\n",
    ").add_trace(\n",
    "    go.Heatmap(\n",
    "        z=f_post_grid,\n",
    "        x=x1_grid,\n",
    "        y=x2_grid,\n",
    "        colorscale='Viridis',\n",
    "        zmin=vmin,\n",
    "        zmax=vmax,\n",
    "        showscale=False,\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>V: %{z:.1f} ppm<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=1\n",
    ").add_trace(\n",
    "    go.Scatter(\n",
    "        x=X_2d[:, 0],\n",
    "        y=X_2d[:, 1],\n",
    "        mode='markers',\n",
    "        marker=dict(\n",
    "            size=8,\n",
    "            color=y_2d,\n",
    "            colorscale='Viridis',\n",
    "            cmin=vmin,\n",
    "            cmax=vmax,\n",
    "            showscale=True,\n",
    "            colorbar=dict(\n",
    "                title=dict(text='V (ppm)', side='right'),\n",
    "                x=1.0,\n",
    "                thickness=15,\n",
    "                len=0.65,\n",
    "                xpad=10\n",
    "            ),\n",
    "            line=dict(width=0.5, color='white')\n",
    "        ),\n",
    "        hovertemplate='X: %{x:.0f}m<br>Y: %{y:.0f}m<br>V: %{marker.color:.1f} ppm<extra></extra>'\n",
    "    ),\n",
    "    row=1, col=2\n",
    ").update_xaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    row=1, col=1\n",
    ").update_xaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    row=1, col=2\n",
    ").update_yaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    scaleanchor='x',\n",
    "    scaleratio=1,\n",
    "    row=1, col=1\n",
    ").update_yaxes(\n",
    "    showgrid=False,\n",
    "    zeroline=False,\n",
    "    showline=False,\n",
    "    ticks='outside',\n",
    "    ticklen=5,\n",
    "    scaleanchor='x2',\n",
    "    scaleratio=1,\n",
    "    row=1, col=2\n",
    ").update_layout(\n",
    "    title=dict(\n",
    "        text='2D HSGP Fit to Walker Lake Data',\n",
    "        x=0.5,\n",
    "        xanchor='center',\n",
    "        font=dict(size=16)\n",
    "    ),\n",
    "    height=520,\n",
    "    plot_bgcolor='white',\n",
    "    paper_bgcolor='white',\n",
    "    showlegend=False,\n",
    "    margin=dict(l=80, r=120, t=100, b=80),\n",
    "    annotations=[\n",
    "        dict(\n",
    "            text='X Location (meters)',\n",
    "            xref='paper', yref='paper',\n",
    "            x=0.5, y=-0.12,\n",
    "            xanchor='center', yanchor='top',\n",
    "            showarrow=False,\n",
    "            font=dict(size=14)\n",
    "        ),\n",
    "        dict(\n",
    "            text='Y Location (meters)',\n",
    "            xref='paper', yref='paper',\n",
    "            x=-0.08, y=0.5,\n",
    "            xanchor='center', yanchor='middle',\n",
    "            textangle=-90,\n",
    "            showarrow=False,\n",
    "            font=dict(size=14)\n",
    "        )\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding 2D HSGP Performance on Real Spatial Data\n",
    "\n",
    "The HSGP successfully learned the spatial structure of mineral concentrations across Walker Lake. Notice several important features:\n",
    "\n",
    "1. **Smooth spatial interpolation**: The predicted surface (left panel) provides smooth estimates even in areas between observations, capturing the underlying spatial pattern while avoiding overfitting to individual noisy measurements.\n",
    "\n",
    "2. **Irregular sampling handled naturally**: Unlike grid-based methods, the HSGP works seamlessly with the irregular sampling pattern (right panel), where some areas have dense measurements and others are sparse.\n",
    "\n",
    "3. **Computational efficiency**: Despite having several hundred observations and predicting on a fine grid (1,600 locations), the HSGP completed fitting and prediction efficiently using basis function expansion.\n",
    "\n",
    "For 2D problems like this spatial dataset, remember that the total number of basis functions is $m_1 \\times m_2$. This is still far more efficient than exact inference, but it shows why HSGP doesn't scale well beyond 3 dimensionsâ€”the basis functions multiply quickly! For higher-dimensional problems, sparse GP methods or other approximations may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ¤– EXERCISE: Exploring HSGP Parameter Choices\n",
    "\n",
    "Now experiment with different HSGP parameter configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your LLM to help explore HSGP parameter choices\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def tune_hsgp_params(X, y, m_grid=(30, 60, 120), c_grid=(1.5, 2.5, 4.0)):\n",
    "    \"\"\"\n",
    "    Grid-search m and c to evaluate speed and accuracy trade-offs.\n",
    "    \n",
    "    Prompt suggestion: \"Help me create a function that runs multiple HSGP fits\n",
    "    over m and c grid and summarizes performance with Plotly heatmaps.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Test on Walker Lake dataset\n",
    "# Visualize RMSE and sampling time as heatmaps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3.7: Comparing All Approaches\n",
    "\n",
    "We've now explored both sparse GPs and HSGP approximations in detail. Let's bring everything together with a comprehensive comparison that highlights when to use each approach.\n",
    "\n",
    "### Computational Complexity Summary\n",
    "\n",
    "Let's visualize the computational complexity of each approach as a function of dataset size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_values = np.logspace(2, 4, 50)  # 100 to 10,000 data points\n",
    "m_sparse = 100  # inducing points\n",
    "m_hsgp = 100    # basis functions\n",
    "\n",
    "# Relative computational cost (arbitrary units)\n",
    "cost_exact = n_values**3 / 1e6  # Scale for visibility\n",
    "cost_sparse = n_values * m_sparse**2 / 1e6\n",
    "cost_hsgp = n_values * m_hsgp / 1e6\n",
    "\n",
    "go.Figure().add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_exact,\n",
    "    mode='lines',\n",
    "    name='Exact GP: O(nÂ³)',\n",
    "    line=dict(color='blue', width=3)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_sparse,\n",
    "    mode='lines',\n",
    "    name=f'Sparse GP: O(nmÂ²), m={m_sparse}',\n",
    "    line=dict(color='green', width=3)\n",
    ")).add_trace(go.Scatter(\n",
    "    x=n_values,\n",
    "    y=cost_hsgp,\n",
    "    mode='lines',\n",
    "    name=f'HSGP: O(nm), m={m_hsgp}',\n",
    "    line=dict(color='red', width=3)\n",
    ")).update_layout(\n",
    "    title='Computational Complexity: Exact GP vs. Approximations',\n",
    "    xaxis_title='Number of data points (n)',\n",
    "    yaxis_title='Relative computational cost',\n",
    "    xaxis_type='log',\n",
    "    yaxis_type='log',\n",
    "    height=500,\n",
    "    showlegend=True,\n",
    "    hovermode='x unified'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Complexity Comparison\n",
    "\n",
    "This log-log plot dramatically illustrates why approximations are essential for large datasets:\n",
    "\n",
    "- **The blue line (exact GP)** curves upward steeply, showing the crushing $\\mathcal{O}(n^3)$ growth. By $n=10,000$, exact inference is essentially infeasible.\n",
    "\n",
    "- **The green line (sparse GP)** grows much more slowly at $\\mathcal{O}(nm^2)$, making datasets of several thousand points tractable.\n",
    "\n",
    "- **The red line (HSGP)** has the gentlest slope at $\\mathcal{O}(nm)$, showing near-linear scaling that makes even very large datasets manageable.\n",
    "\n",
    "The crossover points where approximations become worthwhile depend on your patience, hardware, and accuracy requirements, but as a rough guide: consider sparse GPs beyond ~1,000 points and HSGP beyond ~5,000 points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Guide: Which Method to Use\n",
    "\n",
    "Here's practical guidance for choosing between methods:\n",
    "\n",
    "**Use Standard (Exact) GP when:**\n",
    "- $n < 1,000$ points\n",
    "- You need exact inference without approximation error\n",
    "- You're using non-stationary kernels\n",
    "- Computation time isn't critical\n",
    "\n",
    "**Use Sparse GP (Inducing Points) when:**\n",
    "- $1,000 < n < 10,000$ points  \n",
    "- Data has uneven sampling density\n",
    "- You have domain knowledge about where to place inducing points\n",
    "- You're primarily using Gaussian likelihoods\n",
    "- **Typical use cases**: Spatial data with known regions of interest, time series with known change points\n",
    "\n",
    "**Use HSGP when:**\n",
    "- $n > 5,000$ points\n",
    "- Using stationary kernels (MatÃ©rn, ExpQuad)\n",
    "- Input dimension is 1, 2, or 3\n",
    "- You need to integrate the GP into a larger model\n",
    "- You need predictions at many new locations\n",
    "- **Typical use cases**: Long time series, spatial data on regular grids, any large dataset with smooth variation\n",
    "\n",
    "**Practical tip**: When prototyping, start with a low-fidelity HSGP (small $m$) for fast iteration. Once you understand the relevant lengthscales, dial in appropriate $m$ and $c$ for production."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "section-ard-header",
   "metadata": {},
   "source": [
    "## Section 3.8: Automatic Relevance Determination (ARD)\n",
    "\n",
    "So far, we've focused on computational scaling for large datasets using sparse GPs and HSGP. But there's another challenge when working with real-world data: **handling many input features where not all are equally important**.\n",
    "\n",
    "When you have dozens of potential predictors, manually selecting which features to include becomes tedious and risks overfitting. **Automatic Relevance Determination (ARD)** solves this by assigning a separate lengthscale to each input dimension, allowing the GP to automatically learn which features matter.\n",
    "\n",
    "Think of ARD as built-in feature selection: relevant dimensions get small lengthscales (the model pays close attention to changes in these features), while irrelevant dimensions get large lengthscales (the model becomes insensitive to them, effectively removing them from predictions).\n",
    "\n",
    "Let's see ARD in action on a real dataset where we genuinely don't know which features are most important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Boston Housing Dataset\n",
    "\n",
    "Instead of synthetic data, let's use a real-world dataset: the Boston housing dataset. This classic dataset contains information about housing in Boston suburbs from the 1970s, with 13 features describing each neighborhood and the median home value.\n",
    "\n",
    "The features include:\n",
    "- **CRIM**: Per capita crime rate by town\n",
    "- **ZN**: Proportion of residential land zoned for large lots\n",
    "- **INDUS**: Proportion of non-retail business acres\n",
    "- **CHAS**: Charles River dummy variable (1 if tract bounds river)\n",
    "- **NOX**: Nitric oxides concentration (pollution)\n",
    "- **RM**: Average number of rooms per dwelling\n",
    "- **AGE**: Proportion of owner-occupied units built before 1940\n",
    "- **DIS**: Weighted distances to employment centers\n",
    "- **RAD**: Index of accessibility to radial highways\n",
    "- **TAX**: Property tax rate\n",
    "- **PTRATIO**: Pupil-teacher ratio\n",
    "- **LSTAT**: Percentage of lower status population\n",
    "- **MEDV**: Median value of owner-occupied homes (target)\n",
    "\n",
    "With ARD, we can fit a GP using **all these features** and let the model automatically discover which ones are most relevant for predicting house prices. Features that matter will get small lengthscales (the model pays close attention), while irrelevant features will get large lengthscales (the model effectively ignores them)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Boston housing data\n",
    "boston_df = pl.read_csv(DATA_DIR + 'HousingData.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "boston_df = boston_df.drop_nulls()\n",
    "\n",
    "# Extract all features except MEDV (target) and B (excluded for ethical reasons)\n",
    "feature_cols = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'LSTAT']\n",
    "X_boston = boston_df.select(feature_cols).to_numpy()\n",
    "y_boston = boston_df['MEDV'].to_numpy()\n",
    "\n",
    "# Standardize features for better GP performance\n",
    "X_mean = X_boston.mean(axis=0)\n",
    "X_std = X_boston.std(axis=0)\n",
    "X_boston_std = (X_boston - X_mean) / X_std\n",
    "\n",
    "# Standardize target\n",
    "y_mean = y_boston.mean()\n",
    "y_std = y_boston.std()\n",
    "y_boston_std = (y_boston - y_mean) / y_std\n",
    "\n",
    "print(f\"Boston Housing Dataset: {X_boston_std.shape[0]} observations, {X_boston_std.shape[1]} features\")\n",
    "print(f\"\\nFeature names: {feature_cols}\")\n",
    "print(f\"\\nTarget (MEDV): mean=${y_boston.mean():.1f}k, std=${y_boston.std():.1f}k, range=[${y_boston.min():.1f}k, ${y_boston.max():.1f}k]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the ARD Model\n",
    "\n",
    "Now we'll fit a GP with ARD by giving each of the 12 features its own lengthscale parameter. Notice how we specify `dims=\"features\"` for the lengthscale - this creates a vector of 12 lengthscales, one per feature.\n",
    "\n",
    "The model will simultaneously:\n",
    "1. Learn the overall amplitude and noise level\n",
    "2. Learn a separate lengthscale for each feature\n",
    "3. Use these lengthscales to make predictions\n",
    "\n",
    "**The key insight**: small lengthscales indicate relevance (function changes rapidly with that feature), while large lengthscales indicate irrelevance (covariance nearly constant across that feature's range)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords={\"features\": feature_cols}) as ard_model:\n",
    "\n",
    "    # Separate lengthscale for each feature (ARD)\n",
    "    ls = pm.Gamma(\"ls\", alpha=2, beta=1, dims=\"features\")\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=2)\n",
    "    \n",
    "    # ExpQuad kernel with ARD\n",
    "    cov_func = eta**2 * pm.gp.cov.ExpQuad(input_dim=12, ls=ls)\n",
    "    gp = pm.gp.Marginal(cov_func=cov_func)\n",
    "    \n",
    "    # Observation noise\n",
    "    sigma = pm.HalfNormal(\"sigma\", sigma=0.5)\n",
    "    \n",
    "    # Likelihood\n",
    "    y_ = gp.marginal_likelihood(\"y\", X=X_boston_std, y=y_boston_std, sigma=sigma)\n",
    "    \n",
    "    # Sample posterior\n",
    "    trace_ard = pm.sample(\n",
    "        500, \n",
    "        tune=500, \n",
    "        nuts_sampler=\"nutpie\", \n",
    "        random_seed=RANDOM_SEED, \n",
    "        chains=2\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Learned Lengthscales by Feature\n",
    "\n",
    "Let's examine which features the model learned are most important. We'll create a bar plot showing the posterior mean lengthscale for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "az.plot_forest(trace_ard, var_names=['ls'], figsize=(6, 12), combined=True, rope=[0,4]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the ARD Results\n",
    "\n",
    "The forest plot above shows the posterior distributions of lengthscales for each feature. Because we **standardized all input features** to have mean 0 and standard deviation 1, these lengthscales are directly comparable across features - this is crucial for ARD interpretation.\n",
    "\n",
    "#### Understanding Lengthscales with Standardized Features\n",
    "\n",
    "A lengthscale represents **the distance you need to move along a feature's axis for function values to become uncorrelated**. With standardized features (mean=0, std=1):\n",
    "\n",
    "- **Highly relevant (< 1)**: Moving just one standard deviation causes substantial decorrelation. The model sees the function changing rapidly with this feature.\n",
    "\n",
    "- **Moderately relevant (1-3)**: The function changes moderately. These features contribute to predictions but with less sensitivity.\n",
    "\n",
    "- **Weakly relevant (> 10)**: The covariance becomes nearly constant across this feature's range. Moving even 10 standard deviations barely affects predictions.\n",
    "\n",
    "- **Effectively irrelevant (> 100)**: The model has learned to ignore this feature entirely. The covariance is nearly independent of this input.\n",
    "\n",
    "#### What to Look for in the Forest Plot\n",
    "\n",
    "Examine the lengthscale distributions shown above:\n",
    "\n",
    "1. **Left side of the plot (small lengthscales)**: Features with tight distributions clustered near 0-2 are the most important predictors. These typically include features like **LSTAT** (% lower status population), **RM** (avg rooms), and **PTRATIO** (pupil-teacher ratio) - all well-known drivers of housing prices.\n",
    "\n",
    "2. **Middle range (lengthscales 3-10)**: Features that contribute moderately to predictions. They matter, but changes in these features have less dramatic effects on house prices.\n",
    "\n",
    "3. **Right side of the plot (large lengthscales > 10)**: Features the model has learned to downweight or ignore. This could indicate true irrelevance or that their information is captured by other correlated features. For example, **CHAS** (Charles River proximity) affects relatively few houses in the dataset.\n",
    "\n",
    "#### The Power of ARD\n",
    "\n",
    "This automatic feature discovery happens **during model fitting** without any manual intervention. We didn't need to:\n",
    "- Manually select features beforehand\n",
    "- Run separate feature importance analyses  \n",
    "- Fit multiple models and compare\n",
    "\n",
    "The learned lengthscales serve a dual purpose: they're both **correlation parameters** (controlling how smooth the GP is along each dimension) and **importance weights** (telling us which features actually matter for predictions). This makes ARD an elegant solution to the feature selection problem in high-dimensional GP regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e32c846",
   "metadata": {},
   "source": [
    "### Connecting ARD to Scaling Methods\n",
    "\n",
    "ARD is particularly valuable when combined with the scaling methods we explored earlier. For high-dimensional problems with many features:\n",
    "\n",
    "- **ARD identifies which features matter**, potentially reducing the effective dimensionality\n",
    "- **HSGP** can then efficiently handle the relevant dimensions (though remember HSGP works best for 1-3 dimensions)\n",
    "- **Sparse GPs** can scale to larger datasets while still using ARD kernels\n",
    "\n",
    "The combination of ARD for feature selection and approximation methods for computational scaling allows you to tackle real-world problems with both many observations and many features.\n",
    "\n",
    "This completes our tour of GP scaling methods. You now have the tools to apply GPs to datasets that would be intractable with exact inference, while automatically discovering which features drive your predictions."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
