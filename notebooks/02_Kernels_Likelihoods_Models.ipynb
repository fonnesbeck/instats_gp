{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Kernels, Likelihoods, and Model Building\n",
    "\n",
    "Welcome to Session 2 of our Gaussian Processes with PyMC workshop! In the previous session, we covered the fundamentals of Gaussian processes and their implementation in PyMC. Today, we'll dive deeper into the building blocks that make GPs so powerful and flexible.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Master different kernel families** and understand when to use each type\n",
    "2. **Understand kernel composition** and how to combine kernels for complex patterns\n",
    "3. **Implement non-Gaussian likelihoods** for classification, count data, and robust regression\n",
    "4. **Navigate inference trade-offs** between computational efficiency and model flexibility\n",
    "5. **Build robust GP models** that handle real-world complexities\n",
    "\n",
    "## Session Overview\n",
    "\n",
    "### Part A: Kernel Functions Deep Dive\n",
    "- Understanding the role of kernels in GP modeling\n",
    "- Exploring different kernel families (RBF, Matérn, Periodic, etc.)\n",
    "- Kernel hyperparameters and their effects\n",
    "- Practical guidelines for kernel selection\n",
    "\n",
    "### Part B: Kernel Composition\n",
    "- Mathematical foundations of kernel combination\n",
    "- Addition and multiplication of kernels\n",
    "- Building complex patterns through composition\n",
    "- Real-world examples of composite kernels\n",
    "\n",
    "### Part C: Non-Gaussian Likelihoods\n",
    "- Beyond Gaussian noise: classification and count data\n",
    "- Student-t processes for robust regression\n",
    "- Latent variable implementation for non-Gaussian outcomes\n",
    "- Computational considerations\n",
    "\n",
    "### Part D: Model Building Best Practices\n",
    "- Choosing between Marginal vs Latent implementations\n",
    "- Hyperparameter priors and initialization\n",
    "- Model validation and diagnostics\n",
    "- Handling computational challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Kernel Functions Deep Dive\n",
    "\n",
    "The kernel (or covariance) function is the heart of a Gaussian process. It encodes our assumptions about the smoothness, periodicity, and other structural properties of the function we're modeling.\n",
    "\n",
    "## Understanding Kernel Functions\n",
    "\n",
    "A kernel function $k(x, x')$ measures the similarity between two input points $x$ and $x'$. The key properties of a valid kernel are:\n",
    "\n",
    "1. **Symmetry**: $k(x, x') = k(x', x)$\n",
    "2. **Positive semi-definiteness**: The covariance matrix must be positive semi-definite\n",
    "\n",
    "Let's explore the most commonly used kernel families and their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Radial Basis Function (RBF) Kernel\n",
    "\n",
    "The RBF kernel (also called Gaussian or squared exponential kernel) is perhaps the most widely used kernel:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma^2$ is the variance parameter (controls output scale)\n",
    "- $\\ell$ is the lengthscale parameter (controls input scale)\n",
    "\n",
    "The RBF kernel assumes infinite differentiability, making it suitable for smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore how RBF kernel parameters affect the covariance structure\n",
    "def plot_kernel_comparison():\n",
    "    X = np.linspace(-3, 3, 100)[:, None]\n",
    "    x_test = np.array([[0.0]])  # Reference point\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Effect of Lengthscale (σ²=1)\",\n",
    "            \"Effect of Variance (ℓ=1)\", \n",
    "            \"Sample Functions (ℓ=0.5)\",\n",
    "            \"Sample Functions (ℓ=2.0)\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Effect of lengthscale\n",
    "    lengthscales = [0.3, 1.0, 2.0]\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    for i, (ls, color) in enumerate(zip(lengthscales, colors)):\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'ℓ={ls}', line=dict(color=color)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Effect of variance\n",
    "    variances = [0.5, 1.0, 2.0]\n",
    "    \n",
    "    for i, (var, color) in enumerate(zip(variances, colors)):\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(var, ls=1.0)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'σ²={var}', line=dict(color=color),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Sample functions with different lengthscales\n",
    "    for ls, row_col in [(0.5, (2,1)), (2.0, (2,2))]:\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "            K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "            \n",
    "        # Sample from the prior\n",
    "        L = np.linalg.cholesky(K)\n",
    "        samples = L @ rng.standard_normal((len(X), 3))\n",
    "        \n",
    "        for i in range(3):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X.flatten(), y=samples[:, i],\n",
    "                    name=f'Sample {i+1}', \n",
    "                    line=dict(color=colors[i]),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=row_col[0], col=row_col[1]\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=600, title=\"RBF Kernel Properties\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"k(x, 0)\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_kernel_comparison().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- **Lengthscale ($\\ell$)**: Controls how far the influence of a data point extends. Smaller values create more wiggly functions.\n",
    "- **Variance ($\\sigma^2$)**: Controls the overall scale of variation. Higher values allow for larger deviations from the mean.\n",
    "\n",
    "## 2. Matérn Kernel Family\n",
    "\n",
    "The Matérn kernel family provides more flexibility in modeling smoothness:\n",
    "\n",
    "$$k(x, x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu} \\frac{|x - x'|}{\\ell}\\right)^\\nu K_\\nu\\left(\\sqrt{2\\nu} \\frac{|x - x'|}{\\ell}\\right)$$\n",
    "\n",
    "Where $\\nu$ controls smoothness and $K_\\nu$ is the modified Bessel function of the second kind.\n",
    "\n",
    "Common choices:\n",
    "- $\\nu = 1/2$: Exponential kernel (non-differentiable)\n",
    "- $\\nu = 3/2$: Once differentiable\n",
    "- $\\nu = 5/2$: Twice differentiable\n",
    "- $\\nu \\to \\infty$: RBF kernel (infinitely differentiable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_matern_kernels():\n",
    "    X = np.linspace(-3, 3, 100)[:, None]\n",
    "    x_test = np.array([[0.0]])\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Matérn Kernel Comparison\", \"Sample Functions\"]\n",
    "    )\n",
    "    \n",
    "    nus = [0.5, 1.5, 2.5, np.inf]\n",
    "    nu_labels = ['1/2', '3/2', '5/2', '∞ (RBF)']\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    # Compare kernel shapes\n",
    "    for nu, label, color in zip(nus, nu_labels, colors):\n",
    "        with pm.Model() as model:\n",
    "            if nu == np.inf:\n",
    "                cov = pm.gp.cov.ExpQuad(1, ls=1.0)\n",
    "            else:\n",
    "                cov = pm.gp.cov.Matern52(1, ls=1.0) if nu == 2.5 else \\\n",
    "                      pm.gp.cov.Matern32(1, ls=1.0) if nu == 1.5 else \\\n",
    "                      pm.gp.cov.Exponential(1, ls=1.0)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'ν={label}', line=dict(color=color)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Sample functions from Matérn 3/2\n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Matern32(1, ls=1.0)\n",
    "        K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "        \n",
    "    L = np.linalg.cholesky(K)\n",
    "    samples = L @ rng.standard_normal((len(X), 3))\n",
    "    \n",
    "    for i in range(3):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=samples[:, i],\n",
    "                name=f'Matérn 3/2 Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Matérn Kernel Family\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"Kernel Value / Function Value\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "compare_matern_kernels().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Periodic Kernel\n",
    "\n",
    "The periodic kernel is designed for functions with repeating patterns:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi|x - x'|/p)}{\\ell^2}\\right)$$\n",
    "\n",
    "Where $p$ is the period parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_periodic_kernel():\n",
    "    X = np.linspace(0, 8, 200)[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Periodic Kernel (p=2π)\", \"Sample Functions\"]\n",
    "    )\n",
    "    \n",
    "    # Kernel visualization\n",
    "    x_test = np.array([[2.0]])\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0)\n",
    "        K = cov(X, x_test).eval()\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X.flatten(), y=K.flatten(),\n",
    "            name='Periodic Kernel', line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Sample functions\n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0)\n",
    "        K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "        \n",
    "    L = np.linalg.cholesky(K)\n",
    "    samples = L @ rng.standard_normal((len(X), 3))\n",
    "    \n",
    "    colors = ['red', 'green', 'orange']\n",
    "    for i in range(3):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=samples[:, i],\n",
    "                name=f'Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Periodic Kernel\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "demonstrate_periodic_kernel().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Kernel Composition\n",
    "\n",
    "One of the most powerful aspects of GP modeling is the ability to combine kernels to create more complex covariance structures. This allows us to model functions with multiple characteristics simultaneously.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "If $k_1$ and $k_2$ are valid kernels, then:\n",
    "\n",
    "1. **Addition**: $k(x, x') = k_1(x, x') + k_2(x, x')$ models functions that are the sum of processes with different characteristics\n",
    "\n",
    "2. **Multiplication**: $k(x, x') = k_1(x, x') \\cdot k_2(x, x')$ models functions where both characteristics must be present simultaneously\n",
    "\n",
    "## Example: Trend + Seasonality + Noise\n",
    "\n",
    "Let's build a kernel for modeling data with:\n",
    "- Long-term smooth trends\n",
    "- Seasonal patterns\n",
    "- Short-term variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kernel_composition():\n",
    "    # Generate synthetic data with trend, seasonality, and noise\n",
    "    X = np.linspace(0, 4*np.pi, 100)\n",
    "    \n",
    "    # True components\n",
    "    trend = 0.1 * X\n",
    "    seasonal = 0.5 * np.sin(X)\n",
    "    noise = 0.1 * rng.standard_normal(len(X))\n",
    "    y_true = trend + seasonal + noise\n",
    "    \n",
    "    # Convert to training data\n",
    "    X_train = X[::5]  # Subsample for training\n",
    "    y_train = y_true[::5]\n",
    "    X_test = X[:, None]\n",
    "    X_train = X_train[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Individual Kernels\",\n",
    "            \"Composite Kernel Samples\",\n",
    "            \"GP Regression: Simple Kernel\",\n",
    "            \"GP Regression: Composite Kernel\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Plot individual kernel samples\n",
    "    kernels = {\n",
    "        'Long-term (RBF ℓ=3)': pm.gp.cov.ExpQuad(1, ls=3.0),\n",
    "        'Seasonal (Periodic p=2π)': pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0),\n",
    "        'Short-term (RBF ℓ=0.5)': pm.gp.cov.ExpQuad(1, ls=0.5)\n",
    "    }\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for i, (name, kernel) in enumerate(kernels.items()):\n",
    "        with pm.Model():\n",
    "            K = kernel(X_test).eval() + 1e-6 * np.eye(len(X_test))\n",
    "        \n",
    "        L = np.linalg.cholesky(K)\n",
    "        sample = L @ rng.standard_normal(len(X_test))\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, y=sample.flatten(),\n",
    "                name=name, line=dict(color=colors[i])\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Composite kernel samples\n",
    "    with pm.Model():\n",
    "        composite_kernel = (pm.gp.cov.ExpQuad(1, ls=3.0) + \n",
    "                          pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0) +\n",
    "                          pm.gp.cov.ExpQuad(0.5, ls=0.5))\n",
    "        K_comp = composite_kernel(X_test).eval() + 1e-6 * np.eye(len(X_test))\n",
    "    \n",
    "    L_comp = np.linalg.cholesky(K_comp)\n",
    "    for i in range(3):\n",
    "        sample = L_comp @ rng.standard_normal(len(X_test))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, y=sample.flatten(),\n",
    "                name=f'Composite Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Add true data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_train.flatten(), y=y_train,\n",
    "            mode='markers', name='Training Data',\n",
    "            marker=dict(color='black', size=4)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_train.flatten(), y=y_train,\n",
    "            mode='markers', name='Training Data',\n",
    "            marker=dict(color='black', size=4),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Simple GP regression\n",
    "    with pm.Model() as simple_model:\n",
    "        ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "        η = pm.HalfCauchy(\"η\", beta=5)\n",
    "        \n",
    "        cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        σ = pm.HalfCauchy(\"σ\", beta=5)\n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=σ)\n",
    "        \n",
    "        # Predict\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        \n",
    "        # Sample from posterior\n",
    "        idata_simple = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, \n",
    "                                chains=2, cores=1, progressbar=False)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata_simple, \n",
    "                                                     progressbar=False)\n",
    "    \n",
    "    # Plot simple GP results\n",
    "    f_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X, y=f_mean,\n",
    "            name='GP Mean (Simple)',\n",
    "            line=dict(color='blue'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X, X[::-1]]),\n",
    "            y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Composite GP regression (simplified for demo)\n",
    "    with pm.Model() as composite_model:\n",
    "        # Long-term component\n",
    "        ℓ_long = pm.Gamma(\"ℓ_long\", alpha=2, beta=0.5)\n",
    "        η_long = pm.HalfCauchy(\"η_long\", beta=2)\n",
    "        \n",
    "        # Periodic component  \n",
    "        ℓ_per = pm.Gamma(\"ℓ_per\", alpha=2, beta=2)\n",
    "        η_per = pm.HalfCauchy(\"η_per\", beta=2)\n",
    "        \n",
    "        # Short-term component\n",
    "        ℓ_short = pm.Gamma(\"ℓ_short\", alpha=2, beta=4)\n",
    "        η_short = pm.HalfCauchy(\"η_short\", beta=1)\n",
    "        \n",
    "        # Composite kernel\n",
    "        cov = (η_long**2 * pm.gp.cov.ExpQuad(1, ls=ℓ_long) +\n",
    "               η_per**2 * pm.gp.cov.Periodic(1, period=2*np.pi, ls=ℓ_per) +\n",
    "               η_short**2 * pm.gp.cov.ExpQuad(1, ls=ℓ_short))\n",
    "        \n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        σ = pm.HalfCauchy(\"σ\", beta=0.5)\n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=σ)\n",
    "        \n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        \n",
    "        idata_composite = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED,\n",
    "                                  chains=2, cores=1, progressbar=False)\n",
    "        pred_samples_comp = pm.sample_posterior_predictive(idata_composite,\n",
    "                                                          progressbar=False)\n",
    "    \n",
    "    # Plot composite GP results\n",
    "    f_mean_comp = pred_samples_comp.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_std_comp = pred_samples_comp.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X, y=f_mean_comp,\n",
    "            name='GP Mean (Composite)',\n",
    "            line=dict(color='red'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X, X[::-1]]),\n",
    "            y=np.concatenate([f_mean_comp + 2*f_std_comp, \n",
    "                            (f_mean_comp - 2*f_std_comp)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title=\"Kernel Composition Example\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "demonstrate_kernel_composition().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights from Kernel Composition:**\n",
    "\n",
    "1. **Additive kernels** allow modeling functions as sums of different components\n",
    "2. **Individual kernels** capture specific patterns (trends, seasonality, noise)\n",
    "3. **Composite models** can capture complex real-world patterns that simple kernels cannot\n",
    "4. **Parameter interpretation** becomes more challenging but also more meaningful\n",
    "\n",
    "## Common Kernel Combinations\n",
    "\n",
    "Here are some useful kernel combinations for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common kernel combination patterns\n",
    "kernel_combinations = {\n",
    "    \"Smooth trend + noise\": \"RBF(large_ℓ) + RBF(small_ℓ)\",\n",
    "    \"Periodic + trend\": \"Periodic × RBF + RBF(large_ℓ)\", \n",
    "    \"Local periodicity\": \"Periodic × RBF\",\n",
    "    \"Changepoint model\": \"RBF + Polynomial + White Noise\",\n",
    "    \"Multi-scale patterns\": \"RBF(ℓ₁) + RBF(ℓ₂) + RBF(ℓ₃)\"\n",
    "}\n",
    "\n",
    "print(\"Common Kernel Combination Patterns:\")\n",
    "print(\"=\" * 40)\n",
    "for use_case, formula in kernel_combinations.items():\n",
    "    print(f\"{use_case:20}: {formula}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part C: Non-Gaussian Likelihoods\n",
    "\n",
    "So far, we've focused on Gaussian likelihoods, which are appropriate for continuous data with symmetric noise. However, many real-world problems require different likelihood functions:\n",
    "\n",
    "- **Classification**: Binary or multi-class outcomes\n",
    "- **Count data**: Poisson or negative binomial observations\n",
    "- **Robust regression**: Heavy-tailed noise (Student-t)\n",
    "- **Ordinal data**: Ordered categorical outcomes\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "For non-Gaussian likelihoods, we typically use the **latent variable approach**:\n",
    "\n",
    "1. **Latent function**: $f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))$\n",
    "2. **Link function**: $g: f \\mapsto \\theta$ (transforms GP to likelihood parameters)\n",
    "3. **Likelihood**: $y \\mid \\theta \\sim p(y \\mid \\theta)$\n",
    "\n",
    "This requires using `pm.gp.Latent` instead of `pm.gp.Marginal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Binary Classification with GP\n",
    "\n",
    "For binary classification, we use a Bernoulli likelihood with a logistic link function:\n",
    "\n",
    "$$p(y = 1 \\mid f) = \\text{logit}^{-1}(f) = \\frac{1}{1 + e^{-f}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_classification_demo():\n",
    "    # Generate synthetic classification data\n",
    "    n = 100\n",
    "    X = rng.uniform(-3, 3, n)[:, None]\n",
    "    \n",
    "    # True latent function\n",
    "    f_true = 2 * np.sin(X.flatten()) + 0.5 * X.flatten()**2 - 2\n",
    "    p_true = 1 / (1 + np.exp(-f_true))  # logistic\n",
    "    y = rng.binomial(1, p_true, n)\n",
    "    \n",
    "    # Test points for prediction\n",
    "    X_test = np.linspace(-3, 3, 100)[:, None]\n",
    "    \n",
    "    # GP Classification model\n",
    "    with pm.Model() as gp_classification:\n",
    "        # Kernel hyperparameters\n",
    "        ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "        η = pm.HalfCauchy(\"η\", beta=5)\n",
    "        \n",
    "        # Define covariance function\n",
    "        cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "        \n",
    "        # GP prior on latent function\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Bernoulli likelihood\n",
    "        p = pm.math.invlogit(f)  # logistic transformation\n",
    "        y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)\n",
    "        \n",
    "        # Fit model\n",
    "        idata = pm.sample(1000, tune=1000, chains=2, cores=1, \n",
    "                         random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        # Posterior predictions\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "    \n",
    "    # Extract results\n",
    "    f_pred_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_pred_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    p_pred_mean = 1 / (1 + np.exp(-f_pred_mean))  # Transform to probabilities\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training data\n",
    "    colors = ['red' if yi == 0 else 'blue' for yi in y]\n",
    "    symbols = ['circle' if yi == 0 else 'diamond' for yi in y]\n",
    "    \n",
    "    for yi in [0, 1]:\n",
    "        mask = y == yi\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[mask].flatten(),\n",
    "                y=np.full(np.sum(mask), yi),\n",
    "                mode='markers',\n",
    "                name=f'Class {yi}',\n",
    "                marker=dict(\n",
    "                    color='red' if yi == 0 else 'blue',\n",
    "                    size=8,\n",
    "                    symbol='circle' if yi == 0 else 'diamond'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Plot predicted probabilities\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=p_pred_mean,\n",
    "            name='Predicted P(y=1)',\n",
    "            line=dict(color='green', width=3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add uncertainty bands for latent function\n",
    "    f_upper = 1 / (1 + np.exp(-(f_pred_mean + 2*f_pred_std)))\n",
    "    f_lower = 1 / (1 + np.exp(-(f_pred_mean - 2*f_pred_std)))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "            y=np.concatenate([f_upper, f_lower[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 255, 0, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Credible Interval',\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"GP Classification Example\",\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"P(y=1)\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig, idata\n",
    "\n",
    "classification_fig, classification_idata = gp_classification_demo()\n",
    "classification_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Student-t Process for Robust Regression\n",
    "\n",
    "The Student-t process provides robustness to outliers by using a Student-t likelihood instead of Gaussian:\n",
    "\n",
    "$$y \\mid f, \\nu, \\sigma \\sim \\text{Student-t}(\\nu, f, \\sigma)$$\n",
    "\n",
    "Where $\\nu$ controls the tail heaviness (lower values = heavier tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_t_process_demo():\n",
    "    # Generate data with outliers\n",
    "    n = 50\n",
    "    X = np.linspace(-2, 2, n)[:, None]\n",
    "    \n",
    "    # True function\n",
    "    f_true = np.sin(2 * X.flatten())\n",
    "    \n",
    "    # Add normal noise + some outliers\n",
    "    noise = 0.1 * rng.standard_normal(n)\n",
    "    # Add some outliers\n",
    "    outlier_idx = rng.choice(n, size=5, replace=False)\n",
    "    noise[outlier_idx] += rng.choice([-2, 2], size=5) * rng.uniform(1, 3, 5)\n",
    "    \n",
    "    y = f_true + noise\n",
    "    \n",
    "    X_test = np.linspace(-2, 2, 100)[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Standard GP (Gaussian)\", \"Robust GP (Student-t)\"]\n",
    "    )\n",
    "    \n",
    "    # Standard Gaussian GP\n",
    "    with pm.Model() as gaussian_gp:\n",
    "        ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=2)\n",
    "        η = pm.HalfCauchy(\"η\", beta=5)\n",
    "        σ = pm.HalfCauchy(\"σ\", beta=2)\n",
    "        \n",
    "        cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=σ)\n",
    "        \n",
    "        idata_gaussian = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                                  random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        f_pred_gaussian = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_gaussian = pm.sample_posterior_predictive(idata_gaussian, progressbar=False)\n",
    "    \n",
    "    # Student-t GP\n",
    "    with pm.Model() as studentt_gp:\n",
    "        ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=2) \n",
    "        η = pm.HalfCauchy(\"η\", beta=5)\n",
    "        σ = pm.HalfCauchy(\"σ\", beta=2)\n",
    "        ν = pm.Gamma(\"ν\", alpha=2, beta=0.1)  # Degrees of freedom\n",
    "        \n",
    "        cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Student-t likelihood\n",
    "        y_ = pm.StudentT(\"y\", nu=ν, mu=f, sigma=σ, observed=y)\n",
    "        \n",
    "        idata_studentt = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                                  random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        f_pred_studentt = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_studentt = pm.sample_posterior_predictive(idata_studentt, progressbar=False)\n",
    "    \n",
    "    # Plot results\n",
    "    models = {\n",
    "        \"Gaussian\": (pred_gaussian, (1, 1)),\n",
    "        \"Student-t\": (pred_studentt, (1, 2))\n",
    "    }\n",
    "    \n",
    "    for name, (pred, pos) in models.items():\n",
    "        f_mean = pred.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "        f_std = pred.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "        \n",
    "        # Add training data\n",
    "        colors = ['red' if i in outlier_idx else 'black' for i in range(len(y))]\n",
    "        sizes = [10 if i in outlier_idx else 6 for i in range(len(y))]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(),\n",
    "                y=y,\n",
    "                mode='markers',\n",
    "                name='Data (outliers in red)' if pos[1] == 1 else 'Data',\n",
    "                marker=dict(color=colors, size=sizes),\n",
    "                showlegend=pos[1] == 1\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add GP predictions\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(),\n",
    "                y=f_mean,\n",
    "                name=f'{name} GP Mean',\n",
    "                line=dict(color='blue'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add uncertainty\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "                y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add true function\n",
    "        f_true_test = np.sin(2 * X_test.flatten())\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(),\n",
    "                y=f_true_test,\n",
    "                name='True Function' if pos[1] == 1 else 'True Function',\n",
    "                line=dict(color='green', dash='dash'),\n",
    "                showlegend=pos[1] == 1\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=500, title=\"GP Regression: Gaussian vs Student-t\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig, idata_gaussian, idata_studentt\n",
    "\n",
    "robust_fig, gauss_idata, studentt_idata = student_t_process_demo()\n",
    "robust_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Count Data with Poisson Likelihood\n",
    "\n",
    "For count data, we use a Poisson likelihood with a log link function:\n",
    "\n",
    "$$\\lambda = \\exp(f)$$\n",
    "$$y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_gp_demo():\n",
    "    # Generate synthetic count data\n",
    "    n = 60\n",
    "    X = np.linspace(0, 4*np.pi, n)[:, None]\n",
    "    \n",
    "    # True log-intensity function\n",
    "    f_true = 1 + 0.5 * np.sin(X.flatten()) + 0.3 * np.cos(2 * X.flatten())\n",
    "    lambda_true = np.exp(f_true)\n",
    "    y = rng.poisson(lambda_true)\n",
    "    \n",
    "    X_test = np.linspace(0, 4*np.pi, 100)[:, None]\n",
    "    \n",
    "    # Poisson GP model\n",
    "    with pm.Model() as poisson_gp:\n",
    "        # Kernel parameters\n",
    "        ℓ = pm.Gamma(\"ℓ\", alpha=2, beta=1)\n",
    "        η = pm.HalfCauchy(\"η\", beta=2)\n",
    "        \n",
    "        # GP prior on log-intensity\n",
    "        cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Poisson likelihood\n",
    "        λ = pm.math.exp(f)  # log link\n",
    "        y_obs = pm.Poisson(\"y_obs\", mu=λ, observed=y)\n",
    "        \n",
    "        # Fit model\n",
    "        idata = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                         random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        # Predictions\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "    \n",
    "    # Extract results\n",
    "    f_pred_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_pred_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    lambda_pred_mean = np.exp(f_pred_mean)\n",
    "    lambda_pred_upper = np.exp(f_pred_mean + 2*f_pred_std)\n",
    "    lambda_pred_lower = np.exp(f_pred_mean - 2*f_pred_std)\n",
    "    \n",
    "    # Plot results\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X.flatten(),\n",
    "            y=y,\n",
    "            mode='markers',\n",
    "            name='Count Data',\n",
    "            marker=dict(color='black', size=6)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # True intensity\n",
    "    lambda_true_test = np.exp(1 + 0.5 * np.sin(X_test.flatten()) + \n",
    "                             0.3 * np.cos(2 * X_test.flatten()))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=lambda_true_test,\n",
    "            name='True Intensity',\n",
    "            line=dict(color='green', dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Predicted intensity\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=lambda_pred_mean,\n",
    "            name='Predicted Intensity',\n",
    "            line=dict(color='blue', width=3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Uncertainty bands\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "            y=np.concatenate([lambda_pred_upper, lambda_pred_lower[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Credible Interval',\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"GP with Poisson Likelihood (Count Data)\",\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"Count / Intensity\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig, idata\n",
    "\n",
    "poisson_fig, poisson_idata = poisson_gp_demo()\n",
    "poisson_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Considerations for Non-Gaussian Likelihoods\n",
    "\n",
    "Using non-Gaussian likelihoods comes with computational trade-offs:\n",
    "\n",
    "### 1. **Inference Complexity**\n",
    "- **Gaussian**: Analytical posterior (fast)\n",
    "- **Non-Gaussian**: MCMC required (slower)\n",
    "\n",
    "### 2. **Model Specification**\n",
    "- **Marginal**: `pm.gp.Marginal` for Gaussian likelihoods only\n",
    "- **Latent**: `pm.gp.Latent` required for non-Gaussian likelihoods\n",
    "\n",
    "### 3. **Hyperparameter Sensitivity**\n",
    "- Non-Gaussian models often more sensitive to priors\n",
    "- More careful initialization may be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of likelihood choices\n",
    "likelihood_guide = {\n",
    "    \"Data Type\": [\"Continuous (symmetric noise)\", \"Continuous (outliers)\", \n",
    "                 \"Binary\", \"Count\", \"Positive continuous\"],\n",
    "    \"Likelihood\": [\"Gaussian\", \"Student-t\", \"Bernoulli\", \"Poisson\", \"Gamma/Lognormal\"],\n",
    "    \"Link Function\": [\"Identity\", \"Identity\", \"Logistic\", \"Log\", \"Log\"],\n",
    "    \"PyMC Implementation\": [\"gp.Marginal\", \"gp.Latent\", \"gp.Latent\", \"gp.Latent\", \"gp.Latent\"]\n",
    "}\n",
    "\n",
    "likelihood_df = pd.DataFrame(likelihood_guide)\n",
    "print(\"GP Likelihood Selection Guide:\")\n",
    "print(\"=\" * 60)\n",
    "print(likelihood_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part D: Model Building Best Practices\n",
    "\n",
    "Building effective GP models requires careful consideration of several factors. Let's explore key best practices for successful GP modeling.\n",
    "\n",
    "## 1. Choosing Between Marginal vs Latent\n",
    "\n",
    "The choice between `pm.gp.Marginal` and `pm.gp.Latent` has important implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Marginal vs Latent implementations\n",
    "comparison_data = {\n",
    "    \"Aspect\": [\"Computational Speed\", \"Likelihood Types\", \"Memory Usage\", \n",
    "              \"Posterior Samples\", \"Prediction\", \"Scalability\"],\n",
    "    \"pm.gp.Marginal\": [\"Fast (analytical)\", \"Gaussian only\", \"Low\", \n",
    "                       \"Parameters only\", \"Direct\", \"Better\"],\n",
    "    \"pm.gp.Latent\": [\"Slower (MCMC)\", \"Any likelihood\", \"Higher\", \n",
    "                     \"Full GP samples\", \"Via conditional\", \"Limited\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Marginal vs Latent Implementation Comparison:\")\n",
    "print(\"=\" * 55)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Priors and Initialization\n",
    "\n",
    "Proper prior specification is crucial for GP models. Here are some guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_prior_sensitivity():\n",
    "    \"\"\"Show impact of different prior choices on GP inference\"\"\"\n",
    "    \n",
    "    # Generate simple test data\n",
    "    X = np.linspace(0, 1, 20)[:, None]\n",
    "    y = np.sin(4 * np.pi * X.flatten()) + 0.1 * rng.standard_normal(20)\n",
    "    \n",
    "    X_test = np.linspace(0, 1, 100)[:, None]\n",
    "    \n",
    "    # Different prior specifications\n",
    "    prior_configs = {\n",
    "        \"Informative\": {\n",
    "            \"ℓ_prior\": pm.Gamma(\"ℓ\", alpha=4, beta=20),  # Small lengthscale\n",
    "            \"η_prior\": pm.HalfNormal(\"η\", sigma=1),        # Moderate variance\n",
    "        },\n",
    "        \"Weakly Informative\": {\n",
    "            \"ℓ_prior\": pm.Gamma(\"ℓ\", alpha=2, beta=2),   # Moderate lengthscale\n",
    "            \"η_prior\": pm.HalfCauchy(\"η\", beta=2),        # Moderate variance\n",
    "        },\n",
    "        \"Vague\": {\n",
    "            \"ℓ_prior\": pm.Gamma(\"ℓ\", alpha=1, beta=0.1), # Very flexible\n",
    "            \"η_prior\": pm.HalfCauchy(\"η\", beta=10),       # Very flexible\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=list(prior_configs.keys())\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, (name, priors) in enumerate(prior_configs.items()):\n",
    "        with pm.Model() as model:\n",
    "            # Apply different priors\n",
    "            ℓ = priors[\"ℓ_prior\"]\n",
    "            η = priors[\"η_prior\"]\n",
    "            σ = pm.HalfNormal(\"σ\", sigma=0.5)\n",
    "            \n",
    "            cov = η**2 * pm.gp.cov.ExpQuad(1, ls=ℓ)\n",
    "            gp = pm.gp.Marginal(cov_func=cov)\n",
    "            y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=σ)\n",
    "            \n",
    "            try:\n",
    "                idata = pm.sample(500, tune=500, chains=2, cores=1,\n",
    "                                random_seed=RANDOM_SEED, progressbar=False)\n",
    "                \n",
    "                f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "                pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "                \n",
    "                f_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "                f_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "                \n",
    "                results[name] = (f_mean, f_std, idata)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Sampling failed for {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Plot data\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=y,\n",
    "                mode='markers', name='Data' if i == 0 else None,\n",
    "                marker=dict(color='black', size=4),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot true function\n",
    "        y_true = np.sin(4 * np.pi * X_test.flatten())\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(), y=y_true,\n",
    "                name='True' if i == 0 else None, \n",
    "                line=dict(color='green', dash='dash'),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        \n",
    "        if name in results:\n",
    "            f_mean, f_std, _ = results[name]\n",
    "            \n",
    "            # Plot GP prediction\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X_test.flatten(), y=f_mean,\n",
    "                    name='GP' if i == 0 else None,\n",
    "                    line=dict(color='blue'),\n",
    "                    showlegend=(i == 0)\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "            \n",
    "            # Uncertainty\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "                    y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "                    fill='toself',\n",
    "                    fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "                    line=dict(color='rgba(255,255,255,0)'),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Impact of Prior Specification\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig, results\n",
    "\n",
    "prior_fig, prior_results = demonstrate_prior_sensitivity()\n",
    "prior_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Prior Guidelines\n",
    "\n",
    "Based on the above example and general experience, here are some guidelines for choosing priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior recommendations\n",
    "prior_recommendations = {\n",
    "    \"Parameter\": [\"Lengthscale (ℓ)\", \"Amplitude (η)\", \"Noise (σ)\", \"Period (p)\"],\n",
    "    \"Recommended Prior\": [\n",
    "        \"Gamma(2, β) where β ≈ 2/expected_scale\",\n",
    "        \"HalfCauchy(β) where β ≈ std(y)\", \n",
    "        \"HalfNormal(σ) where σ ≈ 0.1 * std(y)\",\n",
    "        \"Normal(μ, σ) where μ = expected period\"\n",
    "    ],\n",
    "    \"Rationale\": [\n",
    "        \"Weakly informative, avoids extreme values\",\n",
    "        \"Heavy tails allow flexibility\",\n",
    "        \"Conservative, prevents overfitting\",\n",
    "        \"Domain knowledge essential\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "prior_df = pd.DataFrame(prior_recommendations)\n",
    "print(\"GP Hyperparameter Prior Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "print(prior_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Validation and Diagnostics\n",
    "\n",
    "Proper model validation is essential for reliable GP models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_diagnostics(idata, model_name=\"GP Model\"):\n",
    "    \"\"\"Comprehensive diagnostics for GP models\"\"\"\n",
    "    \n",
    "    print(f\"Diagnostics for {model_name}:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Convergence diagnostics\n",
    "    summary = az.summary(idata, hdi_prob=0.95)\n",
    "    \n",
    "    # Check R-hat values\n",
    "    rhat_issues = summary[summary['r_hat'] > 1.1]\n",
    "    if len(rhat_issues) > 0:\n",
    "        print(\"⚠️  WARNING: Some parameters have R̂ > 1.1:\")\n",
    "        print(rhat_issues[['mean', 'r_hat']].to_string())\n",
    "    else:\n",
    "        print(\"✅ All R̂ values < 1.1 (good convergence)\")\n",
    "    \n",
    "    # Check effective sample size\n",
    "    ess_issues = summary[summary['ess_bulk'] < 400]\n",
    "    if len(ess_issues) > 0:\n",
    "        print(\"\\n⚠️  WARNING: Low effective sample size:\")\n",
    "        print(ess_issues[['ess_bulk', 'ess_tail']].to_string())\n",
    "    else:\n",
    "        print(\"✅ Adequate effective sample sizes\")\n",
    "    \n",
    "    # 2. Print key parameter estimates\n",
    "    print(\"\\nParameter Estimates:\")\n",
    "    print(summary[['mean', 'hdi_2.5%', 'hdi_97.5%']].round(4).to_string())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example with our classification model\n",
    "if 'classification_idata' in locals():\n",
    "    classification_diagnostics = gp_diagnostics(classification_idata, \"GP Classification\")\n",
    "else:\n",
    "    print(\"Classification model not available for diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computational Efficiency Tips\n",
    "\n",
    "For larger datasets or complex models, consider these optimization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational efficiency tips\n",
    "efficiency_tips = {\n",
    "    \"Strategy\": [\n",
    "        \"Use pm.gp.Marginal when possible\",\n",
    "        \"Add jitter to diagonal\", \n",
    "        \"Use sparse/inducing points\",\n",
    "        \"Hierarchical GPs for grouped data\",\n",
    "        \"Consider HSGP approximation\",\n",
    "        \"Proper kernel scaling\"\n",
    "    ],\n",
    "    \"When to Use\": [\n",
    "        \"Gaussian likelihood\",\n",
    "        \"Numerical instability\",\n",
    "        \"Large datasets (n > 1000)\", \n",
    "        \"Multiple similar time series\",\n",
    "        \"Stationary kernels, large n\",\n",
    "        \"Always\"\n",
    "    ],\n",
    "    \"Implementation\": [\n",
    "        \"pm.gp.Marginal(cov_func=cov)\",\n",
    "        \"cov_func + 1e-6 * pm.gp.cov.WhiteNoise()\",\n",
    "        \"Use inducing points\",\n",
    "        \"Shared hyperparameters\", \n",
    "        \"pm.gp.HSGP\",\n",
    "        \"Scale inputs to [0,1] or [-1,1]\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_tips)\n",
    "print(\"GP Computational Efficiency Strategies:\")\n",
    "print(\"=\" * 45)\n",
    "print(efficiency_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "In this session, we've covered the essential building blocks for advanced GP modeling:\n",
    "\n",
    "## 🎯 **Key Concepts Mastered**\n",
    "\n",
    "1. **Kernel Selection**: Understanding when to use RBF, Matérn, Periodic, and other kernels\n",
    "2. **Kernel Composition**: Building complex patterns through addition and multiplication\n",
    "3. **Non-Gaussian Likelihoods**: Extending GPs beyond continuous data\n",
    "4. **Model Building**: Best practices for robust, efficient GP models\n",
    "\n",
    "## 📋 **Practical Guidelines**\n",
    "\n",
    "### Kernel Selection Flowchart\n",
    "```\n",
    "Data Characteristics → Kernel Choice\n",
    "├── Smooth, no periodicity → RBF\n",
    "├── Some roughness → Matérn (3/2 or 5/2) \n",
    "├── Periodic patterns → Periodic (+ RBF for trend)\n",
    "├── Multiple scales → Additive kernels\n",
    "└── Multiplicative patterns → Product kernels\n",
    "```\n",
    "\n",
    "### Implementation Strategy\n",
    "```\n",
    "Problem Type → Implementation\n",
    "├── Gaussian noise → pm.gp.Marginal\n",
    "├── Classification → pm.gp.Latent + Bernoulli\n",
    "├── Count data → pm.gp.Latent + Poisson\n",
    "├── Robust regression → pm.gp.Latent + StudentT\n",
    "└── Large datasets → Consider HSGP or sparse approximations\n",
    "```\n",
    "\n",
    "## 🚀 **Next Steps**\n",
    "\n",
    "Building on today's foundation, the remaining sessions will cover:\n",
    "- **Session 3**: Advanced topics and computational methods\n",
    "- **Session 4**: Real-world applications and case studies\n",
    "\n",
    "## 💡 **Practice Exercises**\n",
    "\n",
    "Try applying these concepts to your own data:\n",
    "\n",
    "1. **Start Simple**: Begin with basic RBF kernels\n",
    "2. **Add Complexity**: Experiment with kernel composition\n",
    "3. **Match Likelihood**: Choose appropriate likelihood for your data type\n",
    "4. **Validate**: Always check convergence and model fit\n",
    "5. **Compare**: Test multiple kernel choices and compare performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Additional Resources\n",
    "\n",
    "For deeper understanding, explore:\n",
    "\n",
    "- **PyMC Documentation**: [GP module documentation](https://www.pymc.io/projects/docs/en/stable/api/gp.html)\n",
    "- **Rasmussen & Williams**: *Gaussian Processes for Machine Learning* (the definitive textbook)\n",
    "- **PyMC Examples**: [GP gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/index.html)\n",
    "- **Kernel Cookbook**: David Duvenaud's kernel cookbook for kernel selection guidance\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 2** 🎉\n",
    "\n",
    "*You now have the tools to build sophisticated GP models for a wide range of real-world problems!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}