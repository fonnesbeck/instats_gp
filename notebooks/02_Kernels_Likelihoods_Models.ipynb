{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 2: Kernels, Likelihoods, and Model Building\n",
    "\n",
    "Welcome to Session 2 of our Gaussian Processes with PyMC workshop! In the previous session, we covered the fundamentals of Gaussian processes and their implementation in PyMC. Today, we'll dive deeper into the building blocks that make GPs so powerful and flexible.\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Master different kernel families** and understand when to use each type\n",
    "2. **Understand kernel composition** and how to combine kernels for complex patterns\n",
    "3. **Implement non-Gaussian likelihoods** for classification, count data, and robust regression\n",
    "4. **Navigate inference trade-offs** between computational efficiency and model flexibility\n",
    "5. **Build robust GP models** that handle real-world complexities\n",
    "\n",
    "## Session Overview\n",
    "\n",
    "### Part A: Kernel Functions Deep Dive\n",
    "- Understanding the role of kernels in GP modeling\n",
    "- Exploring different kernel families (RBF, Mat√©rn, Periodic, etc.)\n",
    "- Kernel hyperparameters and their effects\n",
    "- Practical guidelines for kernel selection\n",
    "\n",
    "### Part B: Kernel Composition\n",
    "- Mathematical foundations of kernel combination\n",
    "- Addition and multiplication of kernels\n",
    "- Building complex patterns through composition\n",
    "- Real-world examples of composite kernels\n",
    "\n",
    "### Part C: Non-Gaussian Likelihoods\n",
    "- Beyond Gaussian noise: classification and count data\n",
    "- Student-t processes for robust regression\n",
    "- Latent variable implementation for non-Gaussian outcomes\n",
    "- Computational considerations\n",
    "\n",
    "### Part D: Model Building Best Practices\n",
    "- Choosing between Marginal vs Latent implementations\n",
    "- Hyperparameter priors and initialization\n",
    "- Model validation and diagnostics\n",
    "- Handling computational challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "import pytensor.tensor as pt\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "# Configure plotting\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A: Kernel Functions Deep Dive\n",
    "\n",
    "The kernel (or covariance) function is the heart of a Gaussian process. It encodes our assumptions about the smoothness, periodicity, and other structural properties of the function we're modeling.\n",
    "\n",
    "## Understanding Kernel Functions\n",
    "\n",
    "A kernel function $k(x, x')$ measures the similarity between two input points $x$ and $x'$. The key properties of a valid kernel are:\n",
    "\n",
    "1. **Symmetry**: $k(x, x') = k(x', x)$\n",
    "2. **Positive semi-definiteness**: The covariance matrix must be positive semi-definite\n",
    "\n",
    "Let's explore the most commonly used kernel families and their characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Radial Basis Function (RBF) Kernel\n",
    "\n",
    "The RBF kernel (also called Gaussian or squared exponential kernel) is perhaps the most widely used kernel:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{(x - x')^2}{2\\ell^2}\\right)$$\n",
    "\n",
    "Where:\n",
    "- $\\sigma^2$ is the variance parameter (controls output scale)\n",
    "- $\\ell$ is the lengthscale parameter (controls input scale)\n",
    "\n",
    "The RBF kernel assumes infinite differentiability, making it suitable for smooth functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explore how RBF kernel parameters affect the covariance structure\n",
    "def plot_kernel_comparison():\n",
    "    X = np.linspace(-3, 3, 100)[:, None]\n",
    "    x_test = np.array([[0.0]])  # Reference point\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Effect of Lengthscale (œÉ¬≤=1)\",\n",
    "            \"Effect of Variance (‚Ñì=1)\", \n",
    "            \"Sample Functions (‚Ñì=0.5)\",\n",
    "            \"Sample Functions (‚Ñì=2.0)\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Effect of lengthscale\n",
    "    lengthscales = [0.3, 1.0, 2.0]\n",
    "    colors = ['red', 'blue', 'green']\n",
    "    \n",
    "    for i, (ls, color) in enumerate(zip(lengthscales, colors)):\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'‚Ñì={ls}', line=dict(color=color)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Effect of variance\n",
    "    variances = [0.5, 1.0, 2.0]\n",
    "    \n",
    "    for i, (var, color) in enumerate(zip(variances, colors)):\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(var, ls=1.0)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'œÉ¬≤={var}', line=dict(color=color),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Sample functions with different lengthscales\n",
    "    for ls, row_col in [(0.5, (2,1)), (2.0, (2,2))]:\n",
    "        with pm.Model() as model:\n",
    "            cov = pm.gp.cov.ExpQuad(1, ls=ls)\n",
    "            K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "            \n",
    "        # Sample from the prior\n",
    "        L = np.linalg.cholesky(K)\n",
    "        samples = L @ rng.standard_normal((len(X), 3))\n",
    "        \n",
    "        for i in range(3):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X.flatten(), y=samples[:, i],\n",
    "                    name=f'Sample {i+1}', \n",
    "                    line=dict(color=colors[i]),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=row_col[0], col=row_col[1]\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=600, title=\"RBF Kernel Properties\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"k(x, 0)\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "plot_kernel_comparison().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights:**\n",
    "- **Lengthscale ($\\ell$)**: Controls how far the influence of a data point extends. Smaller values create more wiggly functions.\n",
    "- **Variance ($\\sigma^2$)**: Controls the overall scale of variation. Higher values allow for larger deviations from the mean.\n",
    "\n",
    "## 2. Mat√©rn Kernel Family\n",
    "\n",
    "The Mat√©rn kernel family provides more flexibility in modeling smoothness:\n",
    "\n",
    "$$k(x, x') = \\frac{2^{1-\\nu}}{\\Gamma(\\nu)} \\left(\\sqrt{2\\nu} \\frac{|x - x'|}{\\ell}\\right)^\\nu K_\\nu\\left(\\sqrt{2\\nu} \\frac{|x - x'|}{\\ell}\\right)$$\n",
    "\n",
    "Where $\\nu$ controls smoothness and $K_\\nu$ is the modified Bessel function of the second kind.\n",
    "\n",
    "Common choices:\n",
    "- $\\nu = 1/2$: Exponential kernel (non-differentiable)\n",
    "- $\\nu = 3/2$: Once differentiable\n",
    "- $\\nu = 5/2$: Twice differentiable\n",
    "- $\\nu \\to \\infty$: RBF kernel (infinitely differentiable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_matern_kernels():\n",
    "    X = np.linspace(-3, 3, 100)[:, None]\n",
    "    x_test = np.array([[0.0]])\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Mat√©rn Kernel Comparison\", \"Sample Functions\"]\n",
    "    )\n",
    "    \n",
    "    nus = [0.5, 1.5, 2.5, np.inf]\n",
    "    nu_labels = ['1/2', '3/2', '5/2', '‚àû (RBF)']\n",
    "    colors = ['red', 'blue', 'green', 'orange']\n",
    "    \n",
    "    # Compare kernel shapes\n",
    "    for nu, label, color in zip(nus, nu_labels, colors):\n",
    "        with pm.Model() as model:\n",
    "            if nu == np.inf:\n",
    "                cov = pm.gp.cov.ExpQuad(1, ls=1.0)\n",
    "            else:\n",
    "                cov = pm.gp.cov.Matern52(1, ls=1.0) if nu == 2.5 else \\\n",
    "                      pm.gp.cov.Matern32(1, ls=1.0) if nu == 1.5 else \\\n",
    "                      pm.gp.cov.Exponential(1, ls=1.0)\n",
    "            K = cov(X, x_test).eval()\n",
    "            \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=K.flatten(),\n",
    "                name=f'ŒΩ={label}', line=dict(color=color)\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Sample functions from Mat√©rn 3/2\n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Matern32(1, ls=1.0)\n",
    "        K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "        \n",
    "    L = np.linalg.cholesky(K)\n",
    "    samples = L @ rng.standard_normal((len(X), 3))\n",
    "    \n",
    "    for i in range(3):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=samples[:, i],\n",
    "                name=f'Mat√©rn 3/2 Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Mat√©rn Kernel Family\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"Kernel Value / Function Value\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "compare_matern_kernels().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Periodic Kernel\n",
    "\n",
    "The periodic kernel is designed for functions with repeating patterns:\n",
    "\n",
    "$$k(x, x') = \\sigma^2 \\exp\\left(-\\frac{2\\sin^2(\\pi|x - x'|/p)}{\\ell^2}\\right)$$\n",
    "\n",
    "Where $p$ is the period parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_periodic_kernel():\n",
    "    X = np.linspace(0, 8, 200)[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Periodic Kernel (p=2œÄ)\", \"Sample Functions\"]\n",
    "    )\n",
    "    \n",
    "    # Kernel visualization\n",
    "    x_test = np.array([[2.0]])\n",
    "    \n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0)\n",
    "        K = cov(X, x_test).eval()\n",
    "        \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X.flatten(), y=K.flatten(),\n",
    "            name='Periodic Kernel', line=dict(color='blue')\n",
    "        ),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    # Sample functions\n",
    "    with pm.Model() as model:\n",
    "        cov = pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0)\n",
    "        K = cov(X).eval() + 1e-6 * np.eye(len(X))\n",
    "        \n",
    "    L = np.linalg.cholesky(K)\n",
    "    samples = L @ rng.standard_normal((len(X), 3))\n",
    "    \n",
    "    colors = ['red', 'green', 'orange']\n",
    "    for i in range(3):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=samples[:, i],\n",
    "                name=f'Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Periodic Kernel\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "demonstrate_periodic_kernel().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part B: Kernel Composition\n",
    "\n",
    "One of the most powerful aspects of GP modeling is the ability to combine kernels to create more complex covariance structures. This allows us to model functions with multiple characteristics simultaneously.\n",
    "\n",
    "## Mathematical Foundations\n",
    "\n",
    "If $k_1$ and $k_2$ are valid kernels, then:\n",
    "\n",
    "1. **Addition**: $k(x, x') = k_1(x, x') + k_2(x, x')$ models functions that are the sum of processes with different characteristics\n",
    "\n",
    "2. **Multiplication**: $k(x, x') = k_1(x, x') \\cdot k_2(x, x')$ models functions where both characteristics must be present simultaneously\n",
    "\n",
    "## Example: Trend + Seasonality + Noise\n",
    "\n",
    "Let's build a kernel for modeling data with:\n",
    "- Long-term smooth trends\n",
    "- Seasonal patterns\n",
    "- Short-term variations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_kernel_composition():\n",
    "    # Generate synthetic data with trend, seasonality, and noise\n",
    "    X = np.linspace(0, 4*np.pi, 100)\n",
    "    \n",
    "    # True components\n",
    "    trend = 0.1 * X\n",
    "    seasonal = 0.5 * np.sin(X)\n",
    "    noise = 0.1 * rng.standard_normal(len(X))\n",
    "    y_true = trend + seasonal + noise\n",
    "    \n",
    "    # Convert to training data\n",
    "    X_train = X[::5]  # Subsample for training\n",
    "    y_train = y_true[::5]\n",
    "    X_test = X[:, None]\n",
    "    X_train = X_train[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=2,\n",
    "        subplot_titles=[\n",
    "            \"Individual Kernels\",\n",
    "            \"Composite Kernel Samples\",\n",
    "            \"GP Regression: Simple Kernel\",\n",
    "            \"GP Regression: Composite Kernel\"\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    # Plot individual kernel samples\n",
    "    kernels = {\n",
    "        'Long-term (RBF ‚Ñì=3)': pm.gp.cov.ExpQuad(1, ls=3.0),\n",
    "        'Seasonal (Periodic p=2œÄ)': pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0),\n",
    "        'Short-term (RBF ‚Ñì=0.5)': pm.gp.cov.ExpQuad(1, ls=0.5)\n",
    "    }\n",
    "    \n",
    "    colors = ['blue', 'red', 'green']\n",
    "    for i, (name, kernel) in enumerate(kernels.items()):\n",
    "        with pm.Model():\n",
    "            K = kernel(X_test).eval() + 1e-6 * np.eye(len(X_test))\n",
    "        \n",
    "        L = np.linalg.cholesky(K)\n",
    "        sample = L @ rng.standard_normal(len(X_test))\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, y=sample.flatten(),\n",
    "                name=name, line=dict(color=colors[i])\n",
    "            ),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    # Composite kernel samples\n",
    "    with pm.Model():\n",
    "        composite_kernel = (pm.gp.cov.ExpQuad(1, ls=3.0) + \n",
    "                          pm.gp.cov.Periodic(1, period=2*np.pi, ls=1.0) +\n",
    "                          pm.gp.cov.ExpQuad(0.5, ls=0.5))\n",
    "        K_comp = composite_kernel(X_test).eval() + 1e-6 * np.eye(len(X_test))\n",
    "    \n",
    "    L_comp = np.linalg.cholesky(K_comp)\n",
    "    for i in range(3):\n",
    "        sample = L_comp @ rng.standard_normal(len(X_test))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X, y=sample.flatten(),\n",
    "                name=f'Composite Sample {i+1}',\n",
    "                line=dict(color=colors[i]),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    # Add true data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_train.flatten(), y=y_train,\n",
    "            mode='markers', name='Training Data',\n",
    "            marker=dict(color='black', size=4)\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_train.flatten(), y=y_train,\n",
    "            mode='markers', name='Training Data',\n",
    "            marker=dict(color='black', size=4),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    # Simple GP regression\n",
    "    with pm.Model() as simple_model:\n",
    "        ‚Ñì = pm.Gamma(\"‚Ñì\", alpha=2, beta=1)\n",
    "        Œ∑ = pm.HalfCauchy(\"Œ∑\", beta=5)\n",
    "        \n",
    "        cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        œÉ = pm.HalfCauchy(\"œÉ\", beta=5)\n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=œÉ)\n",
    "        \n",
    "        # Predict\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        \n",
    "        # Sample from posterior\n",
    "        idata_simple = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED, \n",
    "                                chains=2, cores=1, progressbar=False)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata_simple, \n",
    "                                                     progressbar=False)\n",
    "    \n",
    "    # Plot simple GP results\n",
    "    f_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X, y=f_mean,\n",
    "            name='GP Mean (Simple)',\n",
    "            line=dict(color='blue'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X, X[::-1]]),\n",
    "            y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    \n",
    "    # Composite GP regression (simplified for demo)\n",
    "    with pm.Model() as composite_model:\n",
    "        # Long-term component\n",
    "        ‚Ñì_long = pm.Gamma(\"‚Ñì_long\", alpha=2, beta=0.5)\n",
    "        Œ∑_long = pm.HalfCauchy(\"Œ∑_long\", beta=2)\n",
    "        \n",
    "        # Periodic component  \n",
    "        ‚Ñì_per = pm.Gamma(\"‚Ñì_per\", alpha=2, beta=2)\n",
    "        Œ∑_per = pm.HalfCauchy(\"Œ∑_per\", beta=2)\n",
    "        \n",
    "        # Short-term component\n",
    "        ‚Ñì_short = pm.Gamma(\"‚Ñì_short\", alpha=2, beta=4)\n",
    "        Œ∑_short = pm.HalfCauchy(\"Œ∑_short\", beta=1)\n",
    "        \n",
    "        # Composite kernel\n",
    "        cov = (Œ∑_long**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì_long) +\n",
    "               Œ∑_per**2 * pm.gp.cov.Periodic(1, period=2*np.pi, ls=‚Ñì_per) +\n",
    "               Œ∑_short**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì_short))\n",
    "        \n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        œÉ = pm.HalfCauchy(\"œÉ\", beta=0.5)\n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=œÉ)\n",
    "        \n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        \n",
    "        idata_composite = pm.sample(1000, tune=1000, random_seed=RANDOM_SEED,\n",
    "                                  chains=2, cores=1, progressbar=False)\n",
    "        pred_samples_comp = pm.sample_posterior_predictive(idata_composite,\n",
    "                                                          progressbar=False)\n",
    "    \n",
    "    # Plot composite GP results\n",
    "    f_mean_comp = pred_samples_comp.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_std_comp = pred_samples_comp.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X, y=f_mean_comp,\n",
    "            name='GP Mean (Composite)',\n",
    "            line=dict(color='red'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X, X[::-1]]),\n",
    "            y=np.concatenate([f_mean_comp + 2*f_std_comp, \n",
    "                            (f_mean_comp - 2*f_std_comp)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(255, 0, 0, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False\n",
    "        ),\n",
    "        row=2, col=2\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(height=800, title=\"Kernel Composition Example\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig\n",
    "\n",
    "demonstrate_kernel_composition().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insights from Kernel Composition:**\n",
    "\n",
    "1. **Additive kernels** allow modeling functions as sums of different components\n",
    "2. **Individual kernels** capture specific patterns (trends, seasonality, noise)\n",
    "3. **Composite models** can capture complex real-world patterns that simple kernels cannot\n",
    "4. **Parameter interpretation** becomes more challenging but also more meaningful\n",
    "\n",
    "## Common Kernel Combinations\n",
    "\n",
    "Here are some useful kernel combinations for different scenarios:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common kernel combination patterns\n",
    "kernel_combinations = {\n",
    "    \"Smooth trend + noise\": \"RBF(large_‚Ñì) + RBF(small_‚Ñì)\",\n",
    "    \"Periodic + trend\": \"Periodic √ó RBF + RBF(large_‚Ñì)\", \n",
    "    \"Local periodicity\": \"Periodic √ó RBF\",\n",
    "    \"Changepoint model\": \"RBF + Polynomial + White Noise\",\n",
    "    \"Multi-scale patterns\": \"RBF(‚Ñì‚ÇÅ) + RBF(‚Ñì‚ÇÇ) + RBF(‚Ñì‚ÇÉ)\"\n",
    "}\n",
    "\n",
    "print(\"Common Kernel Combination Patterns:\")\n",
    "print(\"=\" * 40)\n",
    "for use_case, formula in kernel_combinations.items():\n",
    "    print(f\"{use_case:20}: {formula}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Hands-On Exercise 1: LLM-Assisted Kernel Composition\n",
    "\n",
    "Let's practice using LLMs to help us design and implement composite kernels for complex real-world patterns. This exercise will teach you how to effectively collaborate with AI assistants for advanced GP modeling.\n",
    "\n",
    "### Exercise Goals\n",
    "\n",
    "1. Use your LLM to analyze data patterns and suggest appropriate kernel compositions\n",
    "2. Get AI assistance in implementing complex composite kernels\n",
    "3. Learn to ask effective questions about kernel properties and behavior\n",
    "4. Practice debugging kernel implementation issues with LLM help\n",
    "\n",
    "### Advanced LLM Prompting for Kernel Design\n",
    "\n",
    "When working with LLMs on kernel composition, try these specific strategies:\n",
    "\n",
    "**For Pattern Analysis:**\n",
    "- \"I have time series data with [describe patterns]. What kernel combination would work best?\"\n",
    "- \"My data shows both trend and seasonality. Help me design a composite kernel.\"\n",
    "\n",
    "**For Implementation:**\n",
    "- \"Show me how to implement [kernel1] + [kernel2] in PyMC with proper hyperparameters\"\n",
    "- \"Help me debug this kernel composition - the model isn't converging\"\n",
    "\n",
    "**For Understanding:**\n",
    "- \"Explain when to use multiplication vs addition for kernel combination\"\n",
    "- \"What are the trade-offs of using complex composite kernels?\"\n",
    "\n",
    "### Ready-to-Use Prompts for This Exercise\n",
    "\n",
    "```\n",
    "PROMPT 1: \"I need to model time series data that has: (1) a long-term trend, \n",
    "(2) seasonal patterns, and (3) short-term noise. Help me design a composite \n",
    "kernel using PyMC that captures all three components. Explain why you chose \n",
    "each kernel and how they combine.\"\n",
    "\n",
    "PROMPT 2: \"Show me how to implement a locally periodic pattern using\n",
    "kernel multiplication in PyMC. I want periodicity that fades away from\n",
    "certain regions. Include proper hyperparameter priors.\"\n",
    "\n",
    "PROMPT 3: \"My composite kernel (RBF + Periodic + Linear) is causing sampling\n",
    "issues in PyMC. Help me diagnose what's wrong and suggest fixes for\n",
    "better convergence. Here's my current code: [paste your attempt].\"\n",
    "\n",
    "PROMPT 4: \"Help me compare 3 different kernel designs for the same dataset:\n",
    "simple RBF, RBF + Periodic, and RBF √ó Periodic. Show me how to implement\n",
    "model comparison with WAIC or LOO in PyMC.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ EXERCISE: Use your LLM to design composite kernels for different scenarios\n",
    "\n",
    "# Scenario: Complex time series with multiple patterns\n",
    "np.random.seed(42)\n",
    "t = np.linspace(0, 4*np.pi, 80)[:, None]\n",
    "trend = 0.2 * t.flatten()  # Linear trend\n",
    "seasonal = 0.6 * np.sin(t.flatten())  # Seasonal pattern\n",
    "noise = 0.15 * rng.standard_normal(80)  # Random noise\n",
    "complex_data = trend + seasonal + noise\n",
    "\n",
    "print(\"Complex time series created with trend + seasonality + noise\")\n",
    "print(\"Challenge: Use your LLM to design appropriate composite kernels!\")\n",
    "\n",
    "# TASK 1: Get LLM help to analyze the data pattern\n",
    "def analyze_data_patterns_with_llm():\n",
    "    \"\"\"\n",
    "    Ask your LLM to help analyze what patterns are present in the data.\n",
    "    \n",
    "    Suggested prompt: \"I have time series data with the following characteristics:\n",
    "    - Linear upward trend (coefficient ~0.2)\n",
    "    - Sinusoidal seasonal pattern with amplitude 0.6 and period 2œÄ\n",
    "    - Gaussian noise with std ~0.15\n",
    "    - 80 data points over interval [0, 4œÄ]\n",
    "    What composite kernel would you recommend and why? Show me how to \n",
    "    implement it in PyMC with appropriate hyperparameter priors.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED ANALYSIS HERE\n",
    "    pass\n",
    "\n",
    "# TASK 2: Implement additive kernel composition with LLM help\n",
    "def build_additive_composite_with_llm():\n",
    "    \"\"\"\n",
    "    Use your LLM to help implement: RBF (trend) + Periodic (seasonal) + WhiteNoise\n",
    "    \n",
    "    Suggested prompt: \"Help me implement an additive composite kernel in PyMC\n",
    "    that combines: (1) RBF kernel for smooth trends, (2) Periodic kernel for\n",
    "    seasonality with period 2œÄ, and (3) WhiteNoise for measurement error. \n",
    "    Show me proper hyperparameter priors for each component and how to fit\n",
    "    the model to my data.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# TASK 3: Get LLM help for multiplicative kernels\n",
    "def build_multiplicative_composite_with_llm():\n",
    "    \"\"\"\n",
    "    Get LLM help to create: RBF √ó Periodic for locally periodic behavior\n",
    "    \n",
    "    Suggested prompt: \"Show me how to create a locally periodic pattern using\n",
    "    multiplicative kernels (RBF √ó Periodic) in PyMC. I want to understand\n",
    "    the difference between additive and multiplicative combinations.\n",
    "    Explain the mathematical intuition and show me the implementation.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# TASK 4: Get help with model comparison\n",
    "def compare_kernel_designs_with_llm():\n",
    "    \"\"\"\n",
    "    Ask your LLM to help compare different compositions and select the best.\n",
    "    \n",
    "    Suggested prompt: \"Help me set up a systematic comparison between these\n",
    "    kernel designs for my time series data:\n",
    "    1. Simple RBF only\n",
    "    2. RBF + Periodic (additive)\n",
    "    3. RBF √ó Periodic (multiplicative)\n",
    "    4. RBF + Periodic + Linear (full additive)\n",
    "    Show me how to fit all models, compute WAIC/LOO for comparison, and\n",
    "    create visualizations to understand the differences.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "print(\"üéØ Goal: Complete all tasks using your LLM as a knowledgeable collaborator!\")\n",
    "print(\"üìä Ask your LLM to help with visualization and interpretation too.\")\n",
    "print(\"ü§î Don't hesitate to ask follow-up questions about anything unclear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part C: Non-Gaussian Likelihoods\n",
    "\n",
    "So far, we've focused on Gaussian likelihoods, which are appropriate for continuous data with symmetric noise. However, many real-world problems require different likelihood functions:\n",
    "\n",
    "- **Classification**: Binary or multi-class outcomes\n",
    "- **Count data**: Poisson or negative binomial observations\n",
    "- **Robust regression**: Heavy-tailed noise (Student-t)\n",
    "- **Ordinal data**: Ordered categorical outcomes\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "For non-Gaussian likelihoods, we typically use the **latent variable approach**:\n",
    "\n",
    "1. **Latent function**: $f(x) \\sim \\mathcal{GP}(m(x), k(x,x'))$\n",
    "2. **Link function**: $g: f \\mapsto \\theta$ (transforms GP to likelihood parameters)\n",
    "3. **Likelihood**: $y \\mid \\theta \\sim p(y \\mid \\theta)$\n",
    "\n",
    "This requires using `pm.gp.Latent` instead of `pm.gp.Marginal`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Binary Classification with GP\n",
    "\n",
    "For binary classification, we use a Bernoulli likelihood with a logistic link function:\n",
    "\n",
    "$$p(y = 1 \\mid f) = \\text{logit}^{-1}(f) = \\frac{1}{1 + e^{-f}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_classification_demo():\n",
    "    # Generate synthetic classification data\n",
    "    n = 100\n",
    "    X = rng.uniform(-3, 3, n)[:, None]\n",
    "    \n",
    "    # True latent function\n",
    "    f_true = 2 * np.sin(X.flatten()) + 0.5 * X.flatten()**2 - 2\n",
    "    p_true = 1 / (1 + np.exp(-f_true))  # logistic\n",
    "    y = rng.binomial(1, p_true, n)\n",
    "    \n",
    "    # Test points for prediction\n",
    "    X_test = np.linspace(-3, 3, 100)[:, None]\n",
    "    \n",
    "    # GP Classification model\n",
    "    with pm.Model() as gp_classification:\n",
    "        # Kernel hyperparameters\n",
    "        ‚Ñì = pm.Gamma(\"‚Ñì\", alpha=2, beta=1)\n",
    "        Œ∑ = pm.HalfCauchy(\"Œ∑\", beta=5)\n",
    "        \n",
    "        # Define covariance function\n",
    "        cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "        \n",
    "        # GP prior on latent function\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Bernoulli likelihood\n",
    "        p = pm.math.invlogit(f)  # logistic transformation\n",
    "        y_obs = pm.Bernoulli(\"y_obs\", p=p, observed=y)\n",
    "        \n",
    "        # Fit model\n",
    "        idata = pm.sample(1000, tune=1000, chains=2, cores=1, \n",
    "                         random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        # Posterior predictions\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "    \n",
    "    # Extract results\n",
    "    f_pred_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_pred_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    p_pred_mean = 1 / (1 + np.exp(-f_pred_mean))  # Transform to probabilities\n",
    "    \n",
    "    # Create plot\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot training data\n",
    "    colors = ['red' if yi == 0 else 'blue' for yi in y]\n",
    "    symbols = ['circle' if yi == 0 else 'diamond' for yi in y]\n",
    "    \n",
    "    for yi in [0, 1]:\n",
    "        mask = y == yi\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X[mask].flatten(),\n",
    "                y=np.full(np.sum(mask), yi),\n",
    "                mode='markers',\n",
    "                name=f'Class {yi}',\n",
    "                marker=dict(\n",
    "                    color='red' if yi == 0 else 'blue',\n",
    "                    size=8,\n",
    "                    symbol='circle' if yi == 0 else 'diamond'\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    \n",
    "    # Plot predicted probabilities\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=p_pred_mean,\n",
    "            name='Predicted P(y=1)',\n",
    "            line=dict(color='green', width=3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Add uncertainty bands for latent function\n",
    "    f_upper = 1 / (1 + np.exp(-(f_pred_mean + 2*f_pred_std)))\n",
    "    f_lower = 1 / (1 + np.exp(-(f_pred_mean - 2*f_pred_std)))\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "            y=np.concatenate([f_upper, f_lower[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 255, 0, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Credible Interval',\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"GP Classification Example\",\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"P(y=1)\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig, idata\n",
    "\n",
    "classification_fig, classification_idata = gp_classification_demo()\n",
    "classification_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Student-t Process for Robust Regression\n",
    "\n",
    "The Student-t process provides robustness to outliers by using a Student-t likelihood instead of Gaussian:\n",
    "\n",
    "$$y \\mid f, \\nu, \\sigma \\sim \\text{Student-t}(\\nu, f, \\sigma)$$\n",
    "\n",
    "Where $\\nu$ controls the tail heaviness (lower values = heavier tails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def student_t_process_demo():\n",
    "    # Generate data with outliers\n",
    "    n = 50\n",
    "    X = np.linspace(-2, 2, n)[:, None]\n",
    "    \n",
    "    # True function\n",
    "    f_true = np.sin(2 * X.flatten())\n",
    "    \n",
    "    # Add normal noise + some outliers\n",
    "    noise = 0.1 * rng.standard_normal(n)\n",
    "    # Add some outliers\n",
    "    outlier_idx = rng.choice(n, size=5, replace=False)\n",
    "    noise[outlier_idx] += rng.choice([-2, 2], size=5) * rng.uniform(1, 3, 5)\n",
    "    \n",
    "    y = f_true + noise\n",
    "    \n",
    "    X_test = np.linspace(-2, 2, 100)[:, None]\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=2,\n",
    "        subplot_titles=[\"Standard GP (Gaussian)\", \"Robust GP (Student-t)\"]\n",
    "    )\n",
    "    \n",
    "    # Standard Gaussian GP\n",
    "    with pm.Model() as gaussian_gp:\n",
    "        ‚Ñì = pm.Gamma(\"‚Ñì\", alpha=2, beta=2)\n",
    "        Œ∑ = pm.HalfCauchy(\"Œ∑\", beta=5)\n",
    "        œÉ = pm.HalfCauchy(\"œÉ\", beta=2)\n",
    "        \n",
    "        cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "        gp = pm.gp.Marginal(cov_func=cov)\n",
    "        \n",
    "        y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=œÉ)\n",
    "        \n",
    "        idata_gaussian = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                                  random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        f_pred_gaussian = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_gaussian = pm.sample_posterior_predictive(idata_gaussian, progressbar=False)\n",
    "    \n",
    "    # Student-t GP\n",
    "    with pm.Model() as studentt_gp:\n",
    "        ‚Ñì = pm.Gamma(\"‚Ñì\", alpha=2, beta=2) \n",
    "        Œ∑ = pm.HalfCauchy(\"Œ∑\", beta=5)\n",
    "        œÉ = pm.HalfCauchy(\"œÉ\", beta=2)\n",
    "        ŒΩ = pm.Gamma(\"ŒΩ\", alpha=2, beta=0.1)  # Degrees of freedom\n",
    "        \n",
    "        cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Student-t likelihood\n",
    "        y_ = pm.StudentT(\"y\", nu=ŒΩ, mu=f, sigma=œÉ, observed=y)\n",
    "        \n",
    "        idata_studentt = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                                  random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        f_pred_studentt = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_studentt = pm.sample_posterior_predictive(idata_studentt, progressbar=False)\n",
    "    \n",
    "    # Plot results\n",
    "    models = {\n",
    "        \"Gaussian\": (pred_gaussian, (1, 1)),\n",
    "        \"Student-t\": (pred_studentt, (1, 2))\n",
    "    }\n",
    "    \n",
    "    for name, (pred, pos) in models.items():\n",
    "        f_mean = pred.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "        f_std = pred.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "        \n",
    "        # Add training data\n",
    "        colors = ['red' if i in outlier_idx else 'black' for i in range(len(y))]\n",
    "        sizes = [10 if i in outlier_idx else 6 for i in range(len(y))]\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(),\n",
    "                y=y,\n",
    "                mode='markers',\n",
    "                name='Data (outliers in red)' if pos[1] == 1 else 'Data',\n",
    "                marker=dict(color=colors, size=sizes),\n",
    "                showlegend=pos[1] == 1\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add GP predictions\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(),\n",
    "                y=f_mean,\n",
    "                name=f'{name} GP Mean',\n",
    "                line=dict(color='blue'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add uncertainty\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "                y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "                fill='toself',\n",
    "                fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "                line=dict(color='rgba(255,255,255,0)'),\n",
    "                showlegend=False\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "        \n",
    "        # Add true function\n",
    "        f_true_test = np.sin(2 * X_test.flatten())\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(),\n",
    "                y=f_true_test,\n",
    "                name='True Function' if pos[1] == 1 else 'True Function',\n",
    "                line=dict(color='green', dash='dash'),\n",
    "                showlegend=pos[1] == 1\n",
    "            ),\n",
    "            row=pos[0], col=pos[1]\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(height=500, title=\"GP Regression: Gaussian vs Student-t\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig, idata_gaussian, idata_studentt\n",
    "\n",
    "robust_fig, gauss_idata, studentt_idata = student_t_process_demo()\n",
    "robust_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3: Count Data with Poisson Likelihood\n",
    "\n",
    "For count data, we use a Poisson likelihood with a log link function:\n",
    "\n",
    "$$\\lambda = \\exp(f)$$\n",
    "$$y \\mid \\lambda \\sim \\text{Poisson}(\\lambda)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_gp_demo():\n",
    "    # Generate synthetic count data\n",
    "    n = 60\n",
    "    X = np.linspace(0, 4*np.pi, n)[:, None]\n",
    "    \n",
    "    # True log-intensity function\n",
    "    f_true = 1 + 0.5 * np.sin(X.flatten()) + 0.3 * np.cos(2 * X.flatten())\n",
    "    lambda_true = np.exp(f_true)\n",
    "    y = rng.poisson(lambda_true)\n",
    "    \n",
    "    X_test = np.linspace(0, 4*np.pi, 100)[:, None]\n",
    "    \n",
    "    # Poisson GP model\n",
    "    with pm.Model() as poisson_gp:\n",
    "        # Kernel parameters\n",
    "        ‚Ñì = pm.Gamma(\"‚Ñì\", alpha=2, beta=1)\n",
    "        Œ∑ = pm.HalfCauchy(\"Œ∑\", beta=2)\n",
    "        \n",
    "        # GP prior on log-intensity\n",
    "        cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "        gp = pm.gp.Latent(cov_func=cov)\n",
    "        f = gp.prior(\"f\", X=X)\n",
    "        \n",
    "        # Poisson likelihood\n",
    "        Œª = pm.math.exp(f)  # log link\n",
    "        y_obs = pm.Poisson(\"y_obs\", mu=Œª, observed=y)\n",
    "        \n",
    "        # Fit model\n",
    "        idata = pm.sample(1000, tune=1000, chains=2, cores=1,\n",
    "                         random_seed=RANDOM_SEED, progressbar=False)\n",
    "        \n",
    "        # Predictions\n",
    "        f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "        pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "    \n",
    "    # Extract results\n",
    "    f_pred_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "    f_pred_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "    lambda_pred_mean = np.exp(f_pred_mean)\n",
    "    lambda_pred_upper = np.exp(f_pred_mean + 2*f_pred_std)\n",
    "    lambda_pred_lower = np.exp(f_pred_mean - 2*f_pred_std)\n",
    "    \n",
    "    # Plot results\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X.flatten(),\n",
    "            y=y,\n",
    "            mode='markers',\n",
    "            name='Count Data',\n",
    "            marker=dict(color='black', size=6)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # True intensity\n",
    "    lambda_true_test = np.exp(1 + 0.5 * np.sin(X_test.flatten()) + \n",
    "                             0.3 * np.cos(2 * X_test.flatten()))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=lambda_true_test,\n",
    "            name='True Intensity',\n",
    "            line=dict(color='green', dash='dash')\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Predicted intensity\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=X_test.flatten(),\n",
    "            y=lambda_pred_mean,\n",
    "            name='Predicted Intensity',\n",
    "            line=dict(color='blue', width=3)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # Uncertainty bands\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "            y=np.concatenate([lambda_pred_upper, lambda_pred_lower[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            name='95% Credible Interval',\n",
    "            showlegend=False\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title=\"GP with Poisson Likelihood (Count Data)\",\n",
    "        xaxis_title=\"x\",\n",
    "        yaxis_title=\"Count / Intensity\",\n",
    "        height=500\n",
    "    )\n",
    "    \n",
    "    return fig, idata\n",
    "\n",
    "poisson_fig, poisson_idata = poisson_gp_demo()\n",
    "poisson_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computational Considerations for Non-Gaussian Likelihoods\n",
    "\n",
    "Using non-Gaussian likelihoods comes with computational trade-offs:\n",
    "\n",
    "### 1. **Inference Complexity**\n",
    "- **Gaussian**: Analytical posterior (fast)\n",
    "- **Non-Gaussian**: MCMC required (slower)\n",
    "\n",
    "### 2. **Model Specification**\n",
    "- **Marginal**: `pm.gp.Marginal` for Gaussian likelihoods only\n",
    "- **Latent**: `pm.gp.Latent` required for non-Gaussian likelihoods\n",
    "\n",
    "### 3. **Hyperparameter Sensitivity**\n",
    "- Non-Gaussian models often more sensitive to priors\n",
    "- More careful initialization may be needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary of likelihood choices\n",
    "likelihood_guide = {\n",
    "    \"Data Type\": [\"Continuous (symmetric noise)\", \"Continuous (outliers)\", \n",
    "                 \"Binary\", \"Count\", \"Positive continuous\"],\n",
    "    \"Likelihood\": [\"Gaussian\", \"Student-t\", \"Bernoulli\", \"Poisson\", \"Gamma/Lognormal\"],\n",
    "    \"Link Function\": [\"Identity\", \"Identity\", \"Logistic\", \"Log\", \"Log\"],\n",
    "    \"PyMC Implementation\": [\"gp.Marginal\", \"gp.Latent\", \"gp.Latent\", \"gp.Latent\", \"gp.Latent\"]\n",
    "}\n",
    "\n",
    "likelihood_df = pd.DataFrame(likelihood_guide)\n",
    "print(\"GP Likelihood Selection Guide:\")\n",
    "print(\"=\" * 60)\n",
    "print(likelihood_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ü§ñ Hands-On Exercise 2: LLM-Assisted Non-Gaussian Likelihood Implementation\n",
    "\n",
    "Now let's practice using LLMs to help us implement GP models with non-Gaussian likelihoods. This is where having an AI assistant becomes especially valuable, as these models have more complexity and common pitfalls.\n",
    "\n",
    "### Exercise Objectives\n",
    "\n",
    "1. Use your LLM to help choose appropriate likelihoods for different data types\n",
    "2. Get AI assistance implementing Latent GP models with various likelihoods\n",
    "3. Practice debugging convergence issues in non-Gaussian GP models\n",
    "4. Learn to interpret and validate non-Gaussian GP model results\n",
    "\n",
    "### Specialized Prompting for Non-Gaussian GPs\n",
    "\n",
    "Non-Gaussian likelihoods require more specialized knowledge. Here are effective prompting strategies:\n",
    "\n",
    "**For Likelihood Selection:**\n",
    "- \"I have [describe data type/properties]. What likelihood and link function should I use with PyMC GPs?\"\n",
    "- \"Help me choose between Student-t, Poisson, and Bernoulli likelihood for [specific problem]\"\n",
    "\n",
    "**For Implementation:**\n",
    "- \"Show me how to implement a GP classification model with Bernoulli likelihood in PyMC\"\n",
    "- \"Help me set up proper priors for a count data GP model with Poisson likelihood\"\n",
    "\n",
    "**For Troubleshooting:**\n",
    "- \"My non-Gaussian GP model isn't converging. Here's my code: [paste]. What could be wrong?\"\n",
    "- \"The predictions from my Student-t GP look strange. How should I validate this model?\"\n",
    "\n",
    "### Comprehensive Prompts for This Exercise\n",
    "\n",
    "```\n",
    "PROMPT 1: \"I have binary classification data (0s and 1s) and want to use a \n",
    "Gaussian Process. Help me implement a GP classification model using PyMC with:\n",
    "- pm.gp.Latent for the underlying function\n",
    "- Appropriate kernel choice\n",
    "- Bernoulli likelihood with logistic link\n",
    "- Proper hyperparameter priors\n",
    "Show me the complete implementation and explain each step.\"\n",
    "\n",
    "PROMPT 2: \"Help me build a robust regression model for data with outliers\n",
    "using a Student-t process in PyMC. Explain why Student-t is better than\n",
    "Gaussian for outliers and show me how to implement it with pm.gp.Latent.\n",
    "Include parameter interpretation and model validation.\"\n",
    "\n",
    "PROMPT 3: \"I have count data (non-negative integers) that I want to model\n",
    "with a GP. Help me implement a Poisson GP regression model in PyMC.\n",
    "Show me how to use the log link function and interpret the results.\n",
    "Include visualization of the intensity function.\"\n",
    "\n",
    "PROMPT 4: \"My non-Gaussian GP model is giving me R-hat > 1.1 and low ESS.\n",
    "Help me diagnose and fix these convergence issues. What are common\n",
    "problems with non-Gaussian GP models and their solutions?\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ü§ñ EXERCISE: Use your LLM to implement non-Gaussian GP models\n",
    "\n",
    "# Create different types of synthetic data for testing\n",
    "np.random.seed(42)\n",
    "n_points = 60\n",
    "X_demo = np.linspace(-2, 2, n_points)[:, None]\n",
    "\n",
    "# Binary classification data\n",
    "f_latent = 1.5 * np.sin(2 * X_demo.flatten()) - 0.5\n",
    "p_binary = 1 / (1 + np.exp(-f_latent))\n",
    "y_binary = rng.binomial(1, p_binary, n_points)\n",
    "\n",
    "# Count data\n",
    "f_count = 1 + 0.7 * np.sin(X_demo.flatten())\n",
    "lambda_count = np.exp(f_count)\n",
    "y_count = rng.poisson(lambda_count)\n",
    "\n",
    "# Data with outliers (for Student-t)\n",
    "f_robust = np.sin(1.5 * X_demo.flatten())\n",
    "y_robust = f_robust + 0.1 * rng.standard_normal(n_points)\n",
    "# Add some outliers\n",
    "outlier_idx = rng.choice(n_points, size=8, replace=False)\n",
    "y_robust[outlier_idx] += rng.choice([-1, 1], size=8) * rng.uniform(2, 4, 8)\n",
    "\n",
    "print(\"Three different datasets created:\")\n",
    "print(f\"- Binary classification: {y_binary.sum()} positive out of {n_points}\")\n",
    "print(f\"- Count data: range {y_count.min()}-{y_count.max()}\")\n",
    "print(f\"- Robust regression: {len(outlier_idx)} outliers added\")\n",
    "\n",
    "# TASK 1: Binary classification with LLM help\n",
    "def implement_gp_classification_with_llm():\n",
    "    \"\"\"\n",
    "    Use your LLM to help implement GP binary classification.\n",
    "    \n",
    "    Suggested prompt: \"I have binary data (0s and 1s) that I want to classify\n",
    "    using a Gaussian Process in PyMC. The input X is 1D and the output y is\n",
    "    binary. Help me implement a complete GP classification model using:\n",
    "    - pm.gp.Latent (required for non-Gaussian)\n",
    "    - ExpQuad kernel with appropriate priors\n",
    "    - Bernoulli likelihood with logistic link function\n",
    "    Show me how to fit the model, make predictions, and visualize the \n",
    "    classification boundary with uncertainty.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED IMPLEMENTATION HERE\n",
    "    global X_demo, y_binary  # Use the global data\n",
    "    pass\n",
    "\n",
    "# TASK 2: Student-t robust regression\n",
    "def implement_robust_gp_with_llm():\n",
    "    \"\"\"\n",
    "    Get LLM help to implement a robust GP regression model.\n",
    "    \n",
    "    Suggested prompt: \"I have regression data with outliers. Help me implement\n",
    "    a robust GP regression model in PyMC using Student-t likelihood instead\n",
    "    of Gaussian. Explain why this is more robust to outliers and show me:\n",
    "    - How to use pm.gp.Latent with StudentT likelihood\n",
    "    - Appropriate priors for the degrees of freedom parameter\n",
    "    - How to compare with standard Gaussian GP\n",
    "    Include visualization showing robustness to outliers.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED IMPLEMENTATION HERE\n",
    "    global X_demo, y_robust  # Use the global data\n",
    "    pass\n",
    "\n",
    "# TASK 3: Poisson count data modeling\n",
    "def implement_poisson_gp_with_llm():\n",
    "    \"\"\"\n",
    "    Use your LLM to help implement GP regression for count data.\n",
    "    \n",
    "    Suggested prompt: \"I have count data (non-negative integers) that I want\n",
    "    to model with a Gaussian Process. Help me implement a Poisson regression\n",
    "    GP model in PyMC that includes:\n",
    "    - pm.gp.Latent for the log-intensity function\n",
    "    - Poisson likelihood with log link function\n",
    "    - Appropriate kernel and hyperparameter priors\n",
    "    Show me how to interpret the results and visualize both the latent\n",
    "    function and the intensity (rate) parameter.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED IMPLEMENTATION HERE\n",
    "    global X_demo, y_count  # Use the global data\n",
    "    pass\n",
    "\n",
    "# TASK 4: Debugging and model comparison\n",
    "def debug_and_compare_with_llm():\n",
    "    \"\"\"\n",
    "    Get LLM help to compare the different models and debug any issues.\n",
    "    \n",
    "    Suggested prompt: \"Help me create a systematic comparison of the three\n",
    "    non-Gaussian GP models I've implemented (classification, robust regression,\n",
    "    count data). Show me how to:\n",
    "    - Check convergence diagnostics for each model\n",
    "    - Compare prediction quality using appropriate metrics\n",
    "    - Create a unified visualization showing all three model types\n",
    "    - Identify and fix any convergence issues\n",
    "    What are the common pitfalls with non-Gaussian GP models?\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "print(\"üéØ Complete all 4 tasks using your LLM as an expert guide!\")\n",
    "print(\"üõ†Ô∏è Focus on understanding WHY each likelihood is appropriate for each data type.\")\n",
    "print(\"üîç Ask your LLM to explain any concepts or code that seem unclear.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Learning Outcomes\n",
    "\n",
    "After completing this LLM-assisted exercise, you should understand:\n",
    "\n",
    "- **When and why** to use non-Gaussian likelihoods with GPs\n",
    "- **How to implement** pm.gp.Latent models with various likelihoods\n",
    "- **Link functions** and their role in connecting GPs to different data types\n",
    "- **Common convergence issues** with non-Gaussian models and their solutions\n",
    "- **Model validation** techniques specific to non-Gaussian GP models\n",
    "\n",
    "### Key Takeaway\n",
    "\n",
    "Non-Gaussian GP models are powerful but more complex. Your LLM can help you navigate the additional complexity by:\n",
    "- Suggesting appropriate likelihood/link combinations\n",
    "- Helping debug sampling issues\n",
    "- Explaining the mathematical connections\n",
    "- Providing model validation strategies\n",
    "\n",
    "Don't hesitate to ask follow-up questions about anything that seems unclear!\n",
    "\n",
    "---\n",
    "\n",
    "# Part D: Model Building Best Practices\n",
    "\n",
    "Building effective GP models requires careful consideration of several factors. Let's explore key best practices for successful GP modeling.\n",
    "\n",
    "## 1. Choosing Between Marginal vs Latent\n",
    "\n",
    "The choice between `pm.gp.Marginal` and `pm.gp.Latent` has important implications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of Marginal vs Latent implementations\n",
    "comparison_data = {\n",
    "    \"Aspect\": [\"Computational Speed\", \"Likelihood Types\", \"Memory Usage\", \n",
    "              \"Posterior Samples\", \"Prediction\", \"Scalability\"],\n",
    "    \"pm.gp.Marginal\": [\"Fast (analytical)\", \"Gaussian only\", \"Low\", \n",
    "                       \"Parameters only\", \"Direct\", \"Better\"],\n",
    "    \"pm.gp.Latent\": [\"Slower (MCMC)\", \"Any likelihood\", \"Higher\", \n",
    "                     \"Full GP samples\", \"Via conditional\", \"Limited\"]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "print(\"Marginal vs Latent Implementation Comparison:\")\n",
    "print(\"=\" * 55)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameter Priors and Initialization\n",
    "\n",
    "Proper prior specification is crucial for GP models. Here are some guidelines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_prior_sensitivity():\n",
    "    \"\"\"Show impact of different prior choices on GP inference\"\"\"\n",
    "    \n",
    "    # Generate simple test data\n",
    "    X = np.linspace(0, 1, 20)[:, None]\n",
    "    y = np.sin(4 * np.pi * X.flatten()) + 0.1 * rng.standard_normal(20)\n",
    "    \n",
    "    X_test = np.linspace(0, 1, 100)[:, None]\n",
    "    \n",
    "    # Different prior specifications\n",
    "    prior_configs = {\n",
    "        \"Informative\": {\n",
    "            \"‚Ñì_prior\": pm.Gamma(\"‚Ñì\", alpha=4, beta=20),  # Small lengthscale\n",
    "            \"Œ∑_prior\": pm.HalfNormal(\"Œ∑\", sigma=1),        # Moderate variance\n",
    "        },\n",
    "        \"Weakly Informative\": {\n",
    "            \"‚Ñì_prior\": pm.Gamma(\"‚Ñì\", alpha=2, beta=2),   # Moderate lengthscale\n",
    "            \"Œ∑_prior\": pm.HalfCauchy(\"Œ∑\", beta=2),        # Moderate variance\n",
    "        },\n",
    "        \"Vague\": {\n",
    "            \"‚Ñì_prior\": pm.Gamma(\"‚Ñì\", alpha=1, beta=0.1), # Very flexible\n",
    "            \"Œ∑_prior\": pm.HalfCauchy(\"Œ∑\", beta=10),       # Very flexible\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=1, cols=3,\n",
    "        subplot_titles=list(prior_configs.keys())\n",
    "    )\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for i, (name, priors) in enumerate(prior_configs.items()):\n",
    "        with pm.Model() as model:\n",
    "            # Apply different priors\n",
    "            ‚Ñì = priors[\"‚Ñì_prior\"]\n",
    "            Œ∑ = priors[\"Œ∑_prior\"]\n",
    "            œÉ = pm.HalfNormal(\"œÉ\", sigma=0.5)\n",
    "            \n",
    "            cov = Œ∑**2 * pm.gp.cov.ExpQuad(1, ls=‚Ñì)\n",
    "            gp = pm.gp.Marginal(cov_func=cov)\n",
    "            y_ = gp.marginal_likelihood(\"y\", X=X, y=y, sigma=œÉ)\n",
    "            \n",
    "            try:\n",
    "                idata = pm.sample(500, tune=500, chains=2, cores=1,\n",
    "                                random_seed=RANDOM_SEED, progressbar=False)\n",
    "                \n",
    "                f_pred = gp.conditional(\"f_pred\", X_test)\n",
    "                pred_samples = pm.sample_posterior_predictive(idata, progressbar=False)\n",
    "                \n",
    "                f_mean = pred_samples.posterior_predictive['f_pred'].mean(dim=['chain', 'draw'])\n",
    "                f_std = pred_samples.posterior_predictive['f_pred'].std(dim=['chain', 'draw'])\n",
    "                \n",
    "                results[name] = (f_mean, f_std, idata)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Sampling failed for {name}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Plot data\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X.flatten(), y=y,\n",
    "                mode='markers', name='Data' if i == 0 else None,\n",
    "                marker=dict(color='black', size=4),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        \n",
    "        # Plot true function\n",
    "        y_true = np.sin(4 * np.pi * X_test.flatten())\n",
    "        fig.add_trace(\n",
    "            go.Scatter(\n",
    "                x=X_test.flatten(), y=y_true,\n",
    "                name='True' if i == 0 else None, \n",
    "                line=dict(color='green', dash='dash'),\n",
    "                showlegend=(i == 0)\n",
    "            ),\n",
    "            row=1, col=i+1\n",
    "        )\n",
    "        \n",
    "        if name in results:\n",
    "            f_mean, f_std, _ = results[name]\n",
    "            \n",
    "            # Plot GP prediction\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=X_test.flatten(), y=f_mean,\n",
    "                    name='GP' if i == 0 else None,\n",
    "                    line=dict(color='blue'),\n",
    "                    showlegend=(i == 0)\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "            \n",
    "            # Uncertainty\n",
    "            fig.add_trace(\n",
    "                go.Scatter(\n",
    "                    x=np.concatenate([X_test.flatten(), X_test.flatten()[::-1]]),\n",
    "                    y=np.concatenate([f_mean + 2*f_std, (f_mean - 2*f_std)[::-1]]),\n",
    "                    fill='toself',\n",
    "                    fillcolor='rgba(0, 0, 255, 0.2)',\n",
    "                    line=dict(color='rgba(255,255,255,0)'),\n",
    "                    showlegend=False\n",
    "                ),\n",
    "                row=1, col=i+1\n",
    "            )\n",
    "    \n",
    "    fig.update_layout(height=400, title=\"Impact of Prior Specification\")\n",
    "    fig.update_xaxes(title=\"x\")\n",
    "    fig.update_yaxes(title=\"y\")\n",
    "    \n",
    "    return fig, results\n",
    "\n",
    "prior_fig, prior_results = demonstrate_prior_sensitivity()\n",
    "prior_fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommended Prior Guidelines\n",
    "\n",
    "Based on the above example and general experience, here are some guidelines for choosing priors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prior recommendations\n",
    "prior_recommendations = {\n",
    "    \"Parameter\": [\"Lengthscale (‚Ñì)\", \"Amplitude (Œ∑)\", \"Noise (œÉ)\", \"Period (p)\"],\n",
    "    \"Recommended Prior\": [\n",
    "        \"Gamma(2, Œ≤) where Œ≤ ‚âà 2/expected_scale\",\n",
    "        \"HalfCauchy(Œ≤) where Œ≤ ‚âà std(y)\", \n",
    "        \"HalfNormal(œÉ) where œÉ ‚âà 0.1 * std(y)\",\n",
    "        \"Normal(Œº, œÉ) where Œº = expected period\"\n",
    "    ],\n",
    "    \"Rationale\": [\n",
    "        \"Weakly informative, avoids extreme values\",\n",
    "        \"Heavy tails allow flexibility\",\n",
    "        \"Conservative, prevents overfitting\",\n",
    "        \"Domain knowledge essential\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "prior_df = pd.DataFrame(prior_recommendations)\n",
    "print(\"GP Hyperparameter Prior Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "print(prior_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Validation and Diagnostics\n",
    "\n",
    "Proper model validation is essential for reliable GP models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gp_diagnostics(idata, model_name=\"GP Model\"):\n",
    "    \"\"\"Comprehensive diagnostics for GP models\"\"\"\n",
    "    \n",
    "    print(f\"Diagnostics for {model_name}:\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # 1. Convergence diagnostics\n",
    "    summary = az.summary(idata, hdi_prob=0.95)\n",
    "    \n",
    "    # Check R-hat values\n",
    "    rhat_issues = summary[summary['r_hat'] > 1.1]\n",
    "    if len(rhat_issues) > 0:\n",
    "        print(\"‚ö†Ô∏è  WARNING: Some parameters have RÃÇ > 1.1:\")\n",
    "        print(rhat_issues[['mean', 'r_hat']].to_string())\n",
    "    else:\n",
    "        print(\"‚úÖ All RÃÇ values < 1.1 (good convergence)\")\n",
    "    \n",
    "    # Check effective sample size\n",
    "    ess_issues = summary[summary['ess_bulk'] < 400]\n",
    "    if len(ess_issues) > 0:\n",
    "        print(\"\\n‚ö†Ô∏è  WARNING: Low effective sample size:\")\n",
    "        print(ess_issues[['ess_bulk', 'ess_tail']].to_string())\n",
    "    else:\n",
    "        print(\"‚úÖ Adequate effective sample sizes\")\n",
    "    \n",
    "    # 2. Print key parameter estimates\n",
    "    print(\"\\nParameter Estimates:\")\n",
    "    print(summary[['mean', 'hdi_2.5%', 'hdi_97.5%']].round(4).to_string())\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Example with our classification model\n",
    "if 'classification_idata' in locals():\n",
    "    classification_diagnostics = gp_diagnostics(classification_idata, \"GP Classification\")\n",
    "else:\n",
    "    print(\"Classification model not available for diagnostics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computational Efficiency Tips\n",
    "\n",
    "For larger datasets or complex models, consider these optimization strategies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computational efficiency tips\n",
    "efficiency_tips = {\n",
    "    \"Strategy\": [\n",
    "        \"Use pm.gp.Marginal when possible\",\n",
    "        \"Add jitter to diagonal\", \n",
    "        \"Use sparse/inducing points\",\n",
    "        \"Hierarchical GPs for grouped data\",\n",
    "        \"Consider HSGP approximation\",\n",
    "        \"Proper kernel scaling\"\n",
    "    ],\n",
    "    \"When to Use\": [\n",
    "        \"Gaussian likelihood\",\n",
    "        \"Numerical instability\",\n",
    "        \"Large datasets (n > 1000)\", \n",
    "        \"Multiple similar time series\",\n",
    "        \"Stationary kernels, large n\",\n",
    "        \"Always\"\n",
    "    ],\n",
    "    \"Implementation\": [\n",
    "        \"pm.gp.Marginal(cov_func=cov)\",\n",
    "        \"cov_func + 1e-6 * pm.gp.cov.WhiteNoise()\",\n",
    "        \"Use inducing points\",\n",
    "        \"Shared hyperparameters\", \n",
    "        \"pm.gp.HSGP\",\n",
    "        \"Scale inputs to [0,1] or [-1,1]\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "efficiency_df = pd.DataFrame(efficiency_tips)\n",
    "print(\"GP Computational Efficiency Strategies:\")\n",
    "print(\"=\" * 45)\n",
    "print(efficiency_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary and Key Takeaways\n",
    "\n",
    "In this session, we've covered the essential building blocks for advanced GP modeling:\n",
    "\n",
    "## üéØ **Key Concepts Mastered**\n",
    "\n",
    "1. **Kernel Selection**: Understanding when to use RBF, Mat√©rn, Periodic, and other kernels\n",
    "2. **Kernel Composition**: Building complex patterns through addition and multiplication\n",
    "3. **Non-Gaussian Likelihoods**: Extending GPs beyond continuous data\n",
    "4. **Model Building**: Best practices for robust, efficient GP models\n",
    "\n",
    "## üìã **Practical Guidelines**\n",
    "\n",
    "### Kernel Selection Flowchart\n",
    "```\n",
    "Data Characteristics ‚Üí Kernel Choice\n",
    "‚îú‚îÄ‚îÄ Smooth, no periodicity ‚Üí RBF\n",
    "‚îú‚îÄ‚îÄ Some roughness ‚Üí Mat√©rn (3/2 or 5/2) \n",
    "‚îú‚îÄ‚îÄ Periodic patterns ‚Üí Periodic (+ RBF for trend)\n",
    "‚îú‚îÄ‚îÄ Multiple scales ‚Üí Additive kernels\n",
    "‚îî‚îÄ‚îÄ Multiplicative patterns ‚Üí Product kernels\n",
    "```\n",
    "\n",
    "### Implementation Strategy\n",
    "```\n",
    "Problem Type ‚Üí Implementation\n",
    "‚îú‚îÄ‚îÄ Gaussian noise ‚Üí pm.gp.Marginal\n",
    "‚îú‚îÄ‚îÄ Classification ‚Üí pm.gp.Latent + Bernoulli\n",
    "‚îú‚îÄ‚îÄ Count data ‚Üí pm.gp.Latent + Poisson\n",
    "‚îú‚îÄ‚îÄ Robust regression ‚Üí pm.gp.Latent + StudentT\n",
    "‚îî‚îÄ‚îÄ Large datasets ‚Üí Consider HSGP or sparse approximations\n",
    "```\n",
    "\n",
    "## üöÄ **Next Steps**\n",
    "\n",
    "Building on today's foundation, the remaining sessions will cover:\n",
    "- **Session 3**: Advanced topics and computational methods\n",
    "- **Session 4**: Real-world applications and case studies\n",
    "\n",
    "## üí° **Practice Exercises with LLM Collaboration**\n",
    "\n",
    "Apply these concepts with your AI coding assistant's help:\n",
    "\n",
    "1. **LLM-Guided Kernel Selection**: Ask your LLM to analyze your data and suggest appropriate kernels\n",
    "2. **AI-Assisted Composition**: Have your LLM help you build composite kernels for complex patterns\n",
    "3. **Likelihood Matching**: Get LLM guidance on choosing the right likelihood for your data type\n",
    "4. **Diagnostic Interpretation**: Use your LLM to help interpret convergence diagnostics and model fit\n",
    "5. **Automated Comparison**: Ask your LLM to implement and compare multiple model choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources\n",
    "\n",
    "For deeper understanding, explore:\n",
    "\n",
    "- **PyMC Documentation**: [GP module documentation](https://www.pymc.io/projects/docs/en/stable/api/gp.html)\n",
    "- **Rasmussen & Williams**: *Gaussian Processes for Machine Learning* (the definitive textbook)\n",
    "- **PyMC Examples**: [GP gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/index.html)\n",
    "- **Kernel Cookbook**: David Duvenaud's kernel cookbook for kernel selection guidance\n",
    "\n",
    "---\n",
    "\n",
    "**End of Session 2** üéâ\n",
    "\n",
    "*You now have the tools to build sophisticated GP models for a wide range of real-world problems!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}