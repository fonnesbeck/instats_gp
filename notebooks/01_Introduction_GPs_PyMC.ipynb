{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Introduction to Gaussian Processes and PyMC\n",
    "\n",
    "**Duration:** 3 hours  \n",
    "**Workshop:** Gaussian Processes with PyMC and LLMs\n",
    "\n",
    "---\n",
    "\n",
    "Gaussian Processes (GPs) represent one of the most elegant and powerful tools in modern machine learning and statistics. Unlike parametric models that assume specific functional forms, GPs provide a **non-parametric approach** that can capture complex patterns while naturally quantifying uncertainty. This makes them particularly valuable for applications where understanding uncertainty is as important as making predictionsâ€”from scientific modeling to decision-making under uncertainty.\n",
    "\n",
    "This session introduces the foundational concepts of Gaussian Processes within the PyMC probabilistic programming framework. We will build intuition about what it means for a process to be \"Gaussian,\" explore the mathematical machinery that makes GPs work, and learn to implement them using PyMC's powerful and expressive interface.\n",
    "\n",
    "## Why Gaussian Processes?\n",
    "\n",
    "Traditional machine learning often focuses on finding the \"best\" parameters for a pre-specified model. Gaussian Processes take a fundamentally different approach: instead of assuming a specific functional form, they place a probability distribution directly over the **space of functions**. This perspective offers several compelling advantages:\n",
    "\n",
    "- **Principled uncertainty quantification**: GPs provide natural confidence intervals and probability distributions over predictions\n",
    "- **Automatic model selection**: Through marginal likelihood optimization, GPs can automatically tune their complexity to the data\n",
    "- **Incorporation of prior knowledge**: Domain expertise can be encoded through choice of mean functions and covariance kernels\n",
    "- **Small data efficiency**: GPs can make meaningful predictions and quantify uncertainty even with limited training data\n",
    "- **Interpretable hyperparameters**: Kernel parameters often have clear physical or domain-specific meanings\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Understand the mathematical foundations of GPs**: Grasp how Gaussian Processes extend multivariate Gaussian distributions to infinite-dimensional function spaces\n",
    "2. **Build intuition through visualization**: Create and interpret samples from GP priors to understand how hyperparameters affect function behavior\n",
    "3. **Master PyMC's probabilistic programming paradigm**: Use PyMC's model contexts, distributions, and inference machinery for GP modeling\n",
    "4. **Construct and analyze covariance functions**: Build kernels from first principles and understand their role in encoding assumptions about function smoothness and structure\n",
    "5. **Navigate PyMC's GP implementations**: Understand the trade-offs between `gp.Marginal` and `gp.Latent` approaches and when to use each\n",
    "6. **Apply GPs to real problems**: Build complete GP regression models, from prior specification through posterior inference to prediction\n",
    "\n",
    "## Session Structure\n",
    "\n",
    "This session is organized into six major sections, each building on the previous one:\n",
    "\n",
    "1. **Mathematical Foundations** (45 minutes): Core concepts, definitions, and the connection between finite and infinite-dimensional Gaussians\n",
    "2. **PyMC Fundamentals** (45 minutes): Model contexts, distributions, random variables, and the probabilistic programming paradigm\n",
    "3. **Kernel Theory and Construction** (45 minutes): Understanding covariance functions as the heart of GP modeling\n",
    "4. **PyMC GP Implementations** (45 minutes): Comparing marginal vs. latent formulations with practical examples\n",
    "5. **Hands-on Practice** (30 minutes): Guided exercises to reinforce key concepts\n",
    "6. **Integration and Next Steps** (15 minutes): Synthesis and preview of advanced topics\n",
    "\n",
    "Let's begin our journey into the world of Gaussian Processes and probabilistic programming.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We begin by setting up our computational environment with the necessary libraries for Gaussian Process modeling, Bayesian inference, and visualization. This section establishes the foundation for all subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import polars as pl\n",
    "\n",
    "# PyMC ecosystem for probabilistic programming\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Configure visualization defaults\n",
    "az.style.use('arviz-doc')\n",
    "pio.templates.default = 'plotly_white'\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 800\n",
    "px.defaults.height = 500\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "print(f\"Environment configured successfully!\")\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Mathematical Foundations of Gaussian Processes\n",
    "\n",
    "To understand Gaussian Processes deeply, we must first establish their mathematical foundations. This section will build intuition by connecting familiar concepts (univariate and multivariate Gaussians) to the more abstract notion of distributions over functions.\n",
    "\n",
    "### From Scalars to Functions: The Gaussian Hierarchy\n",
    "\n",
    "The conceptual progression from simple to complex Gaussian structures provides the key to understanding GPs:\n",
    "\n",
    "1. **Univariate Gaussian**: $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ describes uncertainty about a single scalar value\n",
    "2. **Multivariate Gaussian**: $\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$ describes uncertainty about a finite-dimensional vector\n",
    "3. **Gaussian Process**: $f(\\cdot) \\sim \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))$ describes uncertainty about an infinite-dimensional function\n",
    "\n",
    "The remarkable insight of Gaussian Processes is that we can work with infinite-dimensional function spaces by considering only finite-dimensional marginals at any collection of input points.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "**Definition**: A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.\n",
    "\n",
    "More precisely, a stochastic process $\\{f(x) : x \\in \\mathcal{X}\\}$ is a Gaussian Process if for any finite set of indices $\\{x_1, x_2, \\ldots, x_n\\} \\subset \\mathcal{X}$, the joint distribution of the random vector $(f(x_1), f(x_2), \\ldots, f(x_n))^T$ is multivariate Gaussian.\n",
    "\n",
    "A Gaussian Process is completely specified by two functions:\n",
    "\n",
    "1. **Mean function**: $m(x) = \\mathbb{E}[f(x)]$\n",
    "2. **Covariance function**: $k(x, x') = \\mathbb{Cov}[f(x), f(x')] = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]$\n",
    "\n",
    "We denote this as:\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
    "\n",
    "### The Finite-Dimensional View\n",
    "\n",
    "For any finite collection of input points $\\mathbf{X} = \\{x_1, x_2, \\ldots, x_n\\}$, the corresponding function values $\\mathbf{f} = [f(x_1), f(x_2), \\ldots, f(x_n)]^T$ follow a multivariate Gaussian distribution:\n",
    "\n",
    "$$\\mathbf{f} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{K})$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\mu} = [m(x_1), m(x_2), \\ldots, m(x_n)]^T$ is the mean vector\n",
    "- $\\mathbf{K}$ is the covariance matrix with entries $K_{ij} = k(x_i, x_j)$\n",
    "\n",
    "This finite-dimensional perspective is crucial because it allows us to:\n",
    "- Sample functions from the GP (by sampling from the multivariate Gaussian)\n",
    "- Compute likelihoods (using the multivariate Gaussian density)\n",
    "- Perform inference (using standard multivariate Gaussian conditioning)\n",
    "\n",
    "### Properties of Covariance Functions\n",
    "\n",
    "The covariance function $k(x, x')$ is the heart of a Gaussian Process. It encodes our assumptions about function smoothness, periodicity, and other structural properties. For $k$ to be a valid covariance function, it must be:\n",
    "\n",
    "1. **Symmetric**: $k(x, x') = k(x', x)$ for all $x, x'$\n",
    "2. **Positive semi-definite**: For any finite set $\\{x_1, \\ldots, x_n\\}$, the matrix $\\mathbf{K}$ with $K_{ij} = k(x_i, x_j)$ must be positive semi-definite\n",
    "\n",
    "These conditions ensure that the resulting covariance matrices are valid, guaranteeing that we can sample from and compute probabilities under the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuition: From Multivariate Gaussian to GP\n",
    "\n",
    "Let's build intuition by starting with a simple multivariate Gaussian and then extending to the GP setting. We'll see how increasing the number of dimensions naturally leads us to the function space perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rbf_covariance(X, length_scale=1.0, variance=1.0):\n",
    "    \"\"\"\n",
    "    Create RBF (Radial Basis Function) covariance matrix.\n",
    "    \n",
    "    The RBF kernel is defined as:\n",
    "    k(x, x') = ÏƒÂ² * exp(-||x - x'||Â² / (2â„“Â²))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n,)\n",
    "        Input locations\n",
    "    length_scale : float\n",
    "        Length scale parameter â„“\n",
    "    variance : float  \n",
    "        Variance parameter ÏƒÂ²\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    K : ndarray, shape (n, n)\n",
    "        Covariance matrix\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).reshape(-1, 1) if np.asarray(X).ndim == 1 else np.asarray(X)\n",
    "    \n",
    "    # Compute squared Euclidean distances\n",
    "    sqdist = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * np.dot(X, X.T)\n",
    "    \n",
    "    # RBF covariance\n",
    "    K = variance * np.exp(-0.5 * sqdist / length_scale**2)\n",
    "    \n",
    "    return K\n",
    "\n",
    "def zero_mean_function(X):\n",
    "    \"\"\"Zero mean function.\"\"\"\n",
    "    return np.zeros(len(X))\n",
    "\n",
    "# Demonstrate the progression from few to many points\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=[\"5 points\", \"10 points\", \"25 points\", \"50 points\"],\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "n_points_list = [5, 10, 25, 50]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for idx, (n_points, color) in enumerate(zip(n_points_list, colors)):\n",
    "    # Create input points\n",
    "    X = np.linspace(-3, 3, n_points)\n",
    "    \n",
    "    # Create covariance matrix\n",
    "    K = create_rbf_covariance(X, length_scale=1.0, variance=1.0)\n",
    "    \n",
    "    # Add small jitter for numerical stability\n",
    "    K += 1e-6 * np.eye(len(X))\n",
    "    \n",
    "    # Sample functions\n",
    "    mu = zero_mean_function(X)\n",
    "    f_samples = RNG.multivariate_normal(mu, K, size=3)\n",
    "    \n",
    "    # Plot settings\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # Plot samples\n",
    "    for i, f in enumerate(f_samples):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=X, y=f, mode='lines+markers', \n",
    "                      line=dict(color=color, width=2),\n",
    "                      marker=dict(size=4),\n",
    "                      name=f\"Sample {i+1}\" if idx == 0 else None,\n",
    "                      showlegend=idx == 0,\n",
    "                      opacity=0.7),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Plot mean and confidence bands\n",
    "    std = np.sqrt(np.diag(K))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.concatenate([X, X[::-1]]),\n",
    "                  y=np.concatenate([mu + 2*std, (mu - 2*std)[::-1]]),\n",
    "                  fill='toself', fillcolor='rgba(128,128,128,0.2)',\n",
    "                  line=dict(color='rgba(255,255,255,0)'),\n",
    "                  name=\"Â±2Ïƒ\" if idx == 0 else None,\n",
    "                  showlegend=idx == 0),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X, y=mu, mode='lines',\n",
    "                  line=dict(color='black', width=2, dash='dash'),\n",
    "                  name=\"Mean\" if idx == 0 else None,\n",
    "                  showlegend=idx == 0),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Progression from Multivariate Gaussian to Gaussian Process\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Input x\")\n",
    "fig.update_yaxes(title_text=\"Function value f(x)\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"As we increase the number of points, we approach a continuous function sampled from a GP.\")\n",
    "print(\"Each subplot shows 3 different function samples from the same GP prior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: As we increase the number of evaluation points, the discrete samples begin to resemble continuous functions. In the limit, we have a Gaussian Process that defines a probability distribution over the entire function space.\n",
    "\n",
    "### Understanding Covariance Matrices\n",
    "\n",
    "The covariance matrix $\\mathbf{K}$ encodes all the structural assumptions we make about our functions. Let's visualize how different hyperparameters affect the covariance structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small set of points to visualize covariance matrices\n",
    "X_small = np.linspace(0, 4, 5)\n",
    "\n",
    "# Different hyperparameter configurations\n",
    "configs = [\n",
    "    {'length_scale': 0.5, 'variance': 1.0, 'title': 'Short Length Scale (â„“=0.5)'},\n",
    "    {'length_scale': 2.0, 'variance': 1.0, 'title': 'Long Length Scale (â„“=2.0)'},\n",
    "    {'length_scale': 1.0, 'variance': 0.5, 'title': 'Low Variance (ÏƒÂ²=0.5)'},\n",
    "    {'length_scale': 1.0, 'variance': 2.0, 'title': 'High Variance (ÏƒÂ²=2.0)'}\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=[config['title'] for config in configs],\n",
    "                    vertical_spacing=0.15)\n",
    "\n",
    "for idx, config in enumerate(configs):\n",
    "    # Create covariance matrix\n",
    "    K = create_rbf_covariance(X_small, \n",
    "                             length_scale=config['length_scale'],\n",
    "                             variance=config['variance'])\n",
    "    \n",
    "    # Plot settings\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=K, \n",
    "                   x=[f\"{x:.1f}\" for x in X_small],\n",
    "                   y=[f\"{x:.1f}\" for x in X_small],\n",
    "                   colorscale='Viridis',\n",
    "                   showscale=idx == 0,  # Only show colorscale for first plot\n",
    "                   text=np.round(K, 3),\n",
    "                   texttemplate=\"%{text}\",\n",
    "                   textfont={\"size\": 10}),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Covariance Matrices with Different Hyperparameters\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Covariance Matrix Interpretation:\")\n",
    "print(\"â€¢ Diagonal elements: Variance at each point (should equal ÏƒÂ²)\")\n",
    "print(\"â€¢ Off-diagonal elements: Covariance between different points\")\n",
    "print(\"â€¢ Length scale controls how quickly covariance decays with distance\")\n",
    "print(\"â€¢ Variance parameter scales the overall magnitude of the covariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– Hands-On Exercise 1: Using LLMs for Kernel Construction\n",
    "\n",
    "Before we dive into PyMC specifics, let's practice using **Large Language Models (LLMs) like those in VSCode Copilot or Cursor** to help us understand and implement Gaussian Process kernels. This exercise demonstrates how to effectively collaborate with AI coding assistants for probabilistic programming.\n",
    "\n",
    "### Exercise Instructions\n",
    "\n",
    "**Your task**: Use your LLM to help you build and experiment with different kernel functions. Work through the following steps by asking your AI assistant for guidance:\n",
    "\n",
    "1. **Ask your LLM**: \"Help me implement a custom RBF kernel function in Python that matches PyMC's ExpQuad kernel\"\n",
    "2. **Request comparison**: \"Show me how to compare my custom kernel with PyMC's built-in kernel\"\n",
    "3. **Seek visualization help**: \"Create a function to visualize how different hyperparameters affect kernel shape\"\n",
    "\n",
    "### Effective Prompting Tips for PyMC/GP Problems\n",
    "\n",
    "When working with LLMs on Gaussian Process problems, use these strategies:\n",
    "\n",
    "- **Be specific about the framework**: Mention \"PyMC\", \"Gaussian Processes\", and specific function names\n",
    "- **Include context**: \"I'm working on a regression problem with X inputs and y outputs\"\n",
    "- **Request explanations**: Ask \"Why did you choose this kernel?\" or \"What does this hyperparameter control?\"\n",
    "- **Ask for alternatives**: \"What other kernels could work for this problem?\"\n",
    "- **Request debugging help**: \"This PyMC model isn't converging, what should I check?\"\n",
    "\n",
    "### Sample Prompts to Try\n",
    "\n",
    "Copy and paste these prompts into your LLM assistant (modify with your specific details):\n",
    "\n",
    "```\n",
    "PROMPT 1: \"I'm learning Gaussian Processes with PyMC. Can you help me implement \n",
    "a function that creates an RBF covariance matrix from scratch, then compare it \n",
    "to PyMC's pm.gp.cov.ExpQuad? Include visualization of the kernel shape.\"\n",
    "\n",
    "PROMPT 2: \"Show me how to sample functions from a Gaussian Process prior \n",
    "using my custom kernel. I want to see how changing the lengthscale from 0.1 \n",
    "to 2.0 affects the smoothness of sampled functions.\"\n",
    "\n",
    "PROMPT 3: \"Help me understand why my PyMC GP model is taking a long time \n",
    "to sample. I'm using 100 data points with an ExpQuad kernel. What are \n",
    "common performance issues and how can I optimize it?\"\n",
    "```\n",
    "\n",
    "**Work on the exercise below, but don't hesitate to ask your LLM for help when you get stuck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– EXERCISE: Use your LLM to help complete this kernel implementation\n",
    "\n",
    "# STEP 1: Ask your LLM to help you implement this function\n",
    "def custom_rbf_kernel(X1, X2, lengthscale=1.0, variance=1.0):\n",
    "    \"\"\"\n",
    "    Custom RBF kernel implementation - ask your LLM to help complete this!\n",
    "    \n",
    "    Prompt suggestion: \"Help me complete this RBF kernel function that computes\n",
    "    the covariance matrix between input points X1 and X2 with given lengthscale\n",
    "    and variance parameters. The formula is: k(x,x') = ÏƒÂ² * exp(-||x-x'||Â²/(2â„“Â²))\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 2: Ask your LLM to help you create a comparison with PyMC's kernel\n",
    "def compare_with_pymc_kernel(X_test):\n",
    "    \"\"\"\n",
    "    Compare custom kernel with PyMC's ExpQuad - get LLM help here!\n",
    "    \n",
    "    Prompt suggestion: \"Show me how to compare my custom RBF kernel with\n",
    "    PyMC's pm.gp.cov.ExpQuad kernel using the same hyperparameters. Create\n",
    "    a visualization that shows both kernel shapes side by side.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# STEP 3: Ask your LLM to create a hyperparameter sensitivity analysis\n",
    "def analyze_hyperparameter_effects():\n",
    "    \"\"\"\n",
    "    Analyze how lengthscale and variance affect kernel behavior.\n",
    "    \n",
    "    Prompt suggestion: \"Create an interactive visualization showing how\n",
    "    different lengthscale and variance values affect the RBF kernel shape\n",
    "    and sampled functions. Include at least 3 different parameter combinations.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# Test your implementations\n",
    "X_demo = np.linspace(-3, 3, 50)[:, None]\n",
    "\n",
    "print(\"ğŸ¯ Exercise Goal: Use your LLM assistant to implement the functions above!\")\n",
    "print(\"ğŸ’¡ Remember to ask for explanations of any code you don't understand.\")\n",
    "print(\"ğŸ” Try different prompting strategies to see which work best for you.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: PyMC Fundamentals for Probabilistic Programming\n",
    "\n",
    "Before diving into Gaussian Processes specifically, we need to understand PyMC's approach to probabilistic programming. PyMC provides a powerful framework for specifying, fitting, and analyzing Bayesian models through an intuitive Python interface.\n",
    "\n",
    "### The Philosophy of Probabilistic Programming\n",
    "\n",
    "Probabilistic programming represents a paradigm shift in statistical modeling. Instead of deriving update equations or coding samplers by hand, we declare the structure of our model and let the framework handle the computational details. This approach offers several advantages:\n",
    "\n",
    "- **Model specification mirrors mathematical notation**: Code looks like the mathematical model\n",
    "- **Automatic inference**: No need to implement custom sampling algorithms\n",
    "- **Composability**: Complex models can be built from simpler components\n",
    "- **Flexibility**: Easy to experiment with different model structures\n",
    "\n",
    "### PyMC's Core Components\n",
    "\n",
    "PyMC organizes probabilistic models around several key abstractions:\n",
    "\n",
    "1. **Model Context**: A context manager that tracks all model components\n",
    "2. **Random Variables**: Represent uncertain quantities with probability distributions\n",
    "3. **Deterministic Variables**: Represent quantities that are functions of other variables\n",
    "4. **Observed Variables**: Represent data that we condition on\n",
    "\n",
    "Let's explore each of these concepts through examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Contexts and Random Variables\n",
    "\n",
    "Every PyMC model exists within a **Model context**. This context manager keeps track of all model components and their relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model context\n",
    "with pm.Model() as simple_model:\n",
    "    # Define a random variable\n",
    "    theta = pm.Normal('theta', mu=0, sigma=1)\n",
    "    \n",
    "    # The model automatically tracks this variable\n",
    "    print(f\"Model variables: {list(simple_model.named_vars.keys())}\")\n",
    "    print(f\"Variable type: {type(theta)}\")\n",
    "    \n",
    "# We can examine the model structure\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"Number of free random variables: {len(simple_model.free_RVs)}\")\n",
    "print(f\"Number of observed variables: {len(simple_model.observed_RVs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Distributions\n",
    "\n",
    "PyMC provides a comprehensive library of probability distributions. Let's explore some commonly used distributions and their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different distribution types\n",
    "with pm.Model() as distribution_demo:\n",
    "    \n",
    "    # Continuous distributions\n",
    "    normal_var = pm.Normal('normal', mu=0, sigma=1)\n",
    "    gamma_var = pm.Gamma('gamma', alpha=2, beta=1)\n",
    "    beta_var = pm.Beta('beta', alpha=2, beta=2)\n",
    "    \n",
    "    # Discrete distributions  \n",
    "    binomial_var = pm.Binomial('binomial', n=10, p=0.3)\n",
    "    poisson_var = pm.Poisson('poisson', mu=3)\n",
    "    \n",
    "    # Half-distributions (positive support)\n",
    "    half_normal_var = pm.HalfNormal('half_normal', sigma=1)\n",
    "    \n",
    "    print(\"Distribution types in the model:\")\n",
    "    for var_name, var in distribution_demo.named_vars.items():\n",
    "        print(f\"  {var_name}: {var.owner.op.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Distributions\n",
    "\n",
    "PyMC provides several ways to sample from distributions. The `pm.draw()` function allows us to sample from the prior distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from distributions\n",
    "with distribution_demo:\n",
    "    # Sample single values\n",
    "    print(\"Single samples:\")\n",
    "    print(f\"Normal: {pm.draw(normal_var):.3f}\")\n",
    "    print(f\"Gamma: {pm.draw(gamma_var):.3f}\")\n",
    "    print(f\"Beta: {pm.draw(beta_var):.3f}\")\n",
    "    print(f\"Binomial: {pm.draw(binomial_var)}\")\n",
    "    print(f\"Poisson: {pm.draw(poisson_var)}\")\n",
    "    \n",
    "    # Sample multiple values\n",
    "    normal_samples = pm.draw(normal_var, draws=1000)\n",
    "    print(f\"\\n1000 Normal samples - Mean: {normal_samples.mean():.3f}, Std: {normal_samples.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Log-Probabilities\n",
    "\n",
    "A fundamental operation in Bayesian inference is computing log-probabilities. PyMC provides the `pm.logp()` function for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-probabilities\n",
    "with distribution_demo:\n",
    "    # Evaluate log-probability at specific values\n",
    "    print(\"Log-probabilities:\")\n",
    "    print(f\"Normal(0) at x=0: {pm.logp(normal_var, 0).eval():.3f}\")\n",
    "    print(f\"Normal(0) at x=2: {pm.logp(normal_var, 2).eval():.3f}\")\n",
    "    print(f\"Gamma(Î±=2,Î²=1) at x=1: {pm.logp(gamma_var, 1).eval():.3f}\")\n",
    "    print(f\"Beta(Î±=2,Î²=2) at x=0.5: {pm.logp(beta_var, 0.5).eval():.3f}\")\n",
    "    \n",
    "    # Compare to scipy for verification\n",
    "    scipy_normal_logpdf = stats.norm.logpdf(0, loc=0, scale=1)\n",
    "    pymc_normal_logp = pm.logp(normal_var, 0).eval()\n",
    "    print(f\"\\nVerification - SciPy: {scipy_normal_logpdf:.6f}, PyMC: {pymc_normal_logp:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Variables and Transformations\n",
    "\n",
    "Often we need to create variables that are deterministic functions of other variables. PyMC provides two approaches: anonymous transformations and named deterministic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as transformation_model:\n",
    "    # Base random variables\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    y = pm.Normal('y', mu=0, sigma=1)\n",
    "    \n",
    "    # Anonymous transformation (not tracked in output)\n",
    "    z_anonymous = x + y  # This won't appear in sampling output\n",
    "    \n",
    "    # Named deterministic (tracked in output)\n",
    "    z_named = pm.Deterministic('sum_xy', x + y)\n",
    "    squared = pm.Deterministic('x_squared', x**2)\n",
    "    \n",
    "    # We can also create more complex transformations\n",
    "    complex_transform = pm.Deterministic('complex', \n",
    "                                       pt.sin(x) * pt.exp(y / 2))\n",
    "\n",
    "print(\"Variables in transformation model:\")\n",
    "for name in transformation_model.named_vars.keys():\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Predictive Sampling\n",
    "\n",
    "Before fitting models to data, it's crucial to understand what our priors imply. **Prior predictive sampling** generates data from our model before seeing any observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear regression model for demonstration\n",
    "with pm.Model() as linear_model:\n",
    "    # Priors for regression coefficients\n",
    "    alpha = pm.Normal('intercept', mu=0, sigma=1)\n",
    "    beta = pm.Normal('slope', mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Create some input data\n",
    "    x_data = np.linspace(-2, 2, 50)\n",
    "    \n",
    "    # Define the linear relationship\n",
    "    mu = pm.Deterministic('mu', alpha + beta * x_data)\n",
    "    \n",
    "    # Likelihood (but no observed data yet)\n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma)\n",
    "    \n",
    "    # Sample from the prior predictive distribution\n",
    "    prior_predictive = pm.sample_prior_predictive(samples=500, random_seed=RANDOM_SEED)\n",
    "\n",
    "# Visualize prior predictive samples\n",
    "y_samples = prior_predictive.prior_predictive['y'].values\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot several prior predictive realizations\n",
    "for i in range(min(20, y_samples.shape[0])):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_data, y=y_samples[i, 0, :],\n",
    "                  mode='lines', opacity=0.3,\n",
    "                  line=dict(color='blue'),\n",
    "                  showlegend=i==0, name='Prior samples')\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Prior Predictive Samples from Linear Regression Model\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Prior predictive samples shape: {y_samples.shape}\")\n",
    "print(f\"This shows {y_samples.shape[0]} different realizations of our prior beliefs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Constraints and Transformations\n",
    "\n",
    "Many parameters have natural constraints (e.g., variances must be positive). PyMC automatically handles these constraints through parameter transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as constrained_model:\n",
    "    # Constrained variables\n",
    "    positive_var = pm.HalfNormal('positive', sigma=1)  # x >= 0\n",
    "    bounded_var = pm.Beta('bounded', alpha=2, beta=2)  # 0 <= x <= 1\n",
    "    unrestricted_var = pm.Normal('unrestricted', mu=0, sigma=1)  # x âˆˆ â„\n",
    "    \n",
    "    # PyMC automatically creates transformed versions for sampling\n",
    "    print(\"Free (transformed) variables for sampling:\")\n",
    "    for rv in constrained_model.free_RVs:\n",
    "        print(f\"  {rv}\")\n",
    "    \n",
    "    print(\"\\nValue variables (original scale):\")\n",
    "    for rv in constrained_model.value_vars:\n",
    "        print(f\"  {rv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Point**: PyMC handles parameter transformations automatically. For example, `HalfNormal` variables are log-transformed during sampling to ensure they remain positive, then back-transformed for interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## Part III: Introduction to PyMC Gaussian Processes\n",
    "\n",
    "Now that we understand PyMC's fundamentals, let's explore how to work with Gaussian Processes. PyMC provides a comprehensive GP module (`pm.gp`) with implementations optimized for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC's GP Module Structure\n",
    "\n",
    "PyMC's GP functionality is organized into several key components:\n",
    "\n",
    "1. **Mean Functions** (`pm.gp.mean`): Define the expected function behavior\n",
    "2. **Covariance Functions** (`pm.gp.cov`): Define the correlation structure\n",
    "3. **GP Implementations**: Different computational approaches\n",
    "   - `pm.gp.Marginal`: Efficient for Gaussian likelihoods\n",
    "   - `pm.gp.Latent`: Flexible for non-Gaussian likelihoods\n",
    "\n",
    "### Mean Functions\n",
    "\n",
    "Mean functions specify the expected value of the GP at each input. Let's explore the built-in options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demonstration data\n",
    "X_demo = np.linspace(0, 10, 100)[:, None]\n",
    "\n",
    "# Different mean functions\n",
    "mean_functions = {\n",
    "    'Zero': pm.gp.mean.Zero(),\n",
    "    'Constant': pm.gp.mean.Constant(c=2.5),\n",
    "    'Linear': pm.gp.mean.Linear(coeffs=pt.as_tensor([0.5]), intercept=pt.as_tensor(1.0))\n",
    "}\n",
    "\n",
    "# Evaluate mean functions\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for (name, mean_func), color in zip(mean_functions.items(), colors):\n",
    "    mean_values = mean_func(X_demo).eval()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_demo.flatten(), y=mean_values,\n",
    "                  mode='lines', name=f'{name} Mean',\n",
    "                  line=dict(color=color, width=3))\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PyMC Mean Functions\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Mean function value m(x)\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Mean Functions in PyMC:\")\n",
    "print(\"â€¢ Zero(): m(x) = 0 for all x\")\n",
    "print(\"â€¢ Constant(c): m(x) = c for all x\")\n",
    "print(\"â€¢ Linear(coeffs, intercept): m(x) = intercept + coeffsÂ·x\")\n",
    "print(\"â€¢ And more: Polynomial, custom functions...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Functions (Kernels)\n",
    "\n",
    "Covariance functions are the heart of GP modeling. They encode our assumptions about function behavior. Let's explore PyMC's built-in kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different covariance functions\n",
    "x_test = np.array([[0.0]])  # Reference point\n",
    "X_range = np.linspace(-3, 3, 200)[:, None]\n",
    "\n",
    "# Different covariance functions with similar length scales\n",
    "kernels = {\n",
    "    'ExpQuad (RBF)': pm.gp.cov.ExpQuad(1, ls=1.0),\n",
    "    'MatÃ©rn 5/2': pm.gp.cov.Matern52(1, ls=1.0),\n",
    "    'MatÃ©rn 3/2': pm.gp.cov.Matern32(1, ls=1.0),\n",
    "    'Exponential': pm.gp.cov.Exponential(1, ls=1.0)\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for (name, kernel), color in zip(kernels.items(), colors):\n",
    "    # Compute covariance with reference point\n",
    "    cov_values = [kernel(x_test, x).eval().item() for x in X_range]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_range.flatten(), y=cov_values,\n",
    "                  mode='lines', name=name,\n",
    "                  line=dict(color=color, width=3))\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Covariance Functions: k(0, x) vs x\",\n",
    "    xaxis_title=\"Distance from reference point\",\n",
    "    yaxis_title=\"Covariance k(0, x)\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Kernel Properties:\")\n",
    "print(\"â€¢ ExpQuad: Infinitely differentiable (very smooth functions)\")\n",
    "print(\"â€¢ MatÃ©rn 5/2: Twice differentiable (smooth functions)\")\n",
    "print(\"â€¢ MatÃ©rn 3/2: Once differentiable (moderately smooth)\")\n",
    "print(\"â€¢ Exponential: Continuous but not differentiable (rough functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– Hands-On Exercise 2: Building PyMC GP Models with LLM Assistance\n",
    "\n",
    "Now let's practice using LLMs to help us build complete PyMC Gaussian Process models. This exercise will guide you through using AI assistants to construct, debug, and optimize GP models.\n",
    "\n",
    "### Exercise Objectives\n",
    "\n",
    "1. Use your LLM to help implement both Marginal and Latent GP approaches\n",
    "2. Practice debugging PyMC model issues with AI assistance\n",
    "3. Learn to ask effective questions about hyperparameter selection\n",
    "4. Get help interpreting convergence diagnostics\n",
    "\n",
    "### Advanced Prompting Strategies for PyMC GPs\n",
    "\n",
    "When asking your LLM for help with PyMC GP models, try these specific approaches:\n",
    "\n",
    "**For Model Building:**\n",
    "- \"I have [X-type] data with [Y observations]. Help me choose between pm.gp.Marginal and pm.gp.Latent\"\n",
    "- \"Show me how to set up a PyMC model with a [kernel-type] covariance function for [problem-type]\"\n",
    "\n",
    "**For Debugging:**\n",
    "- \"My PyMC GP model has [specific error]. Here's my code: [paste code]. What's wrong?\"\n",
    "- \"PyMC sampling is very slow with [N] data points. How can I optimize this GP model?\"\n",
    "\n",
    "**For Analysis:**\n",
    "- \"Help me interpret these PyMC convergence diagnostics: [paste results]\"\n",
    "- \"My GP predictions look wrong. How should I validate this PyMC model?\"\n",
    "\n",
    "### Specific Prompts for This Exercise\n",
    "\n",
    "Try these prompts with your LLM (adapt as needed):\n",
    "\n",
    "```\n",
    "PROMPT 1: \"Help me create a PyMC Gaussian Process regression model using\n",
    "pm.gp.Marginal with an ExpQuad kernel. I want to model some 1D noisy sine wave data.\n",
    "Include proper hyperparameter priors and show how to sample from the posterior.\"\n",
    "\n",
    "PROMPT 2: \"Now help me implement the same model using pm.gp.Latent instead.\n",
    "Explain the differences in computational cost and when I'd choose each approach.\n",
    "Include posterior predictive sampling.\"\n",
    "\n",
    "PROMPT 3: \"My PyMC GP model gives R-hat values above 1.1. Help me diagnose\n",
    "and fix convergence issues. What should I check and how can I improve sampling?\"\n",
    "\n",
    "PROMPT 4: \"Show me how to compare two different kernel choices (RBF vs MatÃ©rn)\n",
    "for the same dataset using PyMC. Include model comparison metrics like WAIC.\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ğŸ¤– EXERCISE: Use your LLM to help build complete PyMC GP models\n",
    "\n",
    "# Generate some synthetic data for the exercise\n",
    "np.random.seed(42)\n",
    "X_train = np.linspace(0, 2*np.pi, 15)[:, None]\n",
    "y_train = np.sin(X_train.flatten()) + 0.2 * RNG.standard_normal(15)\n",
    "X_test = np.linspace(-0.5, 2.5*np.pi, 100)[:, None]\n",
    "\n",
    "print(\"Dataset created: 15 noisy sine wave observations\")\n",
    "print(\"Your task: Use your LLM to help you model this data with GPs!\")\n",
    "\n",
    "# TASK 1: Ask your LLM to help implement a Marginal GP model\n",
    "def build_marginal_gp_model():\n",
    "    \"\"\"\n",
    "    Use your LLM to help create a PyMC Marginal GP regression model.\n",
    "    \n",
    "    Suggested prompt: \"Help me create a PyMC model using pm.gp.Marginal to fit\n",
    "    a noisy sine wave dataset. Use an ExpQuad kernel with appropriate priors.\n",
    "    Show me how to fit the model and generate predictions.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# TASK 2: Ask your LLM to help implement a Latent GP model\n",
    "def build_latent_gp_model():\n",
    "    \"\"\"\n",
    "    Use your LLM to help create a PyMC Latent GP regression model.\n",
    "    \n",
    "    Suggested prompt: \"Now help me implement the same regression problem\n",
    "    using pm.gp.Latent instead of pm.gp.Marginal. Explain when I should\n",
    "    use each approach and show the computational differences.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# TASK 3: Ask your LLM for help with model diagnostics\n",
    "def diagnose_model_convergence(trace):\n",
    "    \"\"\"\n",
    "    Get LLM help to check model convergence and sampling quality.\n",
    "    \n",
    "    Suggested prompt: \"Help me create a function that checks PyMC sampling\n",
    "    convergence for GP models. Include R-hat, ESS, and visual diagnostics.\n",
    "    Show me what values indicate good vs. poor convergence.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "# TASK 4: Get LLM help for kernel comparison\n",
    "def compare_kernel_choices():\n",
    "    \"\"\"\n",
    "    Use your LLM to help compare different kernel choices.\n",
    "    \n",
    "    Suggested prompt: \"Help me compare ExpQuad vs. Matern52 kernels\n",
    "    on the same dataset using PyMC. Show me how to compute model\n",
    "    comparison metrics and visualize the differences in predictions.\"\n",
    "    \"\"\"\n",
    "    # YOUR LLM-ASSISTED CODE HERE\n",
    "    pass\n",
    "\n",
    "print(\"ğŸ¯ Goal: Complete all 4 tasks using your LLM assistant!\")\n",
    "print(\"ğŸ¤” Don't just copy code - ask your LLM to explain each step.\")\n",
    "print(\"ğŸ” Experiment with different prompting approaches to see what works best.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Learning Outcomes\n",
    "\n",
    "After completing this LLM-assisted exercise, you should be able to:\n",
    "\n",
    "- **Effectively prompt** LLMs for PyMC-specific help\n",
    "- **Debug common issues** in GP models with AI assistance\n",
    "- **Compare different implementations** (Marginal vs. Latent) intelligently\n",
    "- **Interpret model diagnostics** with LLM guidance\n",
    "- **Iterate on model designs** using AI feedback\n",
    "\n",
    "Remember: The goal isn't to have the LLM write all your code, but to use it as a knowledgeable pair programmer that can help you understand concepts, debug issues, and explore alternatives!\n",
    "\n",
    "---\n",
    "\n",
    "## Part IV: GP Implementations in PyMC - A Real Example\n",
    "\n",
    "Now let's put everything together by building a complete GP regression model using real data. We'll demonstrate both the Marginal and Latent approaches and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Synthetic Regression Data\n",
    "\n",
    "Let's create a realistic regression dataset that will showcase the strengths of GP modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data with non-linear structure\n",
    "def true_function(x):\n",
    "    \"\"\"A complex non-linear function to learn.\"\"\"\n",
    "    return (0.8 * np.sin(2*np.pi*x) + \n",
    "            0.3 * np.cos(6*np.pi*x) + \n",
    "            0.1 * x**2 - 0.05 * x)\n",
    "\n",
    "# Training data - deliberately sparse to show GP uncertainty\n",
    "n_train = 20\n",
    "X_train = RNG.uniform(0, 1, n_train)[:, None]\n",
    "X_train = np.sort(X_train, axis=0)\n",
    "\n",
    "y_true = true_function(X_train.flatten())\n",
    "noise_std = 0.08\n",
    "y_train = y_true + RNG.normal(0, noise_std, n_train)\n",
    "\n",
    "# Test data for predictions\n",
    "X_test = np.linspace(-0.1, 1.1, 150)[:, None]  # Slightly outside training range\n",
    "y_test_true = true_function(X_test.flatten())\n",
    "\n",
    "# Visualize the data\n",
    "fig = go.Figure()\n",
    "\n",
    "# True function\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=y_test_true,\n",
    "              mode='lines', name='True function',\n",
    "              line=dict(color='black', width=3, dash='dash'))\n",
    ")\n",
    "\n",
    "# Training data\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_train.flatten(), y=y_train,\n",
    "              mode='markers', name='Training data',\n",
    "              marker=dict(color='red', size=10, symbol='circle'))\n",
    ")\n",
    "\n",
    "# True (noiseless) training points\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_train.flatten(), y=y_true,\n",
    "              mode='markers', name='True (noiseless)',\n",
    "              marker=dict(color='darkred', size=8, symbol='x'))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Synthetic Regression Dataset\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=500,\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Training data: {n_train} points\")\n",
    "print(f\"Noise standard deviation: {noise_std}\")\n",
    "print(f\"Training range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"Test range: [{X_test.min():.2f}, {X_test.max():.2f}] (includes extrapolation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Marginal GP\n",
    "\n",
    "The marginal approach analytically integrates out the latent function, making it computationally efficient for Gaussian likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as marginal_model:\n",
    "    \n",
    "    # Hyperpriors for kernel hyperparameters\n",
    "    # Length scale: how quickly the covariance decays\n",
    "    â„“ = pm.InverseGamma(\"â„“\", alpha=5, beta=5)  # Weakly informative\n",
    "    \n",
    "    # Marginal standard deviation: overall function scale\n",
    "    Î· = pm.HalfNormal(\"Î·\", sigma=2)\n",
    "    \n",
    "    # Observation noise standard deviation\n",
    "    Ïƒ = pm.HalfNormal(\"Ïƒ\", sigma=0.5)\n",
    "    \n",
    "    # Mean function (zero for simplicity)\n",
    "    mean_func = pm.gp.mean.Zero()\n",
    "    \n",
    "    # Covariance function: scaled MatÃ©rn 5/2 kernel\n",
    "    cov_func = Î·**2 * pm.gp.cov.Matern52(1, â„“)\n",
    "    \n",
    "    # GP prior specification\n",
    "    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=cov_func)\n",
    "    \n",
    "    # Marginal likelihood - integrates out the function analytically\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=Ïƒ)\n",
    "    \n",
    "    print(\"Marginal GP Model Structure:\")\n",
    "    print(f\"Hyperparameters: {[v.name for v in marginal_model.free_RVs]}\")\n",
    "    print(f\"Total free parameters: {len(marginal_model.free_RVs)}\")\n",
    "    print(\"Note: Latent function values are integrated out analytically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Latent GP\n",
    "\n",
    "The latent approach explicitly includes the function values as parameters, providing more flexibility but at higher computational cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as latent_model:\n",
    "    \n",
    "    # Same hyperpriors\n",
    "    â„“ = pm.InverseGamma(\"â„“\", alpha=5, beta=5)\n",
    "    Î· = pm.HalfNormal(\"Î·\", sigma=2)\n",
    "    Ïƒ = pm.HalfNormal(\"Ïƒ\", sigma=0.5)\n",
    "    \n",
    "    # Same mean and covariance functions\n",
    "    mean_func = pm.gp.mean.Zero()\n",
    "    cov_func = Î·**2 * pm.gp.cov.Matern52(1, â„“)\n",
    "    \n",
    "    # GP specification\n",
    "    gp = pm.gp.Latent(mean_func=mean_func, cov_func=cov_func)\n",
    "    \n",
    "    # Explicit prior over function values at training points\n",
    "    f = gp.prior(\"f\", X=X_train)\n",
    "    \n",
    "    # Likelihood connecting function values to observations\n",
    "    y_obs = pm.Normal(\"y\", mu=f, sigma=Ïƒ, observed=y_train)\n",
    "    \n",
    "    print(\"Latent GP Model Structure:\")\n",
    "    print(f\"Hyperparameters: {[v.name for v in latent_model.free_RVs if v.name != 'f']}\")\n",
    "    print(f\"Function values: f (dimension {f.eval().shape})\")\n",
    "    print(f\"Total free parameters: {len(latent_model.free_RVs)}\")\n",
    "    print(\"Note: Function values are explicit random variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Performance Comparison\n",
    "\n",
    "Let's fit both models and compare their computational performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the marginal model\n",
    "print(\"Fitting Marginal GP model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with marginal_model:\n",
    "    trace_marginal = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=False\n",
    "    )\n",
    "\n",
    "marginal_time = time.time() - start_time\n",
    "marginal_ess = az.ess(trace_marginal).min().values\n",
    "\n",
    "print(f\"âœ“ Marginal model fitted in {marginal_time:.1f}s\")\n",
    "print(f\"  Minimum ESS: {marginal_ess:.0f}\")\n",
    "print(f\"  ESS per second: {marginal_ess/marginal_time:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the latent model\n",
    "print(\"\\nFitting Latent GP model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with latent_model:\n",
    "    trace_latent = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=False\n",
    "    )\n",
    "\n",
    "latent_time = time.time() - start_time\n",
    "latent_ess = az.ess(trace_latent, var_names=['â„“', 'Î·', 'Ïƒ']).min().values\n",
    "\n",
    "print(f\"âœ“ Latent model fitted in {latent_time:.1f}s\")\n",
    "print(f\"  Minimum ESS (hyperparameters): {latent_ess:.0f}\")\n",
    "print(f\"  ESS per second: {latent_ess/latent_time:.1f}\")\n",
    "print(f\"\\nSpeedup factor: {latent_time/marginal_time:.1f}x (Marginal is faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Predictions\n",
    "\n",
    "Now let's generate predictions from both models and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from both models\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Marginal model predictions\n",
    "with marginal_model:\n",
    "    f_pred_marginal = gp.conditional(\"f_pred\", X_test)\n",
    "    pred_marginal = pm.sample_posterior_predictive(\n",
    "        trace_marginal,\n",
    "        var_names=[\"f_pred\"],\n",
    "        progressbar=False,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "# Latent model predictions  \n",
    "with latent_model:\n",
    "    f_pred_latent = gp.conditional(\"f_pred\", X_test)\n",
    "    pred_latent = pm.sample_posterior_predictive(\n",
    "        trace_latent,\n",
    "        var_names=[\"f_pred\"],\n",
    "        progressbar=False,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "# Extract prediction statistics\n",
    "f_pred_marginal_samples = pred_marginal.posterior_predictive[\"f_pred\"].values\n",
    "f_pred_mean_marginal = f_pred_marginal_samples.mean(axis=(0, 1))\n",
    "f_pred_std_marginal = f_pred_marginal_samples.std(axis=(0, 1))\n",
    "\n",
    "f_pred_latent_samples = pred_latent.posterior_predictive[\"f_pred\"].values\n",
    "f_pred_mean_latent = f_pred_latent_samples.mean(axis=(0, 1))\n",
    "f_pred_std_latent = f_pred_latent_samples.std(axis=(0, 1))\n",
    "\n",
    "print(\"âœ“ Predictions generated for both models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization and Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=[\"Marginal GP Predictions\", \"Latent GP Predictions\",\n",
    "                                   \"Residuals Comparison\", \"Uncertainty Comparison\"],\n",
    "                    vertical_spacing=0.1, horizontal_spacing=0.1)\n",
    "\n",
    "# Function to add GP predictions to subplot\n",
    "def add_gp_predictions(fig, row, col, X, y_true, y_pred_mean, y_pred_std, \n",
    "                      X_train, y_train, color, name_prefix):\n",
    "    # Confidence interval\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X.flatten(), X.flatten()[::-1]]),\n",
    "            y=np.concatenate([y_pred_mean + 2*y_pred_std,\n",
    "                             (y_pred_mean - 2*y_pred_std)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor=f'rgba({\"0,100,255\" if color==\"blue\" else \"0,200,100\"},0.3)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Prediction mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X.flatten(), y=y_pred_mean,\n",
    "                  mode='lines', name=f'{name_prefix} Mean',\n",
    "                  line=dict(color=color, width=2),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # True function\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X.flatten(), y=y_true,\n",
    "                  mode='lines', name='True Function',\n",
    "                  line=dict(color='black', width=2, dash='dash'),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_train.flatten(), y=y_train,\n",
    "                  mode='markers', name='Training Data',\n",
    "                  marker=dict(color='red', size=6),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Add predictions for both models\n",
    "add_gp_predictions(fig, 1, 1, X_test, y_test_true, f_pred_mean_marginal, \n",
    "                  f_pred_std_marginal, X_train, y_train, 'blue', 'Marginal')\n",
    "add_gp_predictions(fig, 1, 2, X_test, y_test_true, f_pred_mean_latent,\n",
    "                  f_pred_std_latent, X_train, y_train, 'green', 'Latent')\n",
    "\n",
    "# Residuals comparison\n",
    "residuals_marginal = f_pred_mean_marginal - y_test_true\n",
    "residuals_latent = f_pred_mean_latent - y_test_true\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=residuals_marginal,\n",
    "              mode='lines', name='Marginal Residuals',\n",
    "              line=dict(color='blue', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=residuals_latent,\n",
    "              mode='lines', name='Latent Residuals',\n",
    "              line=dict(color='green', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_hline(y=0, line=dict(color='black', dash='dash'), row=2, col=1)\n",
    "\n",
    "# Uncertainty comparison\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=f_pred_std_marginal,\n",
    "              mode='lines', name='Marginal Std',\n",
    "              line=dict(color='blue', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=f_pred_std_latent,\n",
    "              mode='lines', name='Latent Std', \n",
    "              line=dict(color='green', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Comprehensive GP Model Comparison\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_yaxes(title_text=\"y\", row=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Standard Deviation\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Performance metrics\n",
    "mse_marginal = np.mean(residuals_marginal**2)\n",
    "mse_latent = np.mean(residuals_latent**2)\n",
    "mae_marginal = np.mean(np.abs(residuals_marginal))\n",
    "mae_latent = np.mean(np.abs(residuals_latent))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Marginal GP:\")\n",
    "print(f\"  MSE: {mse_marginal:.6f}\")\n",
    "print(f\"  MAE: {mae_marginal:.6f}\")\n",
    "print(f\"  Sampling time: {marginal_time:.1f}s\")\n",
    "print(f\"\\nLatent GP:\")\n",
    "print(f\"  MSE: {mse_latent:.6f}\")\n",
    "print(f\"  MAE: {mae_latent:.6f}\")\n",
    "print(f\"  Sampling time: {latent_time:.1f}s\")\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  Î”MSE: {abs(mse_marginal - mse_latent):.6f}\")\n",
    "print(f\"  Speed ratio: {latent_time/marginal_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Posterior Analysis\n",
    "\n",
    "Let's examine the learned hyperparameters from both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare hyperparameter posteriors\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    subplot_titles=[\"Length Scale (â„“)\", \"Marginal Std (Î·)\", \"Noise Std (Ïƒ)\"])\n",
    "\n",
    "# Extract samples\n",
    "marginal_samples = az.extract(trace_marginal, num_samples=1000)\n",
    "latent_samples = az.extract(trace_latent, num_samples=1000, var_names=['â„“', 'Î·', 'Ïƒ'])\n",
    "\n",
    "params = ['â„“', 'Î·', 'Ïƒ']\n",
    "colors = ['blue', 'green']\n",
    "names = ['Marginal', 'Latent']\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    # Marginal samples\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=marginal_samples[param].values,\n",
    "            name=names[0] if i == 0 else None,\n",
    "            opacity=0.7,\n",
    "            nbinsx=30,\n",
    "            marker_color=colors[0],\n",
    "            showlegend=i==0\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "    \n",
    "    # Latent samples\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=latent_samples[param].values,\n",
    "            name=names[1] if i == 0 else None,\n",
    "            opacity=0.7,\n",
    "            nbinsx=30,\n",
    "            marker_color=colors[1],\n",
    "            showlegend=i==0\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text=\"Hyperparameter Posterior Distributions\",\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Parameter value\")\n",
    "fig.update_yaxes(title_text=\"Frequency\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Posterior summaries\n",
    "print(\"\\nHyperparameter Posterior Summaries:\")\n",
    "print(\"\\nMarginal GP:\")\n",
    "for param in params:\n",
    "    samples = marginal_samples[param].values\n",
    "    mean_val = samples.mean()\n",
    "    std_val = samples.std()\n",
    "    q025, q975 = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"  {param}: {mean_val:.3f} Â± {std_val:.3f} [{q025:.3f}, {q975:.3f}]\")\n",
    "\n",
    "print(\"\\nLatent GP:\")\n",
    "for param in params:\n",
    "    samples = latent_samples[param].values\n",
    "    mean_val = samples.mean()\n",
    "    std_val = samples.std()\n",
    "    q025, q975 = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"  {param}: {mean_val:.3f} Â± {std_val:.3f} [{q025:.3f}, {q975:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: When to Use Which Approach?\n",
    "\n",
    "Based on our comprehensive comparison, here are the key decision criteria:\n",
    "\n",
    "### Use **Marginal GP** (`pm.gp.Marginal`) when:\n",
    "- âœ… **Gaussian likelihood**: You have regression with normal noise\n",
    "- âœ… **Computational efficiency**: Speed and memory are important\n",
    "- âœ… **Large datasets**: More than ~100-200 data points\n",
    "- âœ… **Standard regression**: Basic function interpolation/extrapolation\n",
    "- âœ… **Production deployment**: Need fast inference\n",
    "\n",
    "### Use **Latent GP** (`pm.gp.Latent`) when:\n",
    "- âœ… **Non-Gaussian likelihoods**: Classification, count data, robust regression\n",
    "- âœ… **Function access needed**: Want posterior samples of the function itself\n",
    "- âœ… **Complex models**: Hierarchical models, multi-output GPs\n",
    "- âœ… **Small datasets**: Fewer than ~100 data points\n",
    "- âœ… **Research/exploration**: Flexibility more important than speed\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Performance**: Both approaches yield virtually identical predictions for Gaussian regression\n",
    "2. **Speed**: Marginal approach is typically 2-5x faster\n",
    "3. **Memory**: Marginal approach uses less memory (O(nÂ²) vs O(nÂ² + n))\n",
    "4. **Flexibility**: Latent approach works with any likelihood\n",
    "5. **Hyperparameters**: Both learn very similar hyperparameter values\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Start with Marginal GP** for standard regression problems\n",
    "- **Use informative priors** on hyperparameters when possible\n",
    "- **Check prior predictive samples** before fitting\n",
    "- **Monitor convergence** using effective sample size and R-hat\n",
    "- **Validate predictions** on held-out test data\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of Gaussian Process modeling with PyMC. You now understand:\n",
    "\n",
    "âœ… **Mathematical foundations**: From multivariate Gaussians to function-space distributions  \n",
    "âœ… **PyMC fundamentals**: Model contexts, distributions, and probabilistic programming  \n",
    "âœ… **Kernel theory**: How covariance functions encode function properties  \n",
    "âœ… **Implementation trade-offs**: When to use marginal vs latent approaches  \n",
    "âœ… **Complete workflow**: From prior specification to posterior analysis  \n",
    "\n",
    "### Preview of Session 2\n",
    "\n",
    "In the next session, **\"Advanced Kernels and Applications\"**, we'll explore:\n",
    "\n",
    "- **Kernel composition**: Combining kernels for complex patterns\n",
    "- **Specialized kernels**: Periodic, polynomial, and custom kernels\n",
    "- **Multi-dimensional inputs**: Handling higher-dimensional data\n",
    "- **Non-Gaussian likelihoods**: Classification and count data\n",
    "- **Model selection**: Comparing and validating GP models\n",
    "- **Scalability**: Techniques for larger datasets\n",
    "\n",
    "### Practice Exercises with LLM Assistance\n",
    "\n",
    "Now that you understand the fundamentals, it's time to practice using **Large Language Models (LLMs) like those in VSCode/Cursor to assist with PyMC GP development**. These exercises are specifically designed to help you leverage AI coding assistants effectively for probabilistic programming.\n",
    "\n",
    "1. **LLM-Assisted Kernel Experimentation**: Ask your LLM to help implement different kernels (MatÃ©rn, Periodic, combinations) and explain how they affect model behavior\n",
    "2. **Interactive Hyperparameter Analysis**: Use your LLM to create visualizations showing how different prior choices affect GP predictions\n",
    "3. **Real Data Application**: Have your LLM help you process and model a real dataset with appropriate GP specifications\n",
    "4. **Model Comparison**: Ask your LLM to implement and compare different GP formulations on the same problem\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **PyMC GP Documentation**: [Official Guide](https://www.pymc.io/projects/docs/en/stable/api/gp.html)\n",
    "- **Textbook**: Rasmussen & Williams (2006) \"Gaussian Processes for Machine Learning\" \n",
    "- **Examples**: [PyMC GP Gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/index.html)\n",
    "\n",
    "You've built a solid foundation for advanced GP modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}