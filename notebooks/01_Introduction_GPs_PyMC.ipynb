{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session 1: Introduction to Gaussian Processes and PyMC\n",
    "\n",
    "**Duration:** 3 hours  \n",
    "**Workshop:** Gaussian Processes with PyMC and LLMs\n",
    "\n",
    "---\n",
    "\n",
    "Gaussian Processes (GPs) represent one of the most elegant and powerful tools in modern machine learning and statistics. Unlike parametric models that assume specific functional forms, GPs provide a **non-parametric approach** that can capture complex patterns while naturally quantifying uncertainty. This makes them particularly valuable for applications where understanding uncertainty is as important as making predictions—from scientific modeling to decision-making under uncertainty.\n",
    "\n",
    "This session introduces the foundational concepts of Gaussian Processes within the PyMC probabilistic programming framework. We will build intuition about what it means for a process to be \"Gaussian,\" explore the mathematical machinery that makes GPs work, and learn to implement them using PyMC's powerful and expressive interface.\n",
    "\n",
    "## Why Gaussian Processes?\n",
    "\n",
    "Traditional machine learning often focuses on finding the \"best\" parameters for a pre-specified model. Gaussian Processes take a fundamentally different approach: instead of assuming a specific functional form, they place a probability distribution directly over the **space of functions**. This perspective offers several compelling advantages:\n",
    "\n",
    "- **Principled uncertainty quantification**: GPs provide natural confidence intervals and probability distributions over predictions\n",
    "- **Automatic model selection**: Through marginal likelihood optimization, GPs can automatically tune their complexity to the data\n",
    "- **Incorporation of prior knowledge**: Domain expertise can be encoded through choice of mean functions and covariance kernels\n",
    "- **Small data efficiency**: GPs can make meaningful predictions and quantify uncertainty even with limited training data\n",
    "- **Interpretable hyperparameters**: Kernel parameters often have clear physical or domain-specific meanings\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this session, you will be able to:\n",
    "\n",
    "1. **Understand the mathematical foundations of GPs**: Grasp how Gaussian Processes extend multivariate Gaussian distributions to infinite-dimensional function spaces\n",
    "2. **Build intuition through visualization**: Create and interpret samples from GP priors to understand how hyperparameters affect function behavior\n",
    "3. **Master PyMC's probabilistic programming paradigm**: Use PyMC's model contexts, distributions, and inference machinery for GP modeling\n",
    "4. **Construct and analyze covariance functions**: Build kernels from first principles and understand their role in encoding assumptions about function smoothness and structure\n",
    "5. **Navigate PyMC's GP implementations**: Understand the trade-offs between `gp.Marginal` and `gp.Latent` approaches and when to use each\n",
    "6. **Apply GPs to real problems**: Build complete GP regression models, from prior specification through posterior inference to prediction\n",
    "\n",
    "## Session Structure\n",
    "\n",
    "This session is organized into six major sections, each building on the previous one:\n",
    "\n",
    "1. **Mathematical Foundations** (45 minutes): Core concepts, definitions, and the connection between finite and infinite-dimensional Gaussians\n",
    "2. **PyMC Fundamentals** (45 minutes): Model contexts, distributions, random variables, and the probabilistic programming paradigm\n",
    "3. **Kernel Theory and Construction** (45 minutes): Understanding covariance functions as the heart of GP modeling\n",
    "4. **PyMC GP Implementations** (45 minutes): Comparing marginal vs. latent formulations with practical examples\n",
    "5. **Hands-on Practice** (30 minutes): Guided exercises to reinforce key concepts\n",
    "6. **Integration and Next Steps** (15 minutes): Synthesis and preview of advanced topics\n",
    "\n",
    "Let's begin our journey into the world of Gaussian Processes and probabilistic programming.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "We begin by setting up our computational environment with the necessary libraries for Gaussian Process modeling, Bayesian inference, and visualization. This section establishes the foundation for all subsequent analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core scientific computing\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import polars as pl\n",
    "\n",
    "# PyMC ecosystem for probabilistic programming\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pt\n",
    "import arviz as az\n",
    "\n",
    "# Visualization libraries\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "\n",
    "# Configure visualization defaults\n",
    "az.style.use('arviz-doc')\n",
    "pio.templates.default = 'plotly_white'\n",
    "px.defaults.template = 'plotly_white'\n",
    "px.defaults.width = 800\n",
    "px.defaults.height = 500\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = 20090425\n",
    "RNG = np.random.default_rng(RANDOM_SEED)\n",
    "\n",
    "print(f\"Environment configured successfully!\")\n",
    "print(f\"PyMC version: {pm.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {RANDOM_SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part I: Mathematical Foundations of Gaussian Processes\n",
    "\n",
    "To understand Gaussian Processes deeply, we must first establish their mathematical foundations. This section will build intuition by connecting familiar concepts (univariate and multivariate Gaussians) to the more abstract notion of distributions over functions.\n",
    "\n",
    "### From Scalars to Functions: The Gaussian Hierarchy\n",
    "\n",
    "The conceptual progression from simple to complex Gaussian structures provides the key to understanding GPs:\n",
    "\n",
    "1. **Univariate Gaussian**: $X \\sim \\mathcal{N}(\\mu, \\sigma^2)$ describes uncertainty about a single scalar value\n",
    "2. **Multivariate Gaussian**: $\\mathbf{X} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)$ describes uncertainty about a finite-dimensional vector\n",
    "3. **Gaussian Process**: $f(\\cdot) \\sim \\mathcal{GP}(m(\\cdot), k(\\cdot, \\cdot))$ describes uncertainty about an infinite-dimensional function\n",
    "\n",
    "The remarkable insight of Gaussian Processes is that we can work with infinite-dimensional function spaces by considering only finite-dimensional marginals at any collection of input points.\n",
    "\n",
    "### Formal Definition\n",
    "\n",
    "**Definition**: A Gaussian Process is a collection of random variables, any finite number of which have a joint Gaussian distribution.\n",
    "\n",
    "More precisely, a stochastic process $\\{f(x) : x \\in \\mathcal{X}\\}$ is a Gaussian Process if for any finite set of indices $\\{x_1, x_2, \\ldots, x_n\\} \\subset \\mathcal{X}$, the joint distribution of the random vector $(f(x_1), f(x_2), \\ldots, f(x_n))^T$ is multivariate Gaussian.\n",
    "\n",
    "A Gaussian Process is completely specified by two functions:\n",
    "\n",
    "1. **Mean function**: $m(x) = \\mathbb{E}[f(x)]$\n",
    "2. **Covariance function**: $k(x, x') = \\mathbb{Cov}[f(x), f(x')] = \\mathbb{E}[(f(x) - m(x))(f(x') - m(x'))]$\n",
    "\n",
    "We denote this as:\n",
    "$$f(x) \\sim \\mathcal{GP}(m(x), k(x, x'))$$\n",
    "\n",
    "### The Finite-Dimensional View\n",
    "\n",
    "For any finite collection of input points $\\mathbf{X} = \\{x_1, x_2, \\ldots, x_n\\}$, the corresponding function values $\\mathbf{f} = [f(x_1), f(x_2), \\ldots, f(x_n)]^T$ follow a multivariate Gaussian distribution:\n",
    "\n",
    "$$\\mathbf{f} \\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\mathbf{K})$$\n",
    "\n",
    "where:\n",
    "- $\\boldsymbol{\\mu} = [m(x_1), m(x_2), \\ldots, m(x_n)]^T$ is the mean vector\n",
    "- $\\mathbf{K}$ is the covariance matrix with entries $K_{ij} = k(x_i, x_j)$\n",
    "\n",
    "This finite-dimensional perspective is crucial because it allows us to:\n",
    "- Sample functions from the GP (by sampling from the multivariate Gaussian)\n",
    "- Compute likelihoods (using the multivariate Gaussian density)\n",
    "- Perform inference (using standard multivariate Gaussian conditioning)\n",
    "\n",
    "### Properties of Covariance Functions\n",
    "\n",
    "The covariance function $k(x, x')$ is the heart of a Gaussian Process. It encodes our assumptions about function smoothness, periodicity, and other structural properties. For $k$ to be a valid covariance function, it must be:\n",
    "\n",
    "1. **Symmetric**: $k(x, x') = k(x', x)$ for all $x, x'$\n",
    "2. **Positive semi-definite**: For any finite set $\\{x_1, \\ldots, x_n\\}$, the matrix $\\mathbf{K}$ with $K_{ij} = k(x_i, x_j)$ must be positive semi-definite\n",
    "\n",
    "These conditions ensure that the resulting covariance matrices are valid, guaranteeing that we can sample from and compute probabilities under the GP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Intuition: From Multivariate Gaussian to GP\n",
    "\n",
    "Let's build intuition by starting with a simple multivariate Gaussian and then extending to the GP setting. We'll see how increasing the number of dimensions naturally leads us to the function space perspective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rbf_covariance(X, length_scale=1.0, variance=1.0):\n",
    "    \"\"\"\n",
    "    Create RBF (Radial Basis Function) covariance matrix.\n",
    "    \n",
    "    The RBF kernel is defined as:\n",
    "    k(x, x') = σ² * exp(-||x - x'||² / (2ℓ²))\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X : array-like, shape (n,)\n",
    "        Input locations\n",
    "    length_scale : float\n",
    "        Length scale parameter ℓ\n",
    "    variance : float  \n",
    "        Variance parameter σ²\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    K : ndarray, shape (n, n)\n",
    "        Covariance matrix\n",
    "    \"\"\"\n",
    "    X = np.asarray(X).reshape(-1, 1) if np.asarray(X).ndim == 1 else np.asarray(X)\n",
    "    \n",
    "    # Compute squared Euclidean distances\n",
    "    sqdist = np.sum(X**2, axis=1)[:, None] + np.sum(X**2, axis=1)[None, :] - 2 * np.dot(X, X.T)\n",
    "    \n",
    "    # RBF covariance\n",
    "    K = variance * np.exp(-0.5 * sqdist / length_scale**2)\n",
    "    \n",
    "    return K\n",
    "\n",
    "def zero_mean_function(X):\n",
    "    \"\"\"Zero mean function.\"\"\"\n",
    "    return np.zeros(len(X))\n",
    "\n",
    "# Demonstrate the progression from few to many points\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=[\"5 points\", \"10 points\", \"25 points\", \"50 points\"],\n",
    "                    vertical_spacing=0.1)\n",
    "\n",
    "n_points_list = [5, 10, 25, 50]\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "\n",
    "for idx, (n_points, color) in enumerate(zip(n_points_list, colors)):\n",
    "    # Create input points\n",
    "    X = np.linspace(-3, 3, n_points)\n",
    "    \n",
    "    # Create covariance matrix\n",
    "    K = create_rbf_covariance(X, length_scale=1.0, variance=1.0)\n",
    "    \n",
    "    # Add small jitter for numerical stability\n",
    "    K += 1e-6 * np.eye(len(X))\n",
    "    \n",
    "    # Sample functions\n",
    "    mu = zero_mean_function(X)\n",
    "    f_samples = RNG.multivariate_normal(mu, K, size=3)\n",
    "    \n",
    "    # Plot settings\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # Plot samples\n",
    "    for i, f in enumerate(f_samples):\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=X, y=f, mode='lines+markers', \n",
    "                      line=dict(color=color, width=2),\n",
    "                      marker=dict(size=4),\n",
    "                      name=f\"Sample {i+1}\" if idx == 0 else None,\n",
    "                      showlegend=idx == 0,\n",
    "                      opacity=0.7),\n",
    "            row=row, col=col\n",
    "        )\n",
    "    \n",
    "    # Plot mean and confidence bands\n",
    "    std = np.sqrt(np.diag(K))\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=np.concatenate([X, X[::-1]]),\n",
    "                  y=np.concatenate([mu + 2*std, (mu - 2*std)[::-1]]),\n",
    "                  fill='toself', fillcolor='rgba(128,128,128,0.2)',\n",
    "                  line=dict(color='rgba(255,255,255,0)'),\n",
    "                  name=\"±2σ\" if idx == 0 else None,\n",
    "                  showlegend=idx == 0),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X, y=mu, mode='lines',\n",
    "                  line=dict(color='black', width=2, dash='dash'),\n",
    "                  name=\"Mean\" if idx == 0 else None,\n",
    "                  showlegend=idx == 0),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Progression from Multivariate Gaussian to Gaussian Process\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Input x\")\n",
    "fig.update_yaxes(title_text=\"Function value f(x)\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"As we increase the number of points, we approach a continuous function sampled from a GP.\")\n",
    "print(\"Each subplot shows 3 different function samples from the same GP prior.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Insight**: As we increase the number of evaluation points, the discrete samples begin to resemble continuous functions. In the limit, we have a Gaussian Process that defines a probability distribution over the entire function space.\n",
    "\n",
    "### Understanding Covariance Matrices\n",
    "\n",
    "The covariance matrix $\\mathbf{K}$ encodes all the structural assumptions we make about our functions. Let's visualize how different hyperparameters affect the covariance structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a small set of points to visualize covariance matrices\n",
    "X_small = np.linspace(0, 4, 5)\n",
    "\n",
    "# Different hyperparameter configurations\n",
    "configs = [\n",
    "    {'length_scale': 0.5, 'variance': 1.0, 'title': 'Short Length Scale (ℓ=0.5)'},\n",
    "    {'length_scale': 2.0, 'variance': 1.0, 'title': 'Long Length Scale (ℓ=2.0)'},\n",
    "    {'length_scale': 1.0, 'variance': 0.5, 'title': 'Low Variance (σ²=0.5)'},\n",
    "    {'length_scale': 1.0, 'variance': 2.0, 'title': 'High Variance (σ²=2.0)'}\n",
    "]\n",
    "\n",
    "fig = make_subplots(rows=2, cols=2, \n",
    "                    subplot_titles=[config['title'] for config in configs],\n",
    "                    vertical_spacing=0.15)\n",
    "\n",
    "for idx, config in enumerate(configs):\n",
    "    # Create covariance matrix\n",
    "    K = create_rbf_covariance(X_small, \n",
    "                             length_scale=config['length_scale'],\n",
    "                             variance=config['variance'])\n",
    "    \n",
    "    # Plot settings\n",
    "    row = idx // 2 + 1\n",
    "    col = idx % 2 + 1\n",
    "    \n",
    "    # Create heatmap\n",
    "    fig.add_trace(\n",
    "        go.Heatmap(z=K, \n",
    "                   x=[f\"{x:.1f}\" for x in X_small],\n",
    "                   y=[f\"{x:.1f}\" for x in X_small],\n",
    "                   colorscale='Viridis',\n",
    "                   showscale=idx == 0,  # Only show colorscale for first plot\n",
    "                   text=np.round(K, 3),\n",
    "                   texttemplate=\"%{text}\",\n",
    "                   textfont={\"size\": 10}),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=600,\n",
    "    title_text=\"Covariance Matrices with Different Hyperparameters\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Covariance Matrix Interpretation:\")\n",
    "print(\"• Diagonal elements: Variance at each point (should equal σ²)\")\n",
    "print(\"• Off-diagonal elements: Covariance between different points\")\n",
    "print(\"• Length scale controls how quickly covariance decays with distance\")\n",
    "print(\"• Variance parameter scales the overall magnitude of the covariance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part II: PyMC Fundamentals for Probabilistic Programming\n",
    "\n",
    "Before diving into Gaussian Processes specifically, we need to understand PyMC's approach to probabilistic programming. PyMC provides a powerful framework for specifying, fitting, and analyzing Bayesian models through an intuitive Python interface.\n",
    "\n",
    "### The Philosophy of Probabilistic Programming\n",
    "\n",
    "Probabilistic programming represents a paradigm shift in statistical modeling. Instead of deriving update equations or coding samplers by hand, we declare the structure of our model and let the framework handle the computational details. This approach offers several advantages:\n",
    "\n",
    "- **Model specification mirrors mathematical notation**: Code looks like the mathematical model\n",
    "- **Automatic inference**: No need to implement custom sampling algorithms\n",
    "- **Composability**: Complex models can be built from simpler components\n",
    "- **Flexibility**: Easy to experiment with different model structures\n",
    "\n",
    "### PyMC's Core Components\n",
    "\n",
    "PyMC organizes probabilistic models around several key abstractions:\n",
    "\n",
    "1. **Model Context**: A context manager that tracks all model components\n",
    "2. **Random Variables**: Represent uncertain quantities with probability distributions\n",
    "3. **Deterministic Variables**: Represent quantities that are functions of other variables\n",
    "4. **Observed Variables**: Represent data that we condition on\n",
    "\n",
    "Let's explore each of these concepts through examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Contexts and Random Variables\n",
    "\n",
    "Every PyMC model exists within a **Model context**. This context manager keeps track of all model components and their relationships:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model context\n",
    "with pm.Model() as simple_model:\n",
    "    # Define a random variable\n",
    "    theta = pm.Normal('theta', mu=0, sigma=1)\n",
    "    \n",
    "    # The model automatically tracks this variable\n",
    "    print(f\"Model variables: {list(simple_model.named_vars.keys())}\")\n",
    "    print(f\"Variable type: {type(theta)}\")\n",
    "    \n",
    "# We can examine the model structure\n",
    "print(f\"\\nModel summary:\")\n",
    "print(f\"Number of free random variables: {len(simple_model.free_RVs)}\")\n",
    "print(f\"Number of observed variables: {len(simple_model.observed_RVs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Working with Distributions\n",
    "\n",
    "PyMC provides a comprehensive library of probability distributions. Let's explore some commonly used distributions and their properties:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different distribution types\n",
    "with pm.Model() as distribution_demo:\n",
    "    \n",
    "    # Continuous distributions\n",
    "    normal_var = pm.Normal('normal', mu=0, sigma=1)\n",
    "    gamma_var = pm.Gamma('gamma', alpha=2, beta=1)\n",
    "    beta_var = pm.Beta('beta', alpha=2, beta=2)\n",
    "    \n",
    "    # Discrete distributions  \n",
    "    binomial_var = pm.Binomial('binomial', n=10, p=0.3)\n",
    "    poisson_var = pm.Poisson('poisson', mu=3)\n",
    "    \n",
    "    # Half-distributions (positive support)\n",
    "    half_normal_var = pm.HalfNormal('half_normal', sigma=1)\n",
    "    \n",
    "    print(\"Distribution types in the model:\")\n",
    "    for var_name, var in distribution_demo.named_vars.items():\n",
    "        print(f\"  {var_name}: {var.owner.op.__class__.__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling from Distributions\n",
    "\n",
    "PyMC provides several ways to sample from distributions. The `pm.draw()` function allows us to sample from the prior distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from distributions\n",
    "with distribution_demo:\n",
    "    # Sample single values\n",
    "    print(\"Single samples:\")\n",
    "    print(f\"Normal: {pm.draw(normal_var):.3f}\")\n",
    "    print(f\"Gamma: {pm.draw(gamma_var):.3f}\")\n",
    "    print(f\"Beta: {pm.draw(beta_var):.3f}\")\n",
    "    print(f\"Binomial: {pm.draw(binomial_var)}\")\n",
    "    print(f\"Poisson: {pm.draw(poisson_var)}\")\n",
    "    \n",
    "    # Sample multiple values\n",
    "    normal_samples = pm.draw(normal_var, draws=1000)\n",
    "    print(f\"\\n1000 Normal samples - Mean: {normal_samples.mean():.3f}, Std: {normal_samples.std():.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing Log-Probabilities\n",
    "\n",
    "A fundamental operation in Bayesian inference is computing log-probabilities. PyMC provides the `pm.logp()` function for this purpose:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute log-probabilities\n",
    "with distribution_demo:\n",
    "    # Evaluate log-probability at specific values\n",
    "    print(\"Log-probabilities:\")\n",
    "    print(f\"Normal(0) at x=0: {pm.logp(normal_var, 0).eval():.3f}\")\n",
    "    print(f\"Normal(0) at x=2: {pm.logp(normal_var, 2).eval():.3f}\")\n",
    "    print(f\"Gamma(α=2,β=1) at x=1: {pm.logp(gamma_var, 1).eval():.3f}\")\n",
    "    print(f\"Beta(α=2,β=2) at x=0.5: {pm.logp(beta_var, 0.5).eval():.3f}\")\n",
    "    \n",
    "    # Compare to scipy for verification\n",
    "    scipy_normal_logpdf = stats.norm.logpdf(0, loc=0, scale=1)\n",
    "    pymc_normal_logp = pm.logp(normal_var, 0).eval()\n",
    "    print(f\"\\nVerification - SciPy: {scipy_normal_logpdf:.6f}, PyMC: {pymc_normal_logp:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deterministic Variables and Transformations\n",
    "\n",
    "Often we need to create variables that are deterministic functions of other variables. PyMC provides two approaches: anonymous transformations and named deterministic variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as transformation_model:\n",
    "    # Base random variables\n",
    "    x = pm.Normal('x', mu=0, sigma=1)\n",
    "    y = pm.Normal('y', mu=0, sigma=1)\n",
    "    \n",
    "    # Anonymous transformation (not tracked in output)\n",
    "    z_anonymous = x + y  # This won't appear in sampling output\n",
    "    \n",
    "    # Named deterministic (tracked in output)\n",
    "    z_named = pm.Deterministic('sum_xy', x + y)\n",
    "    squared = pm.Deterministic('x_squared', x**2)\n",
    "    \n",
    "    # We can also create more complex transformations\n",
    "    complex_transform = pm.Deterministic('complex', \n",
    "                                       pt.sin(x) * pt.exp(y / 2))\n",
    "\n",
    "print(\"Variables in transformation model:\")\n",
    "for name in transformation_model.named_vars.keys():\n",
    "    print(f\"  {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior Predictive Sampling\n",
    "\n",
    "Before fitting models to data, it's crucial to understand what our priors imply. **Prior predictive sampling** generates data from our model before seeing any observations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple linear regression model for demonstration\n",
    "with pm.Model() as linear_model:\n",
    "    # Priors for regression coefficients\n",
    "    alpha = pm.Normal('intercept', mu=0, sigma=1)\n",
    "    beta = pm.Normal('slope', mu=0, sigma=1)\n",
    "    sigma = pm.HalfNormal('sigma', sigma=1)\n",
    "    \n",
    "    # Create some input data\n",
    "    x_data = np.linspace(-2, 2, 50)\n",
    "    \n",
    "    # Define the linear relationship\n",
    "    mu = pm.Deterministic('mu', alpha + beta * x_data)\n",
    "    \n",
    "    # Likelihood (but no observed data yet)\n",
    "    y = pm.Normal('y', mu=mu, sigma=sigma)\n",
    "    \n",
    "    # Sample from the prior predictive distribution\n",
    "    prior_predictive = pm.sample_prior_predictive(samples=500, random_seed=RANDOM_SEED)\n",
    "\n",
    "# Visualize prior predictive samples\n",
    "y_samples = prior_predictive.prior_predictive['y'].values\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "# Plot several prior predictive realizations\n",
    "for i in range(min(20, y_samples.shape[0])):\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=x_data, y=y_samples[i, 0, :],\n",
    "                  mode='lines', opacity=0.3,\n",
    "                  line=dict(color='blue'),\n",
    "                  showlegend=i==0, name='Prior samples')\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Prior Predictive Samples from Linear Regression Model\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Prior predictive samples shape: {y_samples.shape}\")\n",
    "print(f\"This shows {y_samples.shape[0]} different realizations of our prior beliefs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter Constraints and Transformations\n",
    "\n",
    "Many parameters have natural constraints (e.g., variances must be positive). PyMC automatically handles these constraints through parameter transformations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as constrained_model:\n",
    "    # Constrained variables\n",
    "    positive_var = pm.HalfNormal('positive', sigma=1)  # x >= 0\n",
    "    bounded_var = pm.Beta('bounded', alpha=2, beta=2)  # 0 <= x <= 1\n",
    "    unrestricted_var = pm.Normal('unrestricted', mu=0, sigma=1)  # x ∈ ℝ\n",
    "    \n",
    "    # PyMC automatically creates transformed versions for sampling\n",
    "    print(\"Free (transformed) variables for sampling:\")\n",
    "    for rv in constrained_model.free_RVs:\n",
    "        print(f\"  {rv}\")\n",
    "    \n",
    "    print(\"\\nValue variables (original scale):\")\n",
    "    for rv in constrained_model.value_vars:\n",
    "        print(f\"  {rv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key Point**: PyMC handles parameter transformations automatically. For example, `HalfNormal` variables are log-transformed during sampling to ensure they remain positive, then back-transformed for interpretation.\n",
    "\n",
    "---\n",
    "\n",
    "## Part III: Introduction to PyMC Gaussian Processes\n",
    "\n",
    "Now that we understand PyMC's fundamentals, let's explore how to work with Gaussian Processes. PyMC provides a comprehensive GP module (`pm.gp`) with implementations optimized for different use cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyMC's GP Module Structure\n",
    "\n",
    "PyMC's GP functionality is organized into several key components:\n",
    "\n",
    "1. **Mean Functions** (`pm.gp.mean`): Define the expected function behavior\n",
    "2. **Covariance Functions** (`pm.gp.cov`): Define the correlation structure\n",
    "3. **GP Implementations**: Different computational approaches\n",
    "   - `pm.gp.Marginal`: Efficient for Gaussian likelihoods\n",
    "   - `pm.gp.Latent`: Flexible for non-Gaussian likelihoods\n",
    "\n",
    "### Mean Functions\n",
    "\n",
    "Mean functions specify the expected value of the GP at each input. Let's explore the built-in options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create demonstration data\n",
    "X_demo = np.linspace(0, 10, 100)[:, None]\n",
    "\n",
    "# Different mean functions\n",
    "mean_functions = {\n",
    "    'Zero': pm.gp.mean.Zero(),\n",
    "    'Constant': pm.gp.mean.Constant(c=2.5),\n",
    "    'Linear': pm.gp.mean.Linear(coeffs=pt.as_tensor([0.5]), intercept=pt.as_tensor(1.0))\n",
    "}\n",
    "\n",
    "# Evaluate mean functions\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['blue', 'red', 'green']\n",
    "for (name, mean_func), color in zip(mean_functions.items(), colors):\n",
    "    mean_values = mean_func(X_demo).eval()\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_demo.flatten(), y=mean_values,\n",
    "                  mode='lines', name=f'{name} Mean',\n",
    "                  line=dict(color=color, width=3))\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"PyMC Mean Functions\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"Mean function value m(x)\",\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Mean Functions in PyMC:\")\n",
    "print(\"• Zero(): m(x) = 0 for all x\")\n",
    "print(\"• Constant(c): m(x) = c for all x\")\n",
    "print(\"• Linear(coeffs, intercept): m(x) = intercept + coeffs·x\")\n",
    "print(\"• And more: Polynomial, custom functions...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Functions (Kernels)\n",
    "\n",
    "Covariance functions are the heart of GP modeling. They encode our assumptions about function behavior. Let's explore PyMC's built-in kernels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate different covariance functions\n",
    "x_test = np.array([[0.0]])  # Reference point\n",
    "X_range = np.linspace(-3, 3, 200)[:, None]\n",
    "\n",
    "# Different covariance functions with similar length scales\n",
    "kernels = {\n",
    "    'ExpQuad (RBF)': pm.gp.cov.ExpQuad(1, ls=1.0),\n",
    "    'Matérn 5/2': pm.gp.cov.Matern52(1, ls=1.0),\n",
    "    'Matérn 3/2': pm.gp.cov.Matern32(1, ls=1.0),\n",
    "    'Exponential': pm.gp.cov.Exponential(1, ls=1.0)\n",
    "}\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "colors = ['blue', 'red', 'green', 'orange']\n",
    "for (name, kernel), color in zip(kernels.items(), colors):\n",
    "    # Compute covariance with reference point\n",
    "    cov_values = [kernel(x_test, x).eval().item() for x in X_range]\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_range.flatten(), y=cov_values,\n",
    "                  mode='lines', name=name,\n",
    "                  line=dict(color=color, width=3))\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Covariance Functions: k(0, x) vs x\",\n",
    "    xaxis_title=\"Distance from reference point\",\n",
    "    yaxis_title=\"Covariance k(0, x)\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(\"Kernel Properties:\")\n",
    "print(\"• ExpQuad: Infinitely differentiable (very smooth functions)\")\n",
    "print(\"• Matérn 5/2: Twice differentiable (smooth functions)\")\n",
    "print(\"• Matérn 3/2: Once differentiable (moderately smooth)\")\n",
    "print(\"• Exponential: Continuous but not differentiable (rough functions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part IV: GP Implementations in PyMC - A Real Example\n",
    "\n",
    "Now let's put everything together by building a complete GP regression model using real data. We'll demonstrate both the Marginal and Latent approaches and compare their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Synthetic Regression Data\n",
    "\n",
    "Let's create a realistic regression dataset that will showcase the strengths of GP modeling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic regression data with non-linear structure\n",
    "def true_function(x):\n",
    "    \"\"\"A complex non-linear function to learn.\"\"\"\n",
    "    return (0.8 * np.sin(2*np.pi*x) + \n",
    "            0.3 * np.cos(6*np.pi*x) + \n",
    "            0.1 * x**2 - 0.05 * x)\n",
    "\n",
    "# Training data - deliberately sparse to show GP uncertainty\n",
    "n_train = 20\n",
    "X_train = RNG.uniform(0, 1, n_train)[:, None]\n",
    "X_train = np.sort(X_train, axis=0)\n",
    "\n",
    "y_true = true_function(X_train.flatten())\n",
    "noise_std = 0.08\n",
    "y_train = y_true + RNG.normal(0, noise_std, n_train)\n",
    "\n",
    "# Test data for predictions\n",
    "X_test = np.linspace(-0.1, 1.1, 150)[:, None]  # Slightly outside training range\n",
    "y_test_true = true_function(X_test.flatten())\n",
    "\n",
    "# Visualize the data\n",
    "fig = go.Figure()\n",
    "\n",
    "# True function\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=y_test_true,\n",
    "              mode='lines', name='True function',\n",
    "              line=dict(color='black', width=3, dash='dash'))\n",
    ")\n",
    "\n",
    "# Training data\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_train.flatten(), y=y_train,\n",
    "              mode='markers', name='Training data',\n",
    "              marker=dict(color='red', size=10, symbol='circle'))\n",
    ")\n",
    "\n",
    "# True (noiseless) training points\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_train.flatten(), y=y_true,\n",
    "              mode='markers', name='True (noiseless)',\n",
    "              marker=dict(color='darkred', size=8, symbol='x'))\n",
    ")\n",
    "\n",
    "fig.update_layout(\n",
    "    title=\"Synthetic Regression Dataset\",\n",
    "    xaxis_title=\"x\",\n",
    "    yaxis_title=\"y\",\n",
    "    height=500,\n",
    "    legend=dict(x=0.02, y=0.98)\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "print(f\"Training data: {n_train} points\")\n",
    "print(f\"Noise standard deviation: {noise_std}\")\n",
    "print(f\"Training range: [{X_train.min():.2f}, {X_train.max():.2f}]\")\n",
    "print(f\"Test range: [{X_test.min():.2f}, {X_test.max():.2f}] (includes extrapolation)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 1: Marginal GP\n",
    "\n",
    "The marginal approach analytically integrates out the latent function, making it computationally efficient for Gaussian likelihoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as marginal_model:\n",
    "    \n",
    "    # Hyperpriors for kernel hyperparameters\n",
    "    # Length scale: how quickly the covariance decays\n",
    "    ℓ = pm.InverseGamma(\"ℓ\", alpha=5, beta=5)  # Weakly informative\n",
    "    \n",
    "    # Marginal standard deviation: overall function scale\n",
    "    η = pm.HalfNormal(\"η\", sigma=2)\n",
    "    \n",
    "    # Observation noise standard deviation\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=0.5)\n",
    "    \n",
    "    # Mean function (zero for simplicity)\n",
    "    mean_func = pm.gp.mean.Zero()\n",
    "    \n",
    "    # Covariance function: scaled Matérn 5/2 kernel\n",
    "    cov_func = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    \n",
    "    # GP prior specification\n",
    "    gp = pm.gp.Marginal(mean_func=mean_func, cov_func=cov_func)\n",
    "    \n",
    "    # Marginal likelihood - integrates out the function analytically\n",
    "    y_obs = gp.marginal_likelihood(\"y\", X=X_train, y=y_train, sigma=σ)\n",
    "    \n",
    "    print(\"Marginal GP Model Structure:\")\n",
    "    print(f\"Hyperparameters: {[v.name for v in marginal_model.free_RVs]}\")\n",
    "    print(f\"Total free parameters: {len(marginal_model.free_RVs)}\")\n",
    "    print(\"Note: Latent function values are integrated out analytically\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Approach 2: Latent GP\n",
    "\n",
    "The latent approach explicitly includes the function values as parameters, providing more flexibility but at higher computational cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as latent_model:\n",
    "    \n",
    "    # Same hyperpriors\n",
    "    ℓ = pm.InverseGamma(\"ℓ\", alpha=5, beta=5)\n",
    "    η = pm.HalfNormal(\"η\", sigma=2)\n",
    "    σ = pm.HalfNormal(\"σ\", sigma=0.5)\n",
    "    \n",
    "    # Same mean and covariance functions\n",
    "    mean_func = pm.gp.mean.Zero()\n",
    "    cov_func = η**2 * pm.gp.cov.Matern52(1, ℓ)\n",
    "    \n",
    "    # GP specification\n",
    "    gp = pm.gp.Latent(mean_func=mean_func, cov_func=cov_func)\n",
    "    \n",
    "    # Explicit prior over function values at training points\n",
    "    f = gp.prior(\"f\", X=X_train)\n",
    "    \n",
    "    # Likelihood connecting function values to observations\n",
    "    y_obs = pm.Normal(\"y\", mu=f, sigma=σ, observed=y_train)\n",
    "    \n",
    "    print(\"Latent GP Model Structure:\")\n",
    "    print(f\"Hyperparameters: {[v.name for v in latent_model.free_RVs if v.name != 'f']}\")\n",
    "    print(f\"Function values: f (dimension {f.eval().shape})\")\n",
    "    print(f\"Total free parameters: {len(latent_model.free_RVs)}\")\n",
    "    print(\"Note: Function values are explicit random variables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Fitting and Performance Comparison\n",
    "\n",
    "Let's fit both models and compare their computational performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the marginal model\n",
    "print(\"Fitting Marginal GP model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "with marginal_model:\n",
    "    trace_marginal = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=False\n",
    "    )\n",
    "\n",
    "marginal_time = time.time() - start_time\n",
    "marginal_ess = az.ess(trace_marginal).min().values\n",
    "\n",
    "print(f\"✓ Marginal model fitted in {marginal_time:.1f}s\")\n",
    "print(f\"  Minimum ESS: {marginal_ess:.0f}\")\n",
    "print(f\"  ESS per second: {marginal_ess/marginal_time:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the latent model\n",
    "print(\"\\nFitting Latent GP model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "with latent_model:\n",
    "    trace_latent = pm.sample(\n",
    "        draws=1000,\n",
    "        tune=1000,\n",
    "        chains=2,\n",
    "        target_accept=0.95,\n",
    "        random_seed=RANDOM_SEED,\n",
    "        progressbar=False\n",
    "    )\n",
    "\n",
    "latent_time = time.time() - start_time\n",
    "latent_ess = az.ess(trace_latent, var_names=['ℓ', 'η', 'σ']).min().values\n",
    "\n",
    "print(f\"✓ Latent model fitted in {latent_time:.1f}s\")\n",
    "print(f\"  Minimum ESS (hyperparameters): {latent_ess:.0f}\")\n",
    "print(f\"  ESS per second: {latent_ess/latent_time:.1f}\")\n",
    "print(f\"\\nSpeedup factor: {latent_time/marginal_time:.1f}x (Marginal is faster)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating Predictions\n",
    "\n",
    "Now let's generate predictions from both models and compare their performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions from both models\n",
    "print(\"Generating predictions...\")\n",
    "\n",
    "# Marginal model predictions\n",
    "with marginal_model:\n",
    "    f_pred_marginal = gp.conditional(\"f_pred\", X_test)\n",
    "    pred_marginal = pm.sample_posterior_predictive(\n",
    "        trace_marginal,\n",
    "        var_names=[\"f_pred\"],\n",
    "        progressbar=False,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "# Latent model predictions  \n",
    "with latent_model:\n",
    "    f_pred_latent = gp.conditional(\"f_pred\", X_test)\n",
    "    pred_latent = pm.sample_posterior_predictive(\n",
    "        trace_latent,\n",
    "        var_names=[\"f_pred\"],\n",
    "        progressbar=False,\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "# Extract prediction statistics\n",
    "f_pred_marginal_samples = pred_marginal.posterior_predictive[\"f_pred\"].values\n",
    "f_pred_mean_marginal = f_pred_marginal_samples.mean(axis=(0, 1))\n",
    "f_pred_std_marginal = f_pred_marginal_samples.std(axis=(0, 1))\n",
    "\n",
    "f_pred_latent_samples = pred_latent.posterior_predictive[\"f_pred\"].values\n",
    "f_pred_mean_latent = f_pred_latent_samples.mean(axis=(0, 1))\n",
    "f_pred_std_latent = f_pred_latent_samples.std(axis=(0, 1))\n",
    "\n",
    "print(\"✓ Predictions generated for both models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results Visualization and Comparison\n",
    "\n",
    "Let's create a comprehensive comparison of both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison plot\n",
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    subplot_titles=[\"Marginal GP Predictions\", \"Latent GP Predictions\",\n",
    "                                   \"Residuals Comparison\", \"Uncertainty Comparison\"],\n",
    "                    vertical_spacing=0.1, horizontal_spacing=0.1)\n",
    "\n",
    "# Function to add GP predictions to subplot\n",
    "def add_gp_predictions(fig, row, col, X, y_true, y_pred_mean, y_pred_std, \n",
    "                      X_train, y_train, color, name_prefix):\n",
    "    # Confidence interval\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=np.concatenate([X.flatten(), X.flatten()[::-1]]),\n",
    "            y=np.concatenate([y_pred_mean + 2*y_pred_std,\n",
    "                             (y_pred_mean - 2*y_pred_std)[::-1]]),\n",
    "            fill='toself',\n",
    "            fillcolor=f'rgba({\"0,100,255\" if color==\"blue\" else \"0,200,100\"},0.3)',\n",
    "            line=dict(color='rgba(255,255,255,0)'),\n",
    "            showlegend=False,\n",
    "            hoverinfo='skip'\n",
    "        ),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Prediction mean\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X.flatten(), y=y_pred_mean,\n",
    "                  mode='lines', name=f'{name_prefix} Mean',\n",
    "                  line=dict(color=color, width=2),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # True function\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X.flatten(), y=y_true,\n",
    "                  mode='lines', name='True Function',\n",
    "                  line=dict(color='black', width=2, dash='dash'),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "    \n",
    "    # Training data\n",
    "    fig.add_trace(\n",
    "        go.Scatter(x=X_train.flatten(), y=y_train,\n",
    "                  mode='markers', name='Training Data',\n",
    "                  marker=dict(color='red', size=6),\n",
    "                  showlegend=row==1 and col==1),\n",
    "        row=row, col=col\n",
    "    )\n",
    "\n",
    "# Add predictions for both models\n",
    "add_gp_predictions(fig, 1, 1, X_test, y_test_true, f_pred_mean_marginal, \n",
    "                  f_pred_std_marginal, X_train, y_train, 'blue', 'Marginal')\n",
    "add_gp_predictions(fig, 1, 2, X_test, y_test_true, f_pred_mean_latent,\n",
    "                  f_pred_std_latent, X_train, y_train, 'green', 'Latent')\n",
    "\n",
    "# Residuals comparison\n",
    "residuals_marginal = f_pred_mean_marginal - y_test_true\n",
    "residuals_latent = f_pred_mean_latent - y_test_true\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=residuals_marginal,\n",
    "              mode='lines', name='Marginal Residuals',\n",
    "              line=dict(color='blue', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=residuals_latent,\n",
    "              mode='lines', name='Latent Residuals',\n",
    "              line=dict(color='green', width=2)),\n",
    "    row=2, col=1\n",
    ")\n",
    "fig.add_hline(y=0, line=dict(color='black', dash='dash'), row=2, col=1)\n",
    "\n",
    "# Uncertainty comparison\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=f_pred_std_marginal,\n",
    "              mode='lines', name='Marginal Std',\n",
    "              line=dict(color='blue', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "fig.add_trace(\n",
    "    go.Scatter(x=X_test.flatten(), y=f_pred_std_latent,\n",
    "              mode='lines', name='Latent Std', \n",
    "              line=dict(color='green', width=2)),\n",
    "    row=2, col=2\n",
    ")\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    height=800,\n",
    "    title_text=\"Comprehensive GP Model Comparison\",\n",
    "    showlegend=True\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"x\")\n",
    "fig.update_yaxes(title_text=\"y\", row=1)\n",
    "fig.update_yaxes(title_text=\"Residual\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"Standard Deviation\", row=2, col=2)\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Performance metrics\n",
    "mse_marginal = np.mean(residuals_marginal**2)\n",
    "mse_latent = np.mean(residuals_latent**2)\n",
    "mae_marginal = np.mean(np.abs(residuals_marginal))\n",
    "mae_latent = np.mean(np.abs(residuals_latent))\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Marginal GP:\")\n",
    "print(f\"  MSE: {mse_marginal:.6f}\")\n",
    "print(f\"  MAE: {mae_marginal:.6f}\")\n",
    "print(f\"  Sampling time: {marginal_time:.1f}s\")\n",
    "print(f\"\\nLatent GP:\")\n",
    "print(f\"  MSE: {mse_latent:.6f}\")\n",
    "print(f\"  MAE: {mae_latent:.6f}\")\n",
    "print(f\"  Sampling time: {latent_time:.1f}s\")\n",
    "print(f\"\\nDifference:\")\n",
    "print(f\"  ΔMSE: {abs(mse_marginal - mse_latent):.6f}\")\n",
    "print(f\"  Speed ratio: {latent_time/marginal_time:.1f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Posterior Analysis\n",
    "\n",
    "Let's examine the learned hyperparameters from both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare hyperparameter posteriors\n",
    "fig = make_subplots(rows=1, cols=3,\n",
    "                    subplot_titles=[\"Length Scale (ℓ)\", \"Marginal Std (η)\", \"Noise Std (σ)\"])\n",
    "\n",
    "# Extract samples\n",
    "marginal_samples = az.extract(trace_marginal, num_samples=1000)\n",
    "latent_samples = az.extract(trace_latent, num_samples=1000, var_names=['ℓ', 'η', 'σ'])\n",
    "\n",
    "params = ['ℓ', 'η', 'σ']\n",
    "colors = ['blue', 'green']\n",
    "names = ['Marginal', 'Latent']\n",
    "\n",
    "for i, param in enumerate(params):\n",
    "    # Marginal samples\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=marginal_samples[param].values,\n",
    "            name=names[0] if i == 0 else None,\n",
    "            opacity=0.7,\n",
    "            nbinsx=30,\n",
    "            marker_color=colors[0],\n",
    "            showlegend=i==0\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "    \n",
    "    # Latent samples\n",
    "    fig.add_trace(\n",
    "        go.Histogram(\n",
    "            x=latent_samples[param].values,\n",
    "            name=names[1] if i == 0 else None,\n",
    "            opacity=0.7,\n",
    "            nbinsx=30,\n",
    "            marker_color=colors[1],\n",
    "            showlegend=i==0\n",
    "        ),\n",
    "        row=1, col=i+1\n",
    "    )\n",
    "\n",
    "fig.update_layout(\n",
    "    height=400,\n",
    "    title_text=\"Hyperparameter Posterior Distributions\",\n",
    "    barmode='overlay'\n",
    ")\n",
    "\n",
    "fig.update_xaxes(title_text=\"Parameter value\")\n",
    "fig.update_yaxes(title_text=\"Frequency\")\n",
    "\n",
    "fig.show()\n",
    "\n",
    "# Posterior summaries\n",
    "print(\"\\nHyperparameter Posterior Summaries:\")\n",
    "print(\"\\nMarginal GP:\")\n",
    "for param in params:\n",
    "    samples = marginal_samples[param].values\n",
    "    mean_val = samples.mean()\n",
    "    std_val = samples.std()\n",
    "    q025, q975 = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"  {param}: {mean_val:.3f} ± {std_val:.3f} [{q025:.3f}, {q975:.3f}]\")\n",
    "\n",
    "print(\"\\nLatent GP:\")\n",
    "for param in params:\n",
    "    samples = latent_samples[param].values\n",
    "    mean_val = samples.mean()\n",
    "    std_val = samples.std()\n",
    "    q025, q975 = np.percentile(samples, [2.5, 97.5])\n",
    "    print(f\"  {param}: {mean_val:.3f} ± {std_val:.3f} [{q025:.3f}, {q975:.3f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: When to Use Which Approach?\n",
    "\n",
    "Based on our comprehensive comparison, here are the key decision criteria:\n",
    "\n",
    "### Use **Marginal GP** (`pm.gp.Marginal`) when:\n",
    "- ✅ **Gaussian likelihood**: You have regression with normal noise\n",
    "- ✅ **Computational efficiency**: Speed and memory are important\n",
    "- ✅ **Large datasets**: More than ~100-200 data points\n",
    "- ✅ **Standard regression**: Basic function interpolation/extrapolation\n",
    "- ✅ **Production deployment**: Need fast inference\n",
    "\n",
    "### Use **Latent GP** (`pm.gp.Latent`) when:\n",
    "- ✅ **Non-Gaussian likelihoods**: Classification, count data, robust regression\n",
    "- ✅ **Function access needed**: Want posterior samples of the function itself\n",
    "- ✅ **Complex models**: Hierarchical models, multi-output GPs\n",
    "- ✅ **Small datasets**: Fewer than ~100 data points\n",
    "- ✅ **Research/exploration**: Flexibility more important than speed\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Performance**: Both approaches yield virtually identical predictions for Gaussian regression\n",
    "2. **Speed**: Marginal approach is typically 2-5x faster\n",
    "3. **Memory**: Marginal approach uses less memory (O(n²) vs O(n² + n))\n",
    "4. **Flexibility**: Latent approach works with any likelihood\n",
    "5. **Hyperparameters**: Both learn very similar hyperparameter values\n",
    "\n",
    "### Best Practices\n",
    "\n",
    "- **Start with Marginal GP** for standard regression problems\n",
    "- **Use informative priors** on hyperparameters when possible\n",
    "- **Check prior predictive samples** before fitting\n",
    "- **Monitor convergence** using effective sample size and R-hat\n",
    "- **Validate predictions** on held-out test data\n",
    "\n",
    "---\n",
    "\n",
    "## Next Steps and Advanced Topics\n",
    "\n",
    "Congratulations! You've mastered the fundamentals of Gaussian Process modeling with PyMC. You now understand:\n",
    "\n",
    "✅ **Mathematical foundations**: From multivariate Gaussians to function-space distributions  \n",
    "✅ **PyMC fundamentals**: Model contexts, distributions, and probabilistic programming  \n",
    "✅ **Kernel theory**: How covariance functions encode function properties  \n",
    "✅ **Implementation trade-offs**: When to use marginal vs latent approaches  \n",
    "✅ **Complete workflow**: From prior specification to posterior analysis  \n",
    "\n",
    "### Preview of Session 2\n",
    "\n",
    "In the next session, **\"Advanced Kernels and Applications\"**, we'll explore:\n",
    "\n",
    "- **Kernel composition**: Combining kernels for complex patterns\n",
    "- **Specialized kernels**: Periodic, polynomial, and custom kernels\n",
    "- **Multi-dimensional inputs**: Handling higher-dimensional data\n",
    "- **Non-Gaussian likelihoods**: Classification and count data\n",
    "- **Model selection**: Comparing and validating GP models\n",
    "- **Scalability**: Techniques for larger datasets\n",
    "\n",
    "### Practice Exercises\n",
    "\n",
    "Before the next session, try these exercises:\n",
    "\n",
    "1. **Experiment with kernels**: Replace Matérn 5/2 with different kernels and observe the changes\n",
    "2. **Hyperparameter sensitivity**: Vary the prior distributions and see how it affects results\n",
    "3. **Real data application**: Find a regression dataset and apply GP modeling\n",
    "4. **Extrapolation analysis**: Evaluate how well GPs extrapolate beyond the training range\n",
    "\n",
    "### Resources\n",
    "\n",
    "- **PyMC GP Documentation**: [Official Guide](https://www.pymc.io/projects/docs/en/stable/api/gp.html)\n",
    "- **Textbook**: Rasmussen & Williams (2006) \"Gaussian Processes for Machine Learning\" \n",
    "- **Examples**: [PyMC GP Gallery](https://www.pymc.io/projects/examples/en/latest/gaussian_processes/index.html)\n",
    "\n",
    "You've built a solid foundation for advanced GP modeling!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}